<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nextflow Blog</title>
    <link>https://www.nextflow.io</link>
    <atom:link href="https://www.nextflow.io/feed.xml" rel="self" type="application/rss+xml" />
    <description>Blogging about Nextflow, computational pipelines and parallel programming</description>
    <language>en-gb</language>
    <pubDate>Mon, 8 Nov 2021 15:09:48 +0000</pubDate>
    <lastBuildDate>Mon, 8 Nov 2021 15:09:48 +0000</lastBuildDate>

    <item>
      <title>Configure Git private repositories with Nextflow</title>
      <link>https://www.nextflow.io/blog/2021/configure-git-repositories-with-nextflow.html</link>
      <pubDate>Thu, 21 Oct 2021 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2021/configure-git-repositories-with-nextflow.html</guid>
      	<description>
	&lt;p&gt;Git has become the de-facto standard for source-code version control system and has seen increasing adoption across the spectrum of software development. &lt;/p&gt;&lt;p&gt;Nextflow provides builtin support for Git and most popular Git hosting platforms such as GitHub, GitLab and Bitbucket between the others, which streamline managing versions and track changes in your pipeline projects and facilitate the collaboration across different users. &lt;/p&gt;&lt;p&gt;In order to access public repositories Nextflow does not require any special configuration, just use the &lt;em&gt;http&lt;/em&gt; URL of the pipeline project you want to run in the run command, for example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run https://github.com/nextflow-io/hello
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;However to allow Nextflow to access private repositories you will need to specifiy the repository credentials, and the server hostname in the case of self-managed Git server installations.&lt;/p&gt;&lt;h2&gt;Configure access to private repositories&lt;/h2&gt;&lt;p&gt;This is done through a file name &lt;code&gt;scm&lt;/code&gt; placed in the &lt;code&gt;$HOME/.nextflow/&lt;/code&gt; directory, containing the credentials and other details for accessing a particular Git hosting solution. You can refer to the Nextflow documentation for all the &lt;a href=&quot;https://www.nextflow.io/docs/edge/sharing.html&quot;&gt;SCM configuration file&lt;/a&gt; options.&lt;/p&gt;&lt;p&gt;All of these platforms have their own authentication mechanisms for Git operations which are captured in the &lt;code&gt;$HOME/.nextflow/scm&lt;/code&gt; file with the following syntax:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;groovy&quot;&gt;providers {

  &amp;#39;&amp;lt;provider-name-1&amp;gt;&amp;#39; {
    user = value
    password = value
    ...
  }

  &amp;#39;&amp;lt;provider-name-2&amp;gt;&amp;#39; {
    user = value
    password = value
    ...
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note: Make sure to enclose the provider name with &lt;code&gt;&amp;#39;&lt;/code&gt; if it containes a &lt;code&gt;-&lt;/code&gt; or a blank character. &lt;/p&gt;&lt;p&gt;As of the 21.09.0-edge release, Nextflow integrates with the following Git providers:&lt;/p&gt;&lt;h2&gt;GitHub&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt; is one of the most well known Git providers and is home to some of the most popular open-source Nextflow pipelines from the &lt;a href=&quot;https://github.com/nf-core/&quot;&gt;nf-core&lt;/a&gt; community project.&lt;/p&gt;&lt;p&gt;If you wish to use Nextflow code from a &lt;strong&gt;public&lt;/strong&gt; repository hosted on GitHub.com, then you don&apos;t need to provide credentials (&lt;code&gt;user&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt;) to pull code from the repository. However, if you wish to interact with a private repository or are running into GitHub API rate limits for public repos, then you must provide elevated access to Nextflow by specifying your credentials in the &lt;code&gt;scm&lt;/code&gt; file.&lt;/p&gt;&lt;p&gt;It is worth noting that &lt;a href=&quot;https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/#what-you-need-to-do-today&quot;&gt;GitHub recently phased out Git password authentication&lt;/a&gt; and now requires that users supply a more secure GitHub-generated &lt;em&gt;Personal Access Token&lt;/em&gt; for authentication. With Nextflow, you can specify your &lt;em&gt;personal access token&lt;/em&gt; in the &lt;code&gt;password&lt;/code&gt; field.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;groovy&quot;&gt;providers {

  github {
    user = &amp;#39;me&amp;#39;
    password = &amp;#39;my-personal-access-token&amp;#39;
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate a &lt;code&gt;personal-access-token&lt;/code&gt; for the GitHub platform, follow the instructions provided &lt;a href=&quot;https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token&quot;&gt;here&lt;/a&gt;. Ensure that the token has at a minimum all the permissions in the &lt;code&gt;repo&lt;/code&gt; scope.&lt;/p&gt;&lt;p&gt;Once you have provided your username and &lt;em&gt;personal access token&lt;/em&gt;, as shown above, you can test the integration by pulling the repository code.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow pull https://github.com/user_name/private_repo
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Bitbucket Cloud&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://bitbucket.org/&quot;&gt;Bitbucket&lt;/a&gt; is a publicly accessible Git solution hosted by Atlassian. Please note that if you are using an on-premises Bitbucket installation, you should follow the instructions for &lt;em&gt;Bitbucket Server&lt;/em&gt; in the following section.&lt;/p&gt;&lt;p&gt;If your Nextflow code is in a public Bitbucket repository, then you don&apos;t need to specify your credentials to pull code from the repository. However, if you wish to interact with a private repository, you need to provide elevated access to Nextflow by specifying your credentials in the &lt;code&gt;scm&lt;/code&gt; file.&lt;/p&gt;&lt;p&gt;Please note that Bitbucket Cloud requires your &lt;code&gt;app password&lt;/code&gt; in the &lt;code&gt;password&lt;/code&gt; field, which is different from your login password.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;groovy&quot;&gt;providers {

  bitbucket {
    user = &amp;#39;me&amp;#39;
    password = &amp;#39;my-app-password&amp;#39;
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate an &lt;code&gt;app password&lt;/code&gt; for the Bitbucket platform, follow the instructions provided &lt;a href=&quot;https://support.atlassian.com/bitbucket-cloud/docs/app-passwords/&quot;&gt;here&lt;/a&gt;. Ensure that the token has at least &lt;code&gt;Repositories: Read&lt;/code&gt; permission.&lt;/p&gt;&lt;p&gt;Once these settings are saved in &lt;code&gt;$HOME/.nextflow/scm&lt;/code&gt;, you can test the integration by pulling the repository code.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow pull https://bitbucket.org/user_name/private_repo
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Bitbucket Server&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.atlassian.com/software/bitbucket/enterprise&quot;&gt;Bitbucket Server&lt;/a&gt; is a Git hosting solution from Atlassian which is meant for teams that require a self-managed solution. If Nextflow code resides in an open Bitbucket repository, then you don&apos;t need to provide credentials to pull code from this repository. However, if you wish to interact with a private repository, you need to give elevated access to Nextflow by specifying your credentials in the &lt;code&gt;scm&lt;/code&gt; file.&lt;/p&gt;&lt;p&gt;For example, if you&apos;d like to call your hosted Bitbucket server as &lt;code&gt;mybitbucketserver&lt;/code&gt;, then you&apos;ll need to add the following snippet in your &lt;code&gt;~/$HOME/.nextflow/scm&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;groovy&quot;&gt;providers {

  mybitbucketserver {
    platform = &amp;#39;bitbucketserver&amp;#39;
    server = &amp;#39;https://your.bitbucket.host.com&amp;#39;
    user = &amp;#39;me&amp;#39;
    password = &amp;#39;my-password&amp;#39; // OR &amp;quot;my-token&amp;quot;
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate a &lt;em&gt;personal access token&lt;/em&gt; for Bitbucket Server, refer to the &lt;a href=&quot;https://confluence.atlassian.com/bitbucketserver/managing-personal-access-tokens-1005339986.html&quot;&gt;Bitbucket Support documentation&lt;/a&gt; from Atlassian.&lt;/p&gt;&lt;p&gt;Once the configuration is saved, you can test the integration by pulling code from a private repository and specifying the &lt;code&gt;mybitbucketserver&lt;/code&gt; Git provider using the &lt;code&gt;-hub&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow pull https://your.bitbucket.host.com/user_name/private_repo -hub mybitbucketserver
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;NOTE: It is worth noting that &lt;a href=&quot;https://www.atlassian.com/migration/assess/journey-to-cloud&quot;&gt;Atlassian is phasing out the Server offering&lt;/a&gt; in favor of cloud product &lt;a href=&quot;https://bitbucket.org&quot;&gt;bitbucket.org&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;GitLab&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://gitlab.com&quot;&gt;GitLab&lt;/a&gt; is a popular Git provider that offers features covering various aspects of the DevOps cycle.&lt;/p&gt;&lt;p&gt;If you wish to run a Nextflow pipeline from a public GitLab repository, there is no need to provide credentials to pull code. However, if you wish to interact with a private repository, then you must give elevated access to Nextflow by specifying your credentials in the &lt;code&gt;scm&lt;/code&gt; file.&lt;/p&gt;&lt;p&gt;Please note that you need to specify your &lt;em&gt;personal access token&lt;/em&gt; in the &lt;code&gt;password&lt;/code&gt; field.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;groovy&quot;&gt;providers {

  mygitlab {
    user = &amp;#39;me&amp;#39;
    password = &amp;#39;my-password&amp;#39; // or &amp;#39;my-personal-access-token&amp;#39;
    token = &amp;#39;my-personal-access-token&amp;#39;
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In addition, you can specify the &lt;code&gt;server&lt;/code&gt; fields for your self-hosted instance of GitLab, by default &lt;a href=&quot;https://gitlab.com&quot;&gt;https://gitlab.com&lt;/a&gt; is assumed as the server.&lt;/p&gt;&lt;p&gt;To generate a &lt;code&gt;personal-access-token&lt;/code&gt; for the GitLab platform follow the instructions provided &lt;a href=&quot;https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html&quot;&gt;here&lt;/a&gt;. Please ensure that the token has at least &lt;code&gt;read_repository&lt;/code&gt;, &lt;code&gt;read_api&lt;/code&gt; permissions.&lt;/p&gt;&lt;p&gt;Once the configuration is saved, you can test the integration by pulling the repository code using the &lt;code&gt;-hub&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow pull https://gitlab.com/user_name/private_repo -hub mygitlab
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Gitea&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://gitea.com/&quot;&gt;Gitea server&lt;/a&gt; is an open source Git-hosting solution that can be self-hosted. If you have your Nextflow code in an open Gitea repository, there is no need to specify credentials to pull code from this repository. However, if you wish to interact with a private repository, you can give elevated access to Nextflow by specifying your credentials in the &lt;code&gt;scm&lt;/code&gt; file.&lt;/p&gt;&lt;p&gt;For example, if you&apos;d like to call your hosted Gitea server &lt;code&gt;mygiteaserver&lt;/code&gt;, then you&apos;ll need to add the following snippet in your &lt;code&gt;~/$HOME/.nextflow/scm&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;groovy&quot;&gt;providers {

  mygiteaserver {
    platform = &amp;#39;gitea&amp;#39;
    server = &amp;#39;https://gitea.host.com&amp;#39;
    user = &amp;#39;me&amp;#39;
    password = &amp;#39;my-password&amp;#39;
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate a &lt;em&gt;personal access token&lt;/em&gt; for your Gitea server, please refer to the &lt;a href=&quot;https://docs.gitea.io/en-us/api-usage/&quot;&gt;official guide&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Once the configuration is set, you can test the integration by pulling the repository code and specifying &lt;code&gt;mygiteaserver&lt;/code&gt; as the Git provider using the &lt;code&gt;-hub&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow pull https://git.host.com/user_name/private_repo -hub mygiteaserver
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Azure Repos&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/devops/repos/&quot;&gt;Azure Repos&lt;/a&gt; is a part of Microsoft Azure Cloud Suite. Nextflow integrates natively Azure Repos via the usual &lt;code&gt;~/$HOME/.nextflow/scm&lt;/code&gt; file.&lt;/p&gt;&lt;p&gt;If you&apos;d like to use the &lt;code&gt;myazure&lt;/code&gt; alias for the &lt;code&gt;azurerepos&lt;/code&gt; provider, then you&apos;ll need to add the following snippet in your &lt;code&gt;~/$HOME/.nextflow/scm&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;groovy&quot;&gt;providers {

  myazure {
    server = &amp;#39;https://dev.azure.com&amp;#39;
    platform = &amp;#39;azurerepos&amp;#39;
    user = &amp;#39;me&amp;#39;
    token = &amp;#39;my-api-token&amp;#39;
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate a &lt;em&gt;personal access token&lt;/em&gt; for your Azure Repos integration, please refer to the &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&amp;tabs=preview-page&quot;&gt;official guide&lt;/a&gt; on Azure.&lt;/p&gt;&lt;p&gt;Once the configuration is set, you can test the integration by pulling the repository code and specifying &lt;code&gt;myazure&lt;/code&gt; as the Git provider using the &lt;code&gt;-hub&lt;/code&gt; option.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow pull https://dev.azure.com/org_name/DefaultCollection/_git/repo_name -hub myazure
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Git is a popular, widely used software system for source code management. The native integration of Nextflow with various Git hosting solutions is an important feature to facilitate reproducible workflows that enable collaborative development and deployment of Nextflow pipelines.&lt;/p&gt;&lt;p&gt;Stay tuned for more integrations as we continue to improve our support for various source code management solutions!&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Setting up a Nextflow environment on Windows 10</title>
      <link>https://www.nextflow.io/blog/2021/setup-nextflow-on-windows.html</link>
      <pubDate>Wed, 13 Oct 2021 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2021/setup-nextflow-on-windows.html</guid>
      	<description>
	&lt;p&gt;For Windows users, getting access to a Linux-based Nextflow development and runtime environment used to be hard. Users would need to run virtual machines, access separate physical servers or cloud instances, or install packages such as &lt;a href=&quot;http://www.cygwin.com/&quot;&gt;Cygwin&lt;/a&gt; or &lt;a href=&quot;https://wiki.ubuntu.com/WubiGuide&quot;&gt;Wubi&lt;/a&gt;. Fortunately, there is now an easier way to deploy a complete Nextflow development environment on Windows.&lt;/p&gt;&lt;p&gt;The Windows Subsystem for Linux (WSL) allows users to build, manage and execute Nextflow pipelines on a Windows 10 laptop or desktop without needing a separate Linux machine or cloud VM. Users can build and test Nextflow pipelines and containerized workflows locally, on an HPC cluster, or their preferred cloud service, including AWS Batch and Azure Batch.&lt;/p&gt;&lt;p&gt;This document provides a step-by-step guide to setting up a Nextflow development environment on Windows 10.&lt;/p&gt;&lt;h2&gt;High-level Steps&lt;/h2&gt;&lt;p&gt;The steps described in this guide are as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Install Windows PowerShell&lt;/li&gt;
  &lt;li&gt;Configure the Windows Subsystem for Linux (WSL2)&lt;/li&gt;
  &lt;li&gt;Obtain and Install a Linux distribution (on WSL2)&lt;/li&gt;
  &lt;li&gt;Install Windows Terminal&lt;/li&gt;
  &lt;li&gt;Install and configure Docker&lt;/li&gt;
  &lt;li&gt;Download and install an IDE (VS Code)&lt;/li&gt;
  &lt;li&gt;Install and test Nextflow&lt;/li&gt;
  &lt;li&gt;Configure X-Windows for use with the Nextflow Console&lt;/li&gt;
  &lt;li&gt;Install and Configure GIT&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Install Windows PowerShell&lt;/h2&gt;&lt;p&gt;PowerShell is a cross-platform command-line shell and scripting language available for Windows, Linux, and macOS. If you are an experienced Windows user, you are probably already familiar with PowerShell. PowerShell is worth taking a few minutes to download and install.&lt;/p&gt;&lt;p&gt;PowerShell is a big improvement over the Command Prompt in Windows 10. It brings features to Windows that Linux/UNIX users have come to expect, such as command-line history, tab completion, and pipeline functionality.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;You can obtain PowerShell for Windows from GitHub at the URL &lt;a href=&quot;https://github.com/PowerShell/PowerShell&quot;&gt;https://github.com/PowerShell/PowerShell&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Download and install the latest stable version of PowerShell for Windows x64 - e.g., &lt;a href=&quot;https://github.com/PowerShell/PowerShell/releases/download/v7.1.3/PowerShell-7.1.3-win-x64.msi&quot;&gt;powershell-7.1.3-win-x64.msi&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;If you run into difficulties, Microsoft provides detailed instructions &lt;a href=&quot;https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell-core-on-windows?view=powershell-7.1&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Configure the Windows Subsystem for Linux (WSL)&lt;/h2&gt;&lt;h3&gt;Enable the Windows Subsystem for Linux&lt;/h3&gt;&lt;p&gt;Make sure you are running Windows 10 Version 1903 with Build 18362 or higher. You can check your Windows version by select WIN-R (using the Windows key to run a command) and running the utility &lt;code&gt;winver&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;From within PowerShell, run the Windows Deployment Image and Service Manager (DISM) tool as an administrator to enable the Windows Subsystem for Linux. To run PowerShell with administrator privileges, right-click on the PowerShell icon from the Start menu or desktop and select “*Run as administrator*”.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PS C:\WINDOWS\System32&amp;gt; dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
Deployment Image Servicing and Management tool
Version: 10.0.19041.844
Image Version: 10.0.19041.1083

Enabling feature(s)
[==========================100.0%==========================]
The operation completed successfully.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can learn more about DISM &lt;a href=&quot;https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/what-is-dism&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Step 2: Enable the Virtual Machine Feature&lt;/h3&gt;&lt;p&gt;Within PowerShell, enable Virtual Machine Platform support using DISM. If you have trouble enabling this feature, make sure that virtual machine support is enabled in your machine’s BIOS.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PS C:\WINDOWS\System32&amp;gt; dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
Deployment Image Servicing and Management tool
Version: 10.0.19041.844
Image Version: 10.0.19041.1083
Enabling feature(s)
[==========================100.0%==========================]
The operation completed successfully.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After enabling the Virtual Machine Platform support, &lt;strong&gt;restart your machine&lt;/strong&gt;.&lt;/p&gt;&lt;h3&gt;Step 3: Download the Linux Kernel Update Package&lt;/h3&gt;&lt;p&gt;Nextflow users will want to take advantage of the latest features in WSL 2. You can learn about differences between WSL 1 and WSL 2 &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/compare-versions&quot;&gt;here&lt;/a&gt;. Before you can enable support for WSL 2, you’ll need to download the kernel update package at the link below:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi&quot;&gt;WSL2 Linux kernel update package for x64 machines&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Once downloaded, double click on the kernel update package and select &quot;Yes&quot; to install it with elevated permissions.&lt;/p&gt;&lt;h3&gt;STEP 4: Set WSL2 as your Default Version&lt;/h3&gt;&lt;p&gt;From within PowerShell:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PS C:\WINDOWS\System32&amp;gt; wsl --set-default-version 2
For information on key differences with WSL 2 please visit https://aka.ms/wsl2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you run into difficulties with any of these steps, Microsoft provides detailed installation instructions &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/install-win10#manual-installation-steps&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;Obtain and Install a Linux Distribution on WSL&lt;/h2&gt;&lt;p&gt;If you normally install Linux on VM environments such as VirtualBox or VMware, this probably sounds like a lot of work. Fortunately, Microsoft provides Linux OS distributions via the Microsoft Store that work with the Windows Subsystem for Linux.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Use this link to access and download a Linux Distribution for WSL through the Microsoft Store - &lt;a href=&quot;https://aka.ms/wslstore&quot;&gt;https://aka.ms/wslstore&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/ms-store.png&quot; alt=&quot;Linux Distributions at the Microsoft Store&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;We selected the Ubuntu 20.04 LTS release. You can use a different distribution if you choose. Installation from the Microsoft Store is automated. Once the Linux distribution is installed, you can run a shell on Ubuntu (or your installed OS) from the Windows Start menu.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;When you start Ubuntu Linux for the first time, you will be prompted to provide a UNIX username and password. The username that you select can be distinct from your Windows username. The UNIX user that you create will automatically have &lt;code&gt;sudo&lt;/code&gt; privileges. Whenever a shell is started, it will default to this user.&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;After setting your username and password, update your packages on Ubuntu from the Linux shell using the following command:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo apt update &amp;amp;&amp;amp; sudo apt upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;This is also a good time to add any additional Linux packages that you will want to use.&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo apt install net-tools
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Install Windows Terminal&lt;/h2&gt;&lt;p&gt;While not necessary, it is a good idea to install &lt;a href=&quot;https://github.com/microsoft/terminal&quot;&gt;Windows Terminal&lt;/a&gt; at this point. When working with Nextflow, it is handy to interact with multiple command lines at the same time. For example, users may want to execute flows, monitor logfiles, and run Docker commands in separate windows.&lt;/p&gt;&lt;p&gt;Windows Terminal provides an X-Windows-like experience on Windows. It helps organize your various command-line environments - Linux shell, Windows Command Prompt, PowerShell, AWS or Azure CLIs.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/windows-terminal.png&quot; alt=&quot;Windows Terminal&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;Instructions for downloading and installing Windows Terminal are available at: &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/terminal/get-started&quot;&gt;https://docs.microsoft.com/en-us/windows/terminal/get-started&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;It is worth spending a few minutes getting familiar with available commands and shortcuts in Windows Terminal. Documentation is available at &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/terminal/command-line-arguments&quot;&gt;https://docs.microsoft.com/en-us/windows/terminal/command-line-arguments&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Some Windows Terminal commands you’ll need right away are provided below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Split the active window vertically: &lt;strong&gt;SHIFT ALT =&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Split the active window horizontally: &lt;strong&gt;SHIFT ALT –&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Resize the active window: &lt;strong&gt;SHIFT ALT &amp;lt;arrow keys&amp;gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Open a new window under the current tab: &lt;strong&gt;ALT v&lt;/strong&gt; (*the new tab icon along the top of the Windows Terminal interface*)&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Installing Docker on Windows&lt;/h2&gt;&lt;p&gt;There are two ways to install Docker for use with the WSL on Windows. One method is to install Docker directly on a hosted WSL Linux instance (Ubuntu in our case) and have the docker daemon run on the Linux kernel as usual. An installation recipe for people that choose this “native Linux” approach is provided &lt;a href=&quot;https://dev.to/bowmanjd/install-docker-on-windows-wsl-without-docker-desktop-34m9&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;A second method is to run &lt;a href=&quot;https://www.docker.com/products/docker-desktop&quot;&gt;Docker Desktop&lt;/a&gt; on Windows. While Docker is more commonly used in Linux environments, it can be used with Windows also. The Docker Desktop supports containers running on Windows and Linux instances running under WSL. Docker Desktop provides some advantages for Windows users:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The installation process is automated&lt;/li&gt;
  &lt;li&gt;Docker Desktop provides a Windows GUI for managing Docker containers and images (including Linux containers running under WSL)&lt;/li&gt;
  &lt;li&gt;Microsoft provides Docker Desktop integration features from within Visual Studio Code via a VS Code extension&lt;/li&gt;
  &lt;li&gt;Docker Desktop provides support for auto-installing a single-node Kubernetes cluster&lt;/li&gt;
  &lt;li&gt;The Docker Desktop WSL 2 back-end provides an elegant Linux integration such that from a Linux user’s perspective, Docker appears to be running natively on Linux.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;An explanation of how the Docker Desktop WSL 2 Back-end works is provided &lt;a href=&quot;https://www.docker.com/blog/new-docker-desktop-wsl2-backend/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Step 1: Install Docker Desktop on Windows&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Download and install Docker Desktop for Windows from the following link: &lt;a href=&quot;https://desktop.docker.com/win/stable/amd64/Docker%20Desktop%20Installer.exe&quot;&gt;https://desktop.docker.com/win/stable/amd64/Docker%20Desktop%20Installer.exe&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Follow the on-screen prompts provided by the Docker Desktop Installer. The installation process will install Docker on Windows and install the Docker back-end components so that Docker commands are accessible from within WSL.&lt;/li&gt;
  &lt;li&gt;After installation, Docker Desktop can be run from the Windows start menu. The Docker Desktop user interface is shown below. Note that Docker containers launched under WSL can be managed from the Windows Docker Desktop GUI or Linux command line.&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The installation process is straightforward, but if you run into difficulties, detailed instructions are available &lt;a href=&quot;https://docs.docker.com/docker-for-windows/install/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/docker-images.png&quot; alt=&quot;Nextflow Visual Studio Code Extension&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The Docker Engineering team provides an architecture diagram explaining how Docker on Windows interacts with WSL. Additional details are available &lt;a href=&quot;https://code.visualstudio.com/blogs/2020/03/02/docker-in-wsl2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;![Nextflow Visual Studio Code Extension](/img/docker-windows-arch.png)
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Step 2: Verify the Docker installation&lt;/h3&gt;&lt;p&gt;Now that Docker is installed, run a Docker container to verify that Docker and the Docker Integration Package on WSL 2 are working properly.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Run a Docker command from the Linux shell as shown below below. This command downloads a &lt;strong&gt;centos&lt;/strong&gt; image from Docker Hub and allows us to interact with the container via an assigned pseudo-tty. Your Docker container may exit with exit code 139 when you run this and other Docker containers. If so, don’t worry – an easy fix to this issue is provided shortly.&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ docker run -ti centos:6
[root@02ac0beb2d2c /]# hostname
02ac0beb2d2c
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;You can run Docker commands in other Linux shell windows via the Windows Terminal environment to monitor and manage Docker containers and images. For example, running &lt;code&gt;docker ps&lt;/code&gt; in another window shows the running CentOS Docker container.&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID IMAGE   COMMAND   CREATED    STATUS    NAMES
f5dad42617f1 centos:6 &amp;quot;/bin/bash&amp;quot; 2 minutes ago Up 2 minutes    happy_hopper
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 3: Dealing with exit code 139&lt;/h3&gt;&lt;p&gt;You may encounter exit code 139 when running Docker containers. This is a known problem when running containers with specific base images within Docker Desktop. Good explanations of the problem and solution are provided &lt;a href=&quot;https://dev.to/damith/docker-desktop-container-crash-with-exit-code-139-on-windows-wsl-fix-438&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://unix.stackexchange.com/questions/478387/running-a-centos-docker-image-on-arch-linux-exits-with-code-139&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The solution is to add two lines to a &lt;code&gt;.wslconfig&lt;/code&gt; file in your Windows home directory. The &lt;code&gt;.wslconfig&lt;/code&gt; file specifies kernel options that apply to all Linux distributions running under WSL 2.&lt;/p&gt;&lt;p&gt;Some of the Nextflow container images served from Docker Hub are affected by this bug since they have older base images, so it is a good idea to apply this fix.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Edit the &lt;code&gt;.wslconfig&lt;/code&gt; file in your Windows home directory. You can do this using PowerShell as shown:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;PS C:\Users\&amp;lt;username&amp;gt; notepad .wslconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Add these two lines to the .wslconfig file and save it:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;[wsl2]
kernelCommandLine = vsyscall=emulate
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;After this, &lt;strong&gt;restart your machine&lt;/strong&gt; to force a restart of the Docker and WSL 2 environment. After making this correction, you should be able to launch containers without seeing exit code 139.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Install Visual Studio Code as your IDE (optional)&lt;/h2&gt;&lt;p&gt;Developers can choose from a variety of IDEs depending on their preferences. Some examples of IDEs and developer-friendly editors are below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Visual Studio Code - &lt;a href=&quot;https://code.visualstudio.com/Download&quot;&gt;https://code.visualstudio.com/Download&lt;/a&gt; (Nextflow VSCode Language plug-in &lt;a href=&quot;https://github.com/nextflow-io/vscode-language-nextflow/blob/master/vsc-extension-quickstart.md&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Eclipse - &lt;a href=&quot;https://www.eclipse.org/&quot;&gt;https://www.eclipse.org/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;VIM - &lt;a href=&quot;https://www.vim.org/&quot;&gt;https://www.vim.org/&lt;/a&gt; (VIM plug-in for Nextflow &lt;a href=&quot;https://github.com/LukeGoodsell/nextflow-vim&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Emacs - &lt;a href=&quot;https://www.gnu.org/software/emacs/download.html&quot;&gt;https://www.gnu.org/software/emacs/download.html&lt;/a&gt; (Nextflow syntax highlighter &lt;a href=&quot;https://github.com/Emiller88/nextflow-mode&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;JetBrains PyCharm - &lt;a href=&quot;https://www.jetbrains.com/pycharm/&quot;&gt;https://www.jetbrains.com/pycharm/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;IntelliJ IDEA - &lt;a href=&quot;https://www.jetbrains.com/idea/&quot;&gt;https://www.jetbrains.com/idea/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Atom – &lt;a href=&quot;https://atom.io/&quot;&gt;https://atom.io/&lt;/a&gt; (Nextflow Atom support available &lt;a href=&quot;https://atom.io/packages/language-nextflow&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Notepad++ - &lt;a href=&quot;https://notepad-plus-plus.org/&quot;&gt;https://notepad-plus-plus.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We decided to install Visual Studio Code because it has some nice features, including: * Support for source code control from within the IDE (Git) * Support for developing on Linux via its WSL 2 Video Studio Code Backend * A library of extensions including Docker and Kubernetes support and extensions for Nextflow, including Nextflow language support and an &lt;a href=&quot;https://github.com/nf-core/vscode-extensionpack&quot;&gt;extension pack for the nf-core community&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Download Visual Studio Code from &lt;a href=&quot;https://code.visualstudio.com/Download&quot;&gt;https://code.visualstudio.com/Download&lt;/a&gt; and follow the installation procedure. The installation process will detect that you are running WSL. You will be invited to download and install the Remote WSL extension.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Within VS Code and other Windows tools, you can access the Linux file system under WSL 2 by accessing the path &lt;code&gt;\\wsl$\&amp;lt;linux-environment&amp;gt;&lt;/code&gt;. In our example, the path from Windows to access files from the root of our Ubuntu Linux instance is: &lt;a href=&quot;file://wsl$/Ubuntu-20.04&quot;&gt;&lt;strong&gt;\wsl$\Ubuntu-20.04&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Note that the reverse is possible also – from within Linux, &lt;code&gt;/mnt/c&lt;/code&gt; maps to the Windows C: drive. You can inspect &lt;code&gt;/etc/mtab&lt;/code&gt; to see the mounted file systems available under Linux.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;It is a good idea to install Nextflow language support in VS Code. You can do this by selecting the Extensions icon from the left panel of the VS Code interface and searching the extensions library for Nextflow as shown. The Nextflow language support extension is on GitHub at &lt;a href=&quot;https://github.com/nextflow-io/vscode-language-nextflow&quot;&gt;https://github.com/nextflow-io/vscode-language-nextflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/nf-vscode-ext.png&quot; alt=&quot;Nextflow Visual Studio Code Extension&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Visual Studio Code Remote Development&lt;/h2&gt;&lt;p&gt;Visual Studio Code Remote Development supports development on remote environments such as containers or remote hosts. For Nextflow users, it is important to realize that VS Code sees the Ubuntu instance we installed on WSL as a remote environment. The Diagram below illustrates how remote development works. From a VS Code perspective, the Linux instance in WSL is considered a remote environment.&lt;/p&gt;&lt;p&gt;Windows users work within VS Code in the Windows environment. However, source code, developer tools, and debuggers all run Linux on WSL, as illustrated below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[The Remote Development Environment in VS Code](/img/vscode-remote-dev.png)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;An explanation of how VS Code Remote Development works is provided &lt;a href=&quot;https://code.visualstudio.com/docs/remote/remote-overview&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;VS Code users see the Windows filesystem, plug-ins specific to VS Code on Windows, and access Windows versions of tools such as Git. If you prefer to develop in Linux, you will want to select WSL as the remote environment.&lt;/p&gt;&lt;p&gt;To open a new VS Code Window running in the context of the WSL Ubuntu-20.04 environment, click the green icon at the lower left of the VS Code window and select “New WSL Window using Distro ..” and select Ubuntu 20.04. You’ll notice that the environment changes to show that you are working in the WSL: Ubuntu-20.04 environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;![Selecting the Remote Dev Environment within VS Code](/img/remote-dev-side-by-side.png)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Selecting the Extensions icon, you can see that different VS Code Marketplace extensions run in different contexts. The Nextflow Language extension installed in the previous step is globally available. It works when developing on Windows or developing on WSL: Ubuntu-20.04.&lt;/p&gt;&lt;p&gt;The Extensions tab in VS Code differentiates between locally installed plug-ins and those installed under WSL.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;![Local vs. Remote Extensions in VS Code](/img/vscode-extensions.png)
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Installing Nextflow&lt;/h2&gt;&lt;p&gt;With Linux, Docker, and an IDE installed, now we can install Nextflow in our WSL 2 hosted Linux environment. Detailed instructions for installing Nextflow are available at &lt;a href=&quot;https://www.nextflow.io/docs/latest/getstarted.html#installation&quot;&gt;https://www.nextflow.io/docs/latest/getstarted.html#installation&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Step 1: Make sure Java is installed (under WSL)&lt;/h3&gt;&lt;p&gt;Java is a prerequisite for running Nextflow. Instructions for installing Java on Ubuntu are available &lt;a href=&quot;https://linuxize.com/post/install-java-on-ubuntu-18-04/&quot;&gt;here&lt;/a&gt;. To install the default OpenJDK, follow the instructions below in a Linux shell window:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Update the &lt;em&gt;apt&lt;/em&gt; package index:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Install the latest default OpenJDK package&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo apt install default-jdk
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Verify the installation&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ java -version
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 2: Make sure curl is installed&lt;/h3&gt;&lt;p&gt;&lt;code&gt;curl&lt;/code&gt; is a convenient way to obtain Nextflow. &lt;code&gt;curl&lt;/code&gt; is included in the default Ubuntu repositories, so installation is straightforward. &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;From the shell:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo apt update
$ sudo apt install curl
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Verify that &lt;code&gt;curl&lt;/code&gt; works:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ curl
curl: try &amp;#39;curl --help&amp;#39; or &amp;#39;curl --manual&amp;#39; for more information
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;STEP 3: Download and install Nextflow&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Use &lt;code&gt;curl&lt;/code&gt; to retrieve Nextflow into a temporary directory and then install it in &lt;code&gt;/usr/bin&lt;/code&gt; so that the Nextflow command is on your path:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ mkdir temp
$ cd temp
$ curl -s https://get.nextflow.io | bash
$ sudo cp nextflow /usr/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Make sure that Nextflow is executable:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo chmod 755 /usr/bin/nextflow
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;or if you prefer:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo chmod +x /usr/bin/nextflow
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 4: Verify the Nextflow installation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Make sure Nextflow runs:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ nextflow -version

    N E X T F L O W
    version 21.04.2 build 5558
    created 12-07-2021 07:54 UTC (03:54 EDT)
    cite doi:10.1038/nbt.3820
    http://nextflow.io
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Run a simple Nextflow pipeline. The example below downloads and executes a sample hello world pipeline from GitHub - &lt;a href=&quot;https://github.com/nextflow-io/hello&quot;&gt;https://github.com/nextflow-io/hello&lt;/a&gt;.&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ nextflow run hello
N E X T F L O W ~ version 21.04.2
Launching `nextflow-io/hello` [distracted_pare] - revision: ec11eb0ec7 [master]
executor &amp;gt; local (4)
[06/c846d8] process &amp;gt; sayHello (3) [100%] 4 of 4 ✔
Ciao world!

Hola world!

Bonjour world!

Hello world!
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 5: Run a Containerized Workflow&lt;/h3&gt;&lt;p&gt;To validate that Nextflow works with containerized workflows, we can run a slightly more complicated example. A sample workflow involving NCBI Blast is available at &lt;a href=&quot;https://github.com/nextflow-io/blast-example&quot;&gt;https://github.com/nextflow-io/blast-example&lt;/a&gt;. Rather than installing Blast on our local Linux instance, it is much easier to pull a container preloaded with Blast and other software that the pipeline depends on.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;nextflow.config&lt;/code&gt; file for the Blast example (below) specifies that process logic is encapsulated in the container &lt;code&gt;nextflow/examples&lt;/code&gt; available from Docker Hub (&lt;a href=&quot;https://hub.docker.com/r/nextflow/examples)&quot;&gt;https://hub.docker.com/r/nextflow/examples)&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;On GitHub: &lt;a href=&quot;https://github.com/nextflow-io/blast-example/blob/master/nextflow.config&quot;&gt;nextflow-io/blast-example/nextflow.config&lt;/a&gt;&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;manifest {
nextflowVersion = &amp;#39;&amp;gt;= 20.01.0&amp;#39;
}

process {
container = &amp;#39;nextflow/examples&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Run the &lt;em&gt;blast-example&lt;/em&gt; pipeline that resides on GitHub directly from WLS and specify Docker as the container runtime using the command below:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ nextflow run blast-example -with-docker
N E X T F L O W ~ version 21.04.2
Launching `nextflow-io/blast-example` [sharp_raman] - revision: 25922a0ae6 [master]
executor &amp;gt; local (2)
[aa/a9f056] process &amp;gt; blast (1)  [100%] 1 of 1 ✔
[b3/c41401] process &amp;gt; extract (1) [100%] 1 of 1 ✔
matching sequences:
&amp;gt;lcl|1ABO:B unnamed protein product
MNDPNLFVALYDFVASGDNTLSITKGEKLRVLGYNHNGEWCEAQTKNGQGWVPSNYITPVNS
&amp;gt;lcl|1ABO:A unnamed protein product
MNDPNLFVALYDFVASGDNTLSITKGEKLRVLGYNHNGEWCEAQTKNGQGWVPSNYITPVNS
&amp;gt;lcl|1YCS:B unnamed protein product
PEITGQVSLPPGKRTNLRKTGSERIAHGMRVKFNPLPLALLLDSSLEGEFDLVQRIIYEVDDPSLPNDEGITALHNAVCA
GHTEIVKFLVQFGVNVNAADSDGWTPLHCAASCNNVQVCKFLVESGAAVFAMTYSDMQTAADKCEEMEEGYTQCSQFLYG
VQEKMGIMNKGVIYALWDYEPQNDDELPMKEGDCMTIIHREDEDEIEWWWARLNDKEGYVPRNLLGLYPRIKPRQRSLA
&amp;gt;lcl|1IHD:C unnamed protein product
LPNITILATGGTIAGGGDSATKSNYTVGKVGVENLVNAVPQLKDIANVKGEQVVNIGSQDMNDNVWLTLAKKINTDCDKT
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
  &lt;li&gt;Nextflow executes the pipeline directly from the GitHub repository and automatically pulls the nextflow/examples container from Docker Hub if the image is unavailable locally. The pipeline then executes the two containerized workflow steps (blast and extract). The pipeline then collects the sequences into a single file and prints the result file content when pipeline execution completes.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Configuring an XServer for the Nextflow Console&lt;/h2&gt;&lt;p&gt;Pipeline developers will probably want to use the Nextflow Console at some point. The Nextflow Console’s REPL (read-eval-print loop) environment allows developers to quickly test parts of scripts or Nextflow code segments interactively.&lt;/p&gt;&lt;p&gt;The Nextflow Console is launched from the Linux command line. However, the Groovy-based interface requires an X-Windows environment to run. You can set up X-Windows with WSL using the procedure below. A good article on this same topic is provided &lt;a href=&quot;https://medium.com/javarevisited/using-wsl-2-with-x-server-linux-on-windows-a372263533c3&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Download an X-Windows server for Windows. In this example, we use the &lt;em&gt;VcXsrv Windows X Server&lt;/em&gt; available from source forge at &lt;a href=&quot;https://sourceforge.net/projects/vcxsrv/&quot;&gt;https://sourceforge.net/projects/vcxsrv/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Accept all the defaults when running the automated installer. The X-server will end up installed in &lt;code&gt;c:\Program Files\VcXsrv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The automated installation of VcXsrv will create an “XLaunch” shortcut on your desktop. It is a good idea to create your own shortcut with a customized command line so that you don’t need to interact with the XLaunch interface every time you start the X-server.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Right-click on the Windows desktop to create a new shortcut, give it a meaningful name, and insert the following for the shortcut target:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;&amp;quot;C:\Program Files\VcXsrv\vcxsrv.exe&amp;quot; :0 -ac -terminate -lesspointer -multiwindow -clipboard -wgl -dpi auto
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Inspecting the new shortcut properties, it should look something like this:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/xserver.png&quot; alt=&quot;X-Server (vcxsrc) Properties&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Double-click on the new shortcut desktop icon to test it. Unfortunately, the X-server runs in the background. When running the X-server in multiwindow mode (which we recommend), it is not obvious whether the X-server is running.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;One way to check that the X-server is running is to use the Microsoft Task Manager and look for the XcSrv process running in the background. You can also verify it is running by using the &lt;code&gt;netstat&lt;/code&gt; command from with PowerShell on Windows to ensure that the X-server is up and listening on the appropriate ports. Using &lt;code&gt;netstat&lt;/code&gt;, you should see output like the following:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;PS C:\WINDOWS\system32&amp;gt; **netstat -abno | findstr 6000**
 TCP  0.0.0.0:6000      0.0.0.0:0       LISTENING    35176
 TCP  127.0.0.1:6000     127.0.0.1:56516    ESTABLISHED   35176
 TCP  127.0.0.1:6000     127.0.0.1:56517    ESTABLISHED   35176
 TCP  127.0.0.1:6000     127.0.0.1:56518    ESTABLISHED   35176
 TCP  127.0.0.1:56516    127.0.0.1:6000     ESTABLISHED   35176
 TCP  127.0.0.1:56517    127.0.0.1:6000     ESTABLISHED   35176
 TCP  127.0.0.1:56518    127.0.0.1:6000     ESTABLISHED   35176
 TCP  172.28.192.1:6000   172.28.197.205:46290  TIME_WAIT    0
 TCP  [::]:6000       [::]:0         LISTENING    35176
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;At this point, the X-server is up and running and awaiting a connection from a client.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Within Ubuntu in WSL, we need to set up the environment to communicate with the X-Windows server. The shell variable DISPLAY needs to be set pointing to the IP address of the X-server and the instance of the X-windows server.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The shell script below will set the DISPLAY variable appropriately and export it to be available to X-Windows client applications launched from the shell. This scripting trick works because WLS sees the Windows host as the nameserver and this is the same IP address that is running the X-Server. You can echo the $DISPLAY variable after setting it to verify that it is set correctly.&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk &amp;#39;{print $2}&amp;#39;):0.0
$ echo $DISPLAY
172.28.192.1:0.0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Add this command to the end of your &lt;code&gt;.bashrc&lt;/code&gt; file in the Linux home directory to avoid needing to set the DISPLAY variable every time you open a new window. This way, if the IP address of the desktop or laptop changes, the DISPLAY variable will be updated accordingly.&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ cd ~
$ vi .bashrc
..
# set the X-Windows display to connect to VcXsrv on Windows
export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk &amp;#39;{print $2}&amp;#39;):0.0
&amp;quot;.bashrc&amp;quot; 120L, 3912C written           
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Use an X-windows client to make sure that the X- server is working. Since X-windows clients are not installed by default, download an xterm client as follows via the Linux shell:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ sudo apt install xterm
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Assuming that the X-server is up and running on Windows, and the Linux DISPLAY variable is set correctly, you’re ready to test X-Windows.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Before testing X-Windows, do yourself a favor and temporarily disable the Windows Firewall. The Windows Firewall will very likely block ports around 6000, preventing client requests on WSL from connecting to the X-server. You can find this under Firewall &amp;amp; network protection on Windows. Clicking the “Private Network” or “Public Network” options will show you the status of the Windows Firewall and indicate whether it is on or off.&lt;/p&gt;&lt;p&gt;Depending on your installation, you may be running a specific Firewall. In this example, we temporarily disable the McAfee LiveSafe Firewall as shown:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;![Ensure that the Firewall is not interfering](/img/firewall.png)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;With the Firewall disabled, you can attempt to launch the xterm client from the Linux shell:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ xterm &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;If everything is working correctly, you should see the new xterm client appear under Windows. The xterm is executing on Ubuntu under WSL but displays alongside other Windows on the Windows desktop. This is what is meant by “multiwindow” mode.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/xterm.png&quot; alt=&quot;Launch an xterm to verify functionality&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Now that you know X-Windows is working correctly turn the Firewall back on, and adjust the settings to allow traffic to and from the required port. Ideally, you want to open only the minimal set of ports and services required. In the case of the McAfee Firewall, getting X-Windows to work required changing access to incoming and outgoing ports to “Open ports to Work and Home networks” for the vcxsrv.exe program only as shown:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/xserver_setup.png&quot; alt=&quot;Allowing access to XServer traffic&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;With the X-server running, the DISPLAY variable set, and the Windows Firewall configured correctly, we can now launch the Nextflow Console from the shell as shown:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ nextflow console
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The command above opens the Nextflow REPL console under X-Windows.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/repl_console.png&quot; alt=&quot;Nextflow REPL Console under X-Windows&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Inside the Nextflow console, you can enter Groovy code and run it interactively, a helpful feature when developing and debugging Nextflow pipelines.&lt;/p&gt;&lt;h1&gt;Installing Git&lt;/h1&gt;&lt;p&gt;Collaborative source code management systems such as BitBucket, GitHub, and GitLab are used to develop and share Nextflow pipelines. To be productive with Nextflow, you will want to install Git.&lt;/p&gt;&lt;p&gt;As explained earlier, VS Code operates in different contexts. When running VS Code in the context of Windows, VS Code will look for a local copy of Git. When using VS Code to operate against the remote WSL environment, a separate installation of Git installed on Ubuntu will be used. (Note that Git is installed by default on Ubuntu 20.04)&lt;/p&gt;&lt;p&gt;Developers will probably want to use Git both from within a Windows context and a Linux context, so we need to make sure that Git is present in both environments.&lt;/p&gt;&lt;h3&gt;Step 1: Install Git on Windows (optional)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Download the install the 64-bit Windows version of Git from &lt;a href=&quot;https://git-scm.com/downloads&quot;&gt;https://git-scm.com/downloads&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Click on the Git installer from the Downloads directory, and click through the default installation options. During the install process, you will be asked to select the default editor to be used with Git. (VIM, Notepad++, etc.). Select Visual Studio Code (assuming that this is the IDE that you plan to use for Nextflow).&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/git-install.png&quot; alt=&quot;Installing Git on Windows&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The Git installer will prompt you for additional settings. If you are not sure, accept the defaults. When asked, adjust the PATH variable to use the recommended option, making the Git command line available from Git Bash, the Command Prompt, and PowerShell.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;After installation Git Bash, Git GUI, and GIT CMD will appear as new entries under the Start menu. If you are running Git from PowerShell, you will need to open a new Windows to force PowerShell to reset the path variable. By default, Git installs in C:\Program Files\Git.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;If you plan to use Git from the command line, GitHub provides a useful cheatsheet &lt;a href=&quot;https://training.github.com/downloads/github-git-cheat-sheet.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;After installing Git, from within VS Code (in the context of the local host), select the Source Control icon from the left pane of the VS Code interface as shown. You can open local folders that contain a git repository or clone repositories from GitHub or your preferred source code management system.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/git-vscode.png&quot; alt=&quot;Using Git within VS Code&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Documentation on using Git with Visual Studio Code is provided at &lt;a href=&quot;https://code.visualstudio.com/docs/editor/versioncontrol&quot;&gt;https://code.visualstudio.com/docs/editor/versioncontrol&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 2: Install Git on Linux&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Open a Remote VS Code Window on &lt;strong&gt;*WSL: Ubuntu 20.04*&lt;/strong&gt; (By selecting the green icon on the lower-left corner of the VS code interface.)&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Git should already be installed in &lt;code&gt;/usr/bin&lt;/code&gt;, but you can validate this from the Ubuntu shell:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ git --version
git version 2.25.1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;To get started using Git with VS Code Remote on WSL, select the &lt;em&gt;Source Control icon&lt;/em&gt; on the left panel of VS code. Assuming VS Code Remote detects that Git is installed on Linux, you should be able to &lt;em&gt;Clone a Repository&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Select “Clone Repository,” and when prompted, clone the GitHub repo for the Blast example that we used earlier - &lt;a href=&quot;https://github.com/nextflow-io/blast-example&quot;&gt;https://github.com/nextflow-io/blast-example&lt;/a&gt;. Clone this repo into your home directory on Linux. You should see &lt;em&gt;blast-example&lt;/em&gt; appear as a source code repository within VS code as shown:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/git-linux-1.png&quot; alt=&quot;Using Git within VS Code&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Select the &lt;em&gt;Explorer&lt;/em&gt; panel in VS Code to see the cloned &lt;em&gt;blast-example&lt;/em&gt; repo. Now we can explore and modify the pipeline code using the IDE.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/git-linux-2.png&quot; alt=&quot;Using Git within VS Code&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;After making modifications to the pipeline, we can execute the &lt;em&gt;local copy&lt;/em&gt; of the pipeline either from the Linux shell or directly via the Terminal window in VS Code as shown:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/git-linux-3.png&quot; alt=&quot;Using Git within VS Code&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;With the Docker VS Code extension, users can select the Docker icon from the left code to view containers and images associated with the Nextflow pipeline.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Git commands are available from within VS Code by selecting the &lt;em&gt;Source Control&lt;/em&gt; icon on the left panel and selecting the three dots (…) to the right of SOURCE CONTROL. Some operations such as pushing or committing code will require that VS Code be authenticated with your GitHub credentials.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/git-linux-4.png&quot; alt=&quot;Using Git within VS Code&quot;&quot;/&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Summary&lt;/h2&gt;&lt;p&gt;With WSL2, Windows 10 is an excellent environment for developing and testing Nextflow pipelines. Users can take advantage of the power and convenience of a Linux command line environment while using Windows-based IDEs such as VS-Code with full support for containers.&lt;/p&gt;&lt;p&gt;Pipelines developed in the Windows environment can easily be extended to compute environments in the cloud.&lt;/p&gt;&lt;p&gt;While installing Nextflow itself is straightforward, installing and testing necessary components such as WSL, Docker, an IDE, and Git can be a little tricky. Hopefully readers will find this guide helpful.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Introducing Nextflow support for SQL databases</title>
      <link>https://www.nextflow.io/blog/2021/nextflow-sql-support.html</link>
      <pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2021/nextflow-sql-support.html</guid>
      	<description>
	&lt;p&gt;The recent tweet introducing the &lt;a href=&quot;https://twitter.com/PaoloDiTommaso/status/1433120149888974854&quot;&gt;Nextflow support for SQL databases&lt;/a&gt; raised a lot of positive reaction. In this post, I want to describe more in detail how this extension works. &lt;/p&gt;&lt;p&gt;Nextflow was designed with the idea to streamline the deployment of complex data pipelines in a scalable, portable and reproducible manner across different computing platforms. To make this all possible, it was decided the resulting pipeline and the runtime should be self-contained i.e. to not depend on separate services such as database servers. &lt;/p&gt;&lt;p&gt;This makes the resulting pipelines easier to configure, deploy, and allows for testing them using &lt;a href=&quot;https://en.wikipedia.org/wiki/Continuous_integration&quot;&gt;CI services&lt;/a&gt;, which is a critical best practice for delivering high-quality and stable software.&lt;/p&gt;&lt;p&gt;Another important consequence is that Nextflow pipelines do not retain the pipeline state on separate storage. Said in a different way, the idea was - and still is - to promote stateless pipeline execution in which the computed results are only determined by the pipeline inputs and the code itself, which is consistent with the &lt;em&gt;functional&lt;/em&gt; dataflow paradigm on which Nextflow is based. &lt;/p&gt;&lt;p&gt;However, the ability to access SQL data sources can be very useful in data pipelines, for example, to ingest input metadata or to store task executions logs. &lt;/p&gt;&lt;h3&gt;How does it work?&lt;/h3&gt;&lt;p&gt;The support for SQL databases in Nextflow is implemented as an optional plugin component. This plugin provides two new operations into your Nextflow script: &lt;/p&gt;&lt;p&gt;1) &lt;code&gt;fromQuery&lt;/code&gt; performs a SQL query against the specified database and returns a Nextflow channel emitting them. This channel can be used in your pipeline as any other Nextflow channel to trigger the process execution with the corresponding values. 2) &lt;code&gt;sqlInsert&lt;/code&gt; takes the values emitted by a Nextflow channel and inserts them into a database table. &lt;/p&gt;&lt;p&gt;The plugin supports out-of-the-box popular database servers such as MySQL, PostgreSQL and MariaDB. It should be noted that the technology is based on the Java JDBC database standard, therefore it could easily support any database technology implementing a driver for this standard interface. &lt;/p&gt;&lt;p&gt;Disclaimer: This plugin is a preview technology. Some features, syntax and configuration settings can change in future releases.&lt;/p&gt;&lt;h3&gt;Let&apos;s get started!&lt;/h3&gt;&lt;p&gt;The use of the SQL plugin requires the use of Nextflow 21.08.0-edge or later. If are using an older version, check &lt;a href=&quot;https://www.nextflow.io/docs/latest/getstarted.html#stable-edge-releases&quot;&gt;this page&lt;/a&gt; on how to update to the latest edge release.&lt;/p&gt;&lt;p&gt;To enable the use of the database plugin, add the following snippet in your pipeline configuration file. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plugins {
  id &amp;#39;nf-sqldb@0.1.0&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It is then required to specify the connection &lt;em&gt;coordinates&lt;/em&gt; of the database service you want to connect to in your pipeline. This is done by adding a snippet similar to the following in your configuration file: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sql {
    db {
        &amp;#39;my-db&amp;#39; {
              url = &amp;#39;jdbc:mysql://localhost:3306/demo&amp;#39;
              user = &amp;#39;my-user&amp;#39;
              password = &amp;#39;my-password&amp;#39;
            }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the above example, replace &lt;code&gt;my-db&lt;/code&gt; with a name of your choice (this name will be used in the script to reference the corresponding database connection coordinates). Also, provide a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt; matching your database server. &lt;/p&gt;&lt;p&gt;Your script should then look like the following: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow.enable.dsl=2

process myProcess {
  input:
    tuple val(sample_id), path(sample_in) 
  output:
    tuple val(sample_id), path(&amp;#39;sample.out&amp;#39;) 

  &amp;quot;&amp;quot;&amp;quot;
  your_command --input $sample_id &amp;gt; sample.out
  &amp;quot;&amp;quot;&amp;quot;
}

workflow {

  query = &amp;#39;select SAMPLE_ID, SAMPLE_FILE from SAMPLES&amp;#39;
  channel.sql.fromQuery(query, db: &amp;#39;my-db&amp;#39;) \
    | myProcess \
    | sqlInsert(table: &amp;#39;RESULTS&amp;#39;, db: &amp;#39;my-db&amp;#39;)

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above example shows how to perform a simple database query, pipe the results to a fictitious process named &lt;code&gt;myProcess&lt;/code&gt; and finally store the process outputs into a database table named &lt;code&gt;RESULTS&lt;/code&gt;. &lt;/p&gt;&lt;p&gt;It is worth noting that Nextflow allows the use of any number of database instances in your pipeline, simply defining them in the configuration file using the syntax shown above. This could be useful to fetch database data from one data source and store the results into a different one. &lt;/p&gt;&lt;p&gt;Also, this makes it straightforward to write &lt;a href=&quot;https://en.wikipedia.org/wiki/Extract,_transform,_load&quot;&gt;ETL&lt;/a&gt; scripts that span across multiple data sources. &lt;/p&gt;&lt;p&gt;Find more details about the SQL plugin for Nextflow at &lt;a href=&quot;https://github.com/nextflow-io/nf-sqldb&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;What about the self-contained property?&lt;/h2&gt;&lt;p&gt;You may wonder if adding this capability breaks the self-contained property of Nextflow pipelines which allows them to be run in a single command and to be tested with continuous integration services e.g. GitHub Action. &lt;/p&gt;&lt;p&gt;The good news is that it does not ... or at least it should not if used properly. &lt;/p&gt;&lt;p&gt;In fact, the SQL plugin includes the &lt;a href=&quot;http://www.h2database.com/html/features.html&quot;&gt;H2&lt;/a&gt; embedded in-memory SQL database that is used by default when no other database is provided in the Nextflow configuration file and can be used for developing and testing your pipeline without the need for a separate database service. &lt;/p&gt;&lt;p&gt;Tip: Other than this, H2 also provides the capability to access and query CSV/TSV files as SQL tables. Read more about this feature at &lt;a href=&quot;http://www.h2database.com/html/tutorial.html?highlight=csv&amp;search=csv#csv&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The use of this plugin adds to Nextflow the capability to query and store data into the SQL databases. Currently, the most popular SQL technologies are supported such as MySQL, PostgreSQL and MariaDB. In the future, support for other database technologies e.g. MongoDB, DynamoDB could be added. &lt;/p&gt;&lt;p&gt;Notably, the support for SQL data-stores has been implemented preserving the core Nextflow capabilities to allow portable and self-contained pipeline scripts that can be developed locally, tested through CI services, and deployed at scale into production environments. &lt;/p&gt;&lt;p&gt;If you have any questions or suggestions, please feel free to comment in the project discussion group at &lt;a href=&quot;https://github.com/nextflow-io/nf-sqldb/discussions&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Credits to &lt;a href=&quot;https://twitter.com/fstrozzi&quot;&gt;Francesco Strozzi&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://twitter.com/bonnalr&quot;&gt;Raoul J.P. Bonnal&lt;/a&gt; for having contributed to this work 🙏.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Five more tips for Nextflow user on HPC</title>
      <link>https://www.nextflow.io/blog/2021/5-more-tips-for-nextflow-user-on-hpc.html</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2021/5-more-tips-for-nextflow-user-on-hpc.html</guid>
      	<description>
	&lt;p&gt;In May we blogged about &lt;a href=&quot;/blog/2021/5_tips_for_hpc_users.html&quot;&gt;Five Nextflow Tips for HPC Users&lt;/a&gt; and now we continue the series with five additional tips for deploying Nextflow with on HPC batch schedulers.&lt;/p&gt;&lt;h3&gt;1. Use the scratch directive&lt;/h3&gt;&lt;p&gt;To allow the pipeline tasks to share data with each other, Nextflow requires a shared file system path as a working directory. When using this model, a common recommendation is to use the node&apos;s local scratch storage as the job working directory to avoid unnecessary use of the network shared file system and achieve better performance.&lt;/p&gt;&lt;p&gt;Nextflow implements this best-practice which can be enabled by adding the following setting in your &lt;code&gt;nextflow.config&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process.scratch = true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When using this option, Nextflow: * Creates a unique directory in the computing node&apos;s local &lt;code&gt;/tmp&lt;/code&gt; or the path assigned by your cluster via the &lt;code&gt;TMPDIR&lt;/code&gt; environment variable. * Creates a &lt;a href=&quot;https://en.wikipedia.org/wiki/Symbolic_link&quot;&gt;symlink&lt;/a&gt; for each input file required by the job execution. * Runs the job in the local scratch path. Copies the job output files into the job shared work directory assigned by Nextflow.&lt;/p&gt;&lt;h3&gt;2. Use -bg option to launch the execution in the background&lt;/h3&gt;&lt;p&gt;In some circumstances, you may need to run your Nextflow pipeline in the background without losing the execution output. In this scenario use the &lt;code&gt;-bg&lt;/code&gt; command line option as shown below. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run &amp;lt;pipeline&amp;gt; -bg &amp;gt; my-file.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This can be very useful when launching the execution from an SSH connected terminal and ensures that any connection issues don&apos;t stop the pipeline. You can use &lt;code&gt;ps&lt;/code&gt; and &lt;code&gt;kill&lt;/code&gt; to find and stop the execution.&lt;/p&gt;&lt;h3&gt;3. Disable interactive logging&lt;/h3&gt;&lt;p&gt;Nextflow has rich terminal logging which uses ANSI escape codes to update the pipeline execution counters interactively. However, this is not very useful when submitting the pipeline execution as a cluster job or in the background. In this case, disable the rich ANSI logging using the command line option &lt;code&gt;-ansi-log false&lt;/code&gt; or the environment variable &lt;code&gt;NXF_ANSI_LOG=false&lt;/code&gt;.&lt;/p&gt;&lt;h3&gt;4. Cluster native options&lt;/h3&gt;&lt;p&gt;Nextlow has portable directives for common resource requests such as &lt;a href=&quot;https://www.nextflow.io/docs/latest/process.html#cpus&quot;&gt;cpus&lt;/a&gt;, &lt;a href=&quot;https://www.nextflow.io/docs/latest/process.html#memory&quot;&gt;memory&lt;/a&gt; and &lt;a href=&quot;https://www.nextflow.io/docs/latest/process.html#disk&quot;&gt;disk&lt;/a&gt; allocation. &lt;/p&gt;&lt;p&gt;These directives allow you to specify the request for a certain number of computing resources e.g CPUs, memory, or disk and Nextflow converts these values to the native setting of the target execution platform specified in the pipeline configuration.&lt;/p&gt;&lt;p&gt;However, there can be settings that are only available on some specific cluster technology or vendors. &lt;/p&gt;&lt;p&gt;The &lt;a href=&quot;https://www.nextflow.io/docs/latest/process.html#clusterOptions&quot;&gt;clusterOptions&lt;/a&gt; directive allows you to specify any option of your resource manager for which there isn&apos;t direct support in Nextflow. &lt;/p&gt;&lt;h3&gt;5. Retry failing jobs increasing resource allocation&lt;/h3&gt;&lt;p&gt;A common scenario is that instances of the same process may require different computing resources. For example, requesting an amount of memory that is too low for some processes will result in those tasks failing. You could specify a higher limit which would accommodate the task with the highest memory utilization, but you then run the risk of decreasing your job’s execution priority. &lt;/p&gt;&lt;p&gt;Nextflow provides a mechanism that allows you to modify the amount of computing resources requested in the case of a process failure and attempt to re-execute it using a higher limit. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process foo {

  memory { 2.GB * task.attempt }
  time { 1.hour * task.attempt }

  errorStrategy { task.exitStatus in 137..140 ? &amp;#39;retry&amp;#39; : &amp;#39;terminate&amp;#39; }
  maxRetries 3

  script:
  &amp;quot;&amp;quot;&amp;quot;
  your_job_command --here
  &amp;quot;&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the above example the memory and execution time limits are defined dynamically. The first time the process is executed the task.attempt is set to 1, thus it will request 2 GB of memory and one hour of maximum execution time.&lt;/p&gt;&lt;p&gt;If the task execution fails, reporting an exit status in the range between 137 and 140, the task is re-submitted (otherwise it terminates immediately). This time the value of task.attempt is 2, thus increasing the amount of the memory to four GB and the time to 2 hours, and so on.&lt;/p&gt;&lt;p&gt;NOTE: These exit statuses are not standard and can change depending on the resource manager you are using. Consult your cluster administrator or scheduler administration guide for details on the exit statuses used by your cluster in similar error conditions. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Nextflow aims to give you control over every aspect of your workflow. These Nextflow options allow you to shape how Nextflow submits your processes to your executor, that can make your workflow more robust by avoiding the overloading of the executor. Some systems have hard limits which if you do not take into account, no processes will be executed. Being aware of these configuration values and how to use them is incredibly helpful when working with larger workflows. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>5 Nextflow Tips for HPC Users</title>
      <link>https://www.nextflow.io/blog/2021/5_tips_for_hpc_users.html</link>
      <pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2021/5_tips_for_hpc_users.html</guid>
      	<description>
	&lt;p&gt;Nextflow is a powerful tool for developing scientific workflows for use on HPC systems. It provides a simple solution to deploy parallelized workloads at scale using an elegant reactive/functional programming model in a portable manner. &lt;/p&gt;&lt;p&gt;It supports the most popular workload managers such as Grid Engine, Slurm, LSF and PBS, among other out-of-the-box executors, and comes with sensible defaults for each. However, each HPC system is a complex machine with its own characteristics and constraints. For this reason you should always consult your system administrator before running a new piece of software or a compute intensive pipeline that spawns a large number of jobs. &lt;/p&gt;&lt;p&gt;In this series of posts, we will be sharing the top tips we have learned along the way that should help you get results faster while keeping in the good books of your sys admins.&lt;/p&gt;&lt;h3&gt;1. Don&apos;t forget the executor&lt;/h3&gt;&lt;p&gt;Nextflow, by default, spawns parallel task executions in the computer on which it is running. This is generally useful for development purposes, however, when using an HPC system you should specify the executor matching your system. This instructs Nextflow to submit pipeline tasks as jobs into your HPC workload manager. This can be done adding the following setting to the &lt;code&gt;nextflow.config&lt;/code&gt; file in the launching directory, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process.executor = &amp;#39;slurm&amp;#39; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the above setting Nextflow will submit the job executions to your Slurm cluster spawning a &lt;code&gt;sbatch&lt;/code&gt; command for each job in your pipeline. Find the executor matching your system at &lt;a href=&quot;https://www.nextflow.io/docs/latest/executor.html&quot;&gt;this link&lt;/a&gt;. Even better, to prevent the undesired use of the local executor in a specific environment, define the &lt;em&gt;default&lt;/em&gt; executor to be used by Nextflow using the following system variable: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export NXF_EXECUTOR=slurm
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;2. Nextflow as a job&lt;/h3&gt;&lt;p&gt;Quite surely your sys admin has already warned you that the login/head node should only be used to submit job executions and not run compute intensive tasks. When running a Nextflow pipeline, the driver application submits and monitors the job executions on your cluster (provided you have correctly specified the executor as stated in point 1), and therefore it should not run compute intensive tasks. &lt;/p&gt;&lt;p&gt;However, it&apos;s never a good practice to launch a long running job in the login node, and therefore a good practice consists of running Nextflow itself as a cluster job. This can be done by wrapping the &lt;code&gt;nextflow run&lt;/code&gt; command in a shell script and submitting it as any other job. An average pipeline may require 2 CPUs and 2 GB of resources allocation. &lt;/p&gt;&lt;p&gt;Note: the queue where the Nextflow driver job is submitted should allow the spawning of the pipeline jobs to carry out the pipeline execution.&lt;/p&gt;&lt;h3&gt;3. Use the queueSize directive&lt;/h3&gt;&lt;p&gt;The &lt;code&gt;queueSize&lt;/code&gt; directive is part of the executor configuration in the &lt;code&gt;nextflow.config&lt;/code&gt; file, and defines how many processes are queued at a given time. By default, Nextflow will submit up to 100 jobs at a time for execution. Increase or decrease this setting depending your HPC system quota and throughput. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;executor {
    name = &amp;#39;slurm&amp;#39;
    queueSize = 50
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;4. Specify the max heap size&lt;/h3&gt;&lt;p&gt;The Nextflow runtime runs on top of the Java virtual machine which, by design, tries to allocate as much memory as is available. This is not a good practice in HPC systems which are designed to share compute resources across many users and applications. To avoid this, specify the maximum amount of memory that can be used by the Java VM using the -Xms and -Xmx Java flags. These can be specified using the &lt;code&gt;NXF_OPTS&lt;/code&gt; environment variable. &lt;/p&gt;&lt;p&gt;For example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export NXF_OPTS=&amp;quot;-Xms500M -Xmx2G&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above setting instructs Nextflow to allocate a Java heap in the range of 500 MB and 2 GB of RAM.&lt;/p&gt;&lt;h3&gt;5. Limit the Nextflow submit rate&lt;/h3&gt;&lt;p&gt;Nextflow attempts to submit the job executions as quickly as possible, which is generally not a problem. However, in some HPC systems the submission throughput is constrained or it should be limited to avoid degrading the overall system performance. To prevent this problem you can use &lt;code&gt;submitRateLimit&lt;/code&gt; to control the Nextflow job submission throughput. This directive is part of the &lt;code&gt;executor&lt;/code&gt; configuration scope, and defines the number of tasks that can be submitted per a unit of time. The default for the &lt;code&gt;submitRateLimit&lt;/code&gt; is unlimited. You can specify the &lt;code&gt;submitRateLimit&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;executor {
    submitRateLimit = &amp;#39;10 sec&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can also more explicitly specify it as a rate of # processes / time unit: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;executor {
    submitRateLimit = &amp;#39;10/2min&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Nextflow aims to give you control over every aspect of your workflow. These options allow you to shape how Nextflow communicates with your HPC system. This can make workflows more robust while avoiding overloading the executor. Some systems have hard limits, and if you do not take them into account, it will stop any jobs from being scheduled. &lt;/p&gt;&lt;p&gt;Stay tuned for part two where we will discuss background executions, retry strategies, maxForks and other tips. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>6 Tips for Setting Up Your Nextflow Dev Environment</title>
      <link>https://www.nextflow.io/blog/2021/nextflow-developer-environment.html</link>
      <pubDate>Thu, 4 Mar 2021 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2021/nextflow-developer-environment.html</guid>
      	<description>
	&lt;p&gt;&lt;em&gt;This blog follows up the Learning Nextflow in 2020 blog &lt;a href=&quot;https://www.nextflow.io/blog/2020/learning-nextflow-in-2020.html&quot;&gt;post&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This guide is designed to walk you through a basic development setup for writing Nextflow pipelines.&lt;/p&gt;&lt;h3&gt;1. Installation&lt;/h3&gt;&lt;p&gt;Nextflow runs on any Linux compatible system and MacOS with Java installed. Windows users can rely on the &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/install-win10&quot;&gt;Windows Subsystem for Linux&lt;/a&gt;. Installing Nextflow is straightforward. You just need to download the &lt;code&gt;nextflow&lt;/code&gt; executable. In your terminal type the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl get.nextflow.io | bash
$ sudo mv nextflow /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first line uses the curl command to download the nextflow executable, and the second line moves the executable to your PATH. Note &lt;code&gt;/usr/local/bin&lt;/code&gt; is the default for MacOS, you might want to choose &lt;code&gt;~/bin&lt;/code&gt; or &lt;code&gt;/usr/bin&lt;/code&gt; depending on your PATH definition and operating system. &lt;/p&gt;&lt;h3&gt;2. Text Editor or IDE?&lt;/h3&gt;&lt;p&gt;Nextflow pipelines can be written in any plain text editor. I&apos;m personally a bit of a Vim fan, however, the advent of the modern IDE provides a more immersive development experience. &lt;/p&gt;&lt;p&gt;My current choice is Visual Studio Code which provides a wealth of add-ons, the most obvious of these being syntax highlighting. With &lt;a href=&quot;https://code.visualstudio.com/download&quot;&gt;VSCode installed&lt;/a&gt;, you can search for the Nextflow extension in the marketplace.&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;VSCode with Nextflow Syntax Highlighting&apos; width=&apos;796&apos; height=&apos;482&apos; src=&apos;/img/vscode-nf-highlighting.png&apos; /&gt;&lt;/p&gt;&lt;p&gt;Other syntax highlighting has been made available by the community including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://atom.io/packages/language-nextflow&quot;&gt;Atom&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/LukeGoodsell/nextflow-vim&quot;&gt;Vim&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Emiller88/nextflow-mode&quot;&gt;Emacs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;3. The Nextflow REPL console&lt;/h3&gt;&lt;p&gt;The Nextflow console is a REPL (read-eval-print loop) environment that allows one to quickly test part of a script or segments of Nextflow code in an interactive manner. This can be particularly useful to quickly evaluate channels and operators behaviour and prototype small snippets that can be included in your pipeline scripts.&lt;/p&gt;&lt;p&gt;Start the Nextflow console with the following command: &lt;code&gt;
$ nextflow console
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;Nextflow REPL console&apos; width=&apos;796&apos; height=&apos;549&apos; src=&apos;/img/nf-repl-console.png&apos; /&gt;&lt;/p&gt;&lt;p&gt;Use the &lt;code&gt;CTRL+R&lt;/code&gt; keyboard shortcut to run (&lt;code&gt;⌘+R&lt;/code&gt;on the Mac) and to evaluate your code. You can also evaluate by selecting code and use the &lt;strong&gt;Run selection&lt;/strong&gt;.&lt;/p&gt;&lt;h3&gt;4. Containerize all the things&lt;/h3&gt;&lt;p&gt;Containers are a key component of developing scalable and reproducible pipelines. We can build Docker images that contain an OS, all libraries and the software we need for each process. Pipelines are typically developed using Docker containers and tooling as these can then be used on many different container engines such as Singularity and Podman. &lt;/p&gt;&lt;p&gt;Once you have &lt;a href=&quot;https://docs.docker.com/engine/install/&quot;&gt;downloaded and installed Docker&lt;/a&gt;, try pull a public docker image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker pull quay.io/nextflow/rnaseq-nf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To run a Nextflow pipeline using the latest tag of the image, we can use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run nextflow-io/rnaseq-nf -with-docker quay.io/nextflow/rnaseq-nf:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To learn more about building Docker containers, see the &lt;a href=&quot;https://seqera.io/training/#_manage_dependencies_containers&quot;&gt;Seqera Labs tutorial&lt;/a&gt; on managing dependencies with containers. &lt;/p&gt;&lt;p&gt;Additionally, you can install the VSCode marketplace addon for Docker to manage and interactively run and test the containers and images on your machine. You can even connect to remote registries such as Dockerhub, Quay.io, AWS ECR, Google Cloud and Azure Container registries.&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;VSCode with Docker Extension&apos; width=&apos;796&apos; height=&apos;482&apos; src=&apos;/img/vs-code-with-docker-extension.png&apos; /&gt;&lt;/p&gt;&lt;h3&gt;5. Use Tower to monitor your pipelines&lt;/h3&gt;&lt;p&gt;When developing real-world pipelines, it can become inevitable that pipelines will require significant resources. For long-running workflows, monitoring becomes all the more crucial. With &lt;a href=&quot;https://tower.nf&quot;&gt;Nextflow Tower&lt;/a&gt;, we can invoke any Nextflow pipeline execution from the CLI and use the integrated dashboard to follow the workflow run. &lt;/p&gt;&lt;p&gt;Sign-in to Tower using your GitHub credentials, obtain your token from the Getting Started page and export them into your terminal, &lt;code&gt;~/.bashrc&lt;/code&gt;, or include them in your nextflow.config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export TOWER_ACCESS_TOKEN=my-secret-tower-key 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then add the &lt;code&gt;-with-tower&lt;/code&gt; child-option to any Nextflow run command. A URL with the monitoring dashboard will appear.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run nextflow-io/rnaseq-nf -with-tower
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;6. nf-core tools&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://nf-co.re/&quot;&gt;nf-core&lt;/a&gt; is a community effort to collect a curated set of analysis pipelines built using Nextflow. The pipelines continue to come on in leaps and bounds and nf-core tools is a python package for helping with developing nf-core pipelines. It includes options for listing, creating, and even downloading pipelines for offline usage. &lt;/p&gt;&lt;p&gt;These tools are particularly useful for developers contributing to the community pipelines on &lt;a href=&quot;https://github.com/nf-core/&quot;&gt;GitHub&lt;/a&gt; with linting and syncing options that keep pipelines up-to-date against nf-core guidelines.&lt;/p&gt;&lt;p&gt;&lt;code&gt;nf-core tools&lt;/code&gt; is a python package that can be installed in your development environment from Bioconda or PyPi.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda install nf-core
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;or&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install nf-core
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img alt=&apos;nf-core tools&apos; width=&apos;796&apos; height=&apos;561&apos; src=&apos;/img/nf-core-tools.png&apos; /&gt;&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Developer workspaces are evolving rapidly. While your own development environment may be highly dependent on personal preferences, community contributions are keeping Nextflow users at the forefront of the modern developer experience. &lt;/p&gt;&lt;p&gt;Solutions such as &lt;a href=&quot;https://github.com/features/codespaces&quot;&gt;GitHub Codespaces&lt;/a&gt; and &lt;a href=&quot;https://www.gitpod.io/&quot;&gt;Gitpod&lt;/a&gt; are now offering extendible, cloud-based options that may well be the future. I’m sure we can all look forward to a one-click, pre-configured, cloud-based, Nextflow developer environment sometime soon!&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Introducing Nextflow for Azure Batch</title>
      <link>https://www.nextflow.io/blog/2021/introducing-nextflow-for-azure-batch.html</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2021/introducing-nextflow-for-azure-batch.html</guid>
      	<description>
	&lt;p&gt;When the Nextflow project was created, one of the main drivers was to enable reproducible data pipelines that could be deployed across a wide range of execution platforms with minimal effort as well as to empower users to scale their data analysis while facilitating the migration to the cloud.&lt;/p&gt;&lt;p&gt;Throughout the years, the computing services provided by cloud vendors have evolved in a spectacular manner. Eight years ago, the model was focused on launching virtual machines in the cloud, then came containers and then the idea of serverless computing which changed everything again. However, the power of the Nextflow abstraction consists of hiding the complexity of the underlying platform. Through the concept of executors, emerging technologies and new platforms can be easily adapted with no changes required to user pipelines.&lt;/p&gt;&lt;p&gt;With this in mind, we could not be more excited to announce that over the past months we have been working with Microsoft to implement built-in support for &lt;a href=&quot;https://azure.microsoft.com/en-us/services/batch/&quot;&gt;Azure Batch&lt;/a&gt; into Nextflow. Today we are delighted to make it available to all users as a beta release. &lt;/p&gt;&lt;h3&gt;How does it work&lt;/h3&gt;&lt;p&gt;Azure Batch is a cloud-based computing service that allows the execution of highly scalable, container based, workloads in the Azure cloud.&lt;/p&gt;&lt;p&gt;The support for Nextflow comes in the form of a plugin which implements a new executor, not surprisingly named &lt;code&gt;azurebatch&lt;/code&gt;, which offloads the execution of the pipeline jobs to corresponding Azure Batch jobs. &lt;/p&gt;&lt;p&gt;Each job run consists in practical terms of a container execution which ships the job dependencies and carries out the job computation. As usual, each job is assigned a unique working directory allocated into a &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/blobs/&quot;&gt;Azure Blob&lt;/a&gt; container.&lt;/p&gt;&lt;h3&gt;Let&apos;s get started!&lt;/h3&gt;&lt;p&gt;The support for Azure Batch requires the latest release of Nextflow from the &lt;em&gt;edge&lt;/em&gt; channel (version 21.02-edge or later). If you don&apos;t have this, you can install it using these commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export NXF_EDGE=1 
curl get.nextflow.io | bash
./nextflow -self-update
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note for Windows users, as Nextflow is *nix based tool you will need to run it using the &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/install-win10&quot;&gt;Windows subsystem for Linux&lt;/a&gt;. Also make sure Java 8 or later is installed in the Linux environment.&lt;/p&gt;&lt;p&gt;Once Nextflow is installed, to run your data pipelines with Azure Batch, you will need to create an Azure Batch account in the region of your choice using the Azure Portal. In a similar manner, you will need an Azure Blob container.&lt;/p&gt;&lt;p&gt;With the Azure Batch and Blob storage container configured, your &lt;code&gt;nextflow.config&lt;/code&gt; file should be set up similar to the example below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plugins {
  id &amp;#39;nf-azure&amp;#39;
}

process {
  executor = &amp;#39;azurebatch&amp;#39;
}

azure {
  batch {
    location = &amp;#39;westeurope&amp;#39;
    accountName = &amp;#39;&amp;lt;YOUR BATCH ACCOUNT NAME&amp;gt;&amp;#39;
    accountKey = &amp;#39;&amp;lt;YOUR BATCH ACCOUNT KEY&amp;gt;&amp;#39;
    autoPoolMode = true
  }
  storage {
    accountName = &amp;quot;&amp;lt;YOUR STORAGE ACCOUNT NAME&amp;gt;&amp;quot;
    accountKey = &amp;quot;&amp;lt;YOUR STORAGE ACCOUNT KEY&amp;gt;&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Using this configuration snippet, Nextflow will automatically create the virtual machine pool(s) required to deploy the pipeline execution in the Azure Batch service.&lt;/p&gt;&lt;p&gt;Now you will be able to launch the pipeline execution using the following command: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run &amp;lt;pipeline name&amp;gt; -w az://my-container/work
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Replace &lt;code&gt;&amp;lt;pipeline name&amp;gt;&lt;/code&gt; with a pipeline name e.g. nextflow-io/rnaseq-nf and &lt;code&gt;my-container&lt;/code&gt; with a blob container in the storage account as defined in the above configuration.&lt;/p&gt;&lt;p&gt;For more details regarding the Nextflow configuration setting for Azure Batch refers to the Nextflow documentation at &lt;a href=&quot;/docs/edge/azure.html&quot;&gt;this link&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The support for Azure Batch further expands the wide range of computing platforms supported by Nextflow and empowers Nextflow users to deploy their data pipelines in the cloud provider of their choice. Above all, it allows researchers to scale, collaborate and share their work without being locked into a specific platform.&lt;/p&gt;&lt;p&gt;We thank Microsoft, and in particular &lt;a href=&quot;https://www.linkedin.com/in/jermingchia/&quot;&gt;Jer-Ming Chia&lt;/a&gt; who works in the HPC and AI team for having supported and sponsored this open source contribution to the Nextflow framework.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Learning Nextflow in 2020</title>
      <link>https://www.nextflow.io/blog/2020/learning-nextflow-in-2020.html</link>
      <pubDate>Tue, 1 Dec 2020 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2020/learning-nextflow-in-2020.html</guid>
      	<description>
	&lt;p&gt;With the year nearly over, we thought it was about time to pull together the best-of-the-best guide for learning Nextflow in 2020. These resources will support anyone in the journey from total noob to Nextflow expert so this holiday season, give yourself or someone you know the gift of learning Nextflow!&lt;/p&gt;&lt;h3&gt;Prerequisites to get started&lt;/h3&gt;&lt;p&gt;We recommend that learners are comfortable with using the command line and the basic concepts of a scripting language such as Python or Perl before they start writing pipelines. Nextflow is widely used for bioinformatics applications, and the examples in these guides often focus on applications in these topics. However, Nextflow is now adopted in a number of data-intensive domains such as radio astronomy, satellite imaging and machine learning. No domain expertise is expected.&lt;/p&gt;&lt;h3&gt;Time commitment&lt;/h3&gt;&lt;p&gt;We estimate that the speediest of learners can complete the material in around 12 hours. It all depends on your background and how deep you want to dive into the rabbit-hole! Most of the content is introductory with some more advanced dataflow and configuration material in the workshops and patterns sections.&lt;/p&gt;&lt;h3&gt;Overview of the material&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Why learn Nextflow?&lt;/li&gt;
  &lt;li&gt;Introduction to Nextflow - AWS HPC Conference 2020 (8m)&lt;/li&gt;
  &lt;li&gt;A simple RNA-Seq hands-on tutorial (2h)&lt;/li&gt;
  &lt;li&gt;Full-immersion workshop (8h)&lt;/li&gt;
  &lt;li&gt;Nextflow advanced implementation Patterns (2h)&lt;/li&gt;
  &lt;li&gt;Other resources&lt;/li&gt;
  &lt;li&gt;Community and Support&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;1. Why learn Nextflow?&lt;/h3&gt;&lt;p&gt;Nextflow is an open-source workflow framework for writing and scaling data-intensive computational pipelines. It is designed around the Linux philosophy of simple yet powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations. Combined with support for containerization, support for major cloud providers and on-premise architectures, Nextflow simplifies the writing and deployment of complex data pipelines on any infrastructure. &lt;/p&gt;&lt;p&gt;The following are some high-level motivations on why people choose to adopt Nextflow:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Integrating Nextflow in your analysis workflows helps you implement &lt;strong&gt;reproducible&lt;/strong&gt; pipelines. Nextflow pipelines follow FDA repeatability and reproducibility guidelines with version-control and containers to manage all software dependencies.&lt;/li&gt;
  &lt;li&gt;Avoid vendor lock-in by ensuring portability. Nextflow is &lt;strong&gt;portable&lt;/strong&gt;; the same pipeline written on a laptop can quickly scale to run HPC cluster, Amazon and Google cloud services, and Kubernetes. The code stays constant across varying infrastructures allowing collaboration and avoiding lock-in.&lt;/li&gt;
  &lt;li&gt;It is &lt;strong&gt;scalable&lt;/strong&gt; allowing the parallelization of tasks using the dataflow paradigm without having to hard-code the pipeline to a specific platform architecture.&lt;/li&gt;
  &lt;li&gt;It is &lt;strong&gt;flexible&lt;/strong&gt; and supports scientific workflow requirements like caching processes to prevent re-computation, and workflow reports to better understand the workflows’ executions.&lt;/li&gt;
  &lt;li&gt;It is &lt;strong&gt;growing fast&lt;/strong&gt; and has &lt;strong&gt;long-term support&lt;/strong&gt;. Developed since 2013 by the same team, the Nextflow ecosystem is expanding rapidly.&lt;/li&gt;
  &lt;li&gt;It is &lt;strong&gt;open source&lt;/strong&gt; and licensed under Apache 2.0. You are free to use it, modify it and distribute it.&lt;/li&gt;
&lt;/ol&gt;&lt;h3&gt;2. Introduction to Nextflow from the HPC on AWS Conference 2020&lt;/h3&gt;&lt;p&gt;This short YouTube video provides a general overview of Nextflow, the motivations behind its development and a demonstration of some of the latest features.&lt;/p&gt;&lt;p&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SYhDkUgcOXo&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;h3&gt;3. A simple RNA-Seq hands-on tutorial&lt;/h3&gt;&lt;p&gt;This hands-on tutorial from Seqera Labs will guide you through implementing a proof-of-concept RNA-seq pipeline. The goal is to become familiar with basic concepts, including how to define parameters, use channels for data and write processes to perform tasks. It includes all scripts, data and resources and is perfect for getting a flavor for Nextflow.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/seqeralabs/nextflow-tutorial&quot;&gt;Tutorial link on GitHub&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;4. Full-immersion workshop&lt;/h3&gt;&lt;p&gt;Here you’ll dive deeper into Nextflow’s most prominent features and learn how to apply them. The full workshop includes an excellent section on containers, how to build them and how to use them with Nextflow. The written materials come with examples and hands-on exercises. Optionally, you can also follow with a series of videos from a live training workshop.&lt;/p&gt;&lt;p&gt;The workshop includes topics on:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Environment Setup&lt;/li&gt;
  &lt;li&gt;Basic NF Script and Concepts&lt;/li&gt;
  &lt;li&gt;Nextflow Processes&lt;/li&gt;
  &lt;li&gt;Nextflow Channels&lt;/li&gt;
  &lt;li&gt;Nextflow Operators&lt;/li&gt;
  &lt;li&gt;Basic RNA-Seq pipeline&lt;/li&gt;
  &lt;li&gt;Containers &amp;amp; Conda&lt;/li&gt;
  &lt;li&gt;Nextflow Configuration&lt;/li&gt;
  &lt;li&gt;On-premise &amp;amp; Cloud Deployment&lt;/li&gt;
  &lt;li&gt;DSL 2 &amp;amp; Modules&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://seqera.io/training/handson/&quot;&gt;GATK hands-on exercise&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://seqera.io/training&quot;&gt;Workshop&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://www.youtube.com/playlist?list=PLPZ8WHdZGxmUv4W8ZRlmstkZwhb_fencI&quot;&gt;YouTube playlist&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;5. Nextflow implementation Patterns&lt;/h3&gt;&lt;p&gt;This advanced section discusses recurring patterns and solutions to many common implementation requirements. Code examples are available with notes to follow along with as well as a GitHub repository.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://nextflow-io.github.io/patterns/index.html&quot;&gt;Nextflow Patterns&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://github.com/nextflow-io/patterns&quot;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Other resources&lt;/h3&gt;&lt;p&gt;The following resources will help you dig deeper into Nextflow and other related projects like the nf-core community who maintain curated pipelines and a very active Slack channel. There are plenty of Nextflow tutorials and videos online, and the following list is no way exhaustive. Please let us know if we are missing something.&lt;/p&gt;&lt;h4&gt;Nextflow docs&lt;/h4&gt;&lt;p&gt;The reference for the Nextflow language and runtime. The docs should be your first point of reference when something is not clear. Newest features are documented in edge documentation pages released every month with the latest stable releases every three months.&lt;/p&gt;&lt;p&gt;Latest &lt;a href=&quot;https://www.nextflow.io/docs/latest/index.html&quot;&gt;stable&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://www.nextflow.io/docs/edge/index.html&quot;&gt;edge&lt;/a&gt; documentation.&lt;/p&gt;&lt;h4&gt;nf-core&lt;/h4&gt;&lt;p&gt;nf-core is a growing community of Nextflow users and developers. You can find curated sets of biomedical analysis pipelines built by domain experts with Nextflow, that have passed tests and have been implemented according to best practice guidelines. Be sure to sign up to the Slack channel.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://nf-co.re&quot;&gt;nf-core website&lt;/a&gt;&lt;/p&gt;&lt;h4&gt;Tower Docs&lt;/h4&gt;&lt;p&gt;Nextflow Tower is a platform to easily monitor, launch and scale Nextflow pipelines on cloud providers and on-premise infrastructure. The documentation provides details on setting up compute environments, monitoring pipelines and launching using either the web graphic interface or API.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://help.tower.nf&quot;&gt;Nextflow Tower documentation&lt;/a&gt;&lt;/p&gt;&lt;h4&gt;Nextflow Biotech Blueprint by AWS&lt;/h4&gt;&lt;p&gt;A quickstart for deploying a genomics analysis environment on Amazon Web Services (AWS) cloud, using Nextflow to create and orchestrate analysis workflows and AWS Batch to run the workflow processes.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/quickstart/biotech-blueprint/nextflow/&quot;&gt;Biotech Blueprint by AWS&lt;/a&gt;&lt;/p&gt;&lt;h4&gt;Running Nextflow by Google Cloud&lt;/h4&gt;&lt;p&gt;Google Cloud Nextflow step-by-step guide to launching Nextflow Pipelines in Google Cloud.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/life-sciences/docs/tutorials/nextflow&quot;&gt;Nextflow on Google Cloud &lt;/a&gt;&lt;/p&gt;&lt;h4&gt;Awesome Nextflow&lt;/h4&gt;&lt;p&gt;A collections of Nextflow based pipelines and other resources. &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/nextflow-io/awesome-nextflow&quot;&gt;Awesome Nextflow&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Community and support&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Nextflow &lt;a href=&quot;https://gitter.im/nextflow-io/nextflow&quot;&gt;Gitter channel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Nextflow &lt;a href=&quot;https://groups.google.com/forum/#!forum/nextflow&quot;&gt;Forums&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nfcore.slack.com/&quot;&gt;nf-core Slack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Twitter &lt;a href=&quot;https://twitter.com/nextflowio?lang=en&quot;&gt;@nextflowio&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.seqera.io&quot;&gt;Seqera Labs&lt;/a&gt; technical support &amp;amp; consulting&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Nextflow is a community-driven project. The list of links below has been collated from a diverse collection of resources and experts to guide you in learning Nextflow. If you have any suggestions, please make a pull request to this page on GitHub.&lt;/p&gt;&lt;p&gt;Also stay tuned for our upcoming post, where we will discuss the ultimate Nextflow development environment. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>More syntax sugar for Nextflow developers!</title>
      <link>https://www.nextflow.io/blog/2020/groovy3-syntax-sugar.html</link>
      <pubDate>Tue, 3 Nov 2020 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2020/groovy3-syntax-sugar.html</guid>
      	<description>
	&lt;p&gt;The latest Nextflow version 2020.10.0 is the first stable release running on Groovy 3. &lt;/p&gt;&lt;p&gt;The first benefit of this change is that now Nextflow can be compiled and run on any modern Java virtual machine, from Java 8, all the way up to the latest Java 15! &lt;/p&gt;&lt;p&gt;Along with this, the new Groovy runtime brings a whole lot of syntax enhancements that can be useful in the everyday life of pipeline developers. Let&apos;s see them more in detail. &lt;/p&gt;&lt;h3&gt;Improved not operator&lt;/h3&gt;&lt;p&gt;The &lt;code&gt;!&lt;/code&gt; (not) operator can now prefix the &lt;code&gt;in&lt;/code&gt; and &lt;code&gt;instanceof&lt;/code&gt; keywords. This makes for more concise writing of some conditional expression, for example, the following snippet: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;list = [10,20,30]

if( !(x in list) ) {
  // .. 
} 
else if( !(x instanceof String) ) {
  // .. 
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;could be replaced by the following: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;list = [10,20,30]

if( x !in list ) {
   // .. 
}
else if( x !instanceof String ) {
   // .. 
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Again, this is a small syntax change which makes the code a little more readable. &lt;/p&gt;&lt;h3&gt;Elvis assignment operator&lt;/h3&gt;&lt;p&gt;The elvis assignment operator &lt;code&gt;?=&lt;/code&gt; allows the assignment of a value only if it was not previously assigned (or if it evaluates to &lt;code&gt;null&lt;/code&gt;). Consider the following example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def opts = [foo: 1]

opts.foo ?= 10 
opts.bar ?= 20

assert opts.foo == 1 
assert opts.bar == 20
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this snippet, the assignment &lt;code&gt;opts.foo ?= 10&lt;/code&gt; would be ignored because the dictionary &lt;code&gt;opts&lt;/code&gt; already contains a value for the &lt;code&gt;foo&lt;/code&gt; attribute, while it is now assigned as expected. &lt;/p&gt;&lt;p&gt;In other words this is a shortcut for the following idiom: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if( some_variable != null ) {
  some_variable = &amp;#39;Hello&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you are wondering why it&apos;s called &lt;em&gt;Elvis&lt;/em&gt; assigment, well it&apos;s simple, because there&apos;s also the &lt;a href=&quot;https://groovy-lang.org/operators.html#_elvis_operator&quot;&gt;Elvis operator&lt;/a&gt; that you should know (and use!) already. 😆 &lt;/p&gt;&lt;h3&gt;Java style lambda expressions&lt;/h3&gt;&lt;p&gt;Groovy 3 supports the syntax for Java lambda expression. If you don&apos;t know what a Java lambda expression is don&apos;t worry; it&apos;s a concept very similar to a Groovy closure, though with slight differences both in the syntax and the semantic. In a few words, a Groovy closure can modify a variable in the outside scope, while a Java lambda cannot. &lt;/p&gt;&lt;p&gt;In terms of syntax, a Groovy closure is defined as: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{ it -&amp;gt; SOME_EXPRESSION_HERE }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;While Java lambda expression looks like: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;it -&amp;gt; { SOME_EXPRESSION_HERE }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;which can be simplified to the following form when the expression is a single statement: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;it -&amp;gt; SOME_EXPRESSION_HERE
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The good news is that the two syntaxes are interoperable in many cases and we can use the &lt;em&gt;lambda&lt;/em&gt; syntax to get rid-off of the curly bracket parentheses used by the Groovy notation to make our Nextflow script more readable. &lt;/p&gt;&lt;p&gt;For example, the following Nextflow idiom: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Channel
    .of( 1,2,3 ) 
    .map { it * it +1 }
    .view { &amp;quot;the value is $it&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Can be rewritten using the lambda syntax as: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Channel
    .of( 1,2,3 ) 
    .map( it -&amp;gt; it * it +1 )
    .view( it -&amp;gt; &amp;quot;the value is $it&amp;quot; )
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It is a bit more consistent. Note however that the &lt;code&gt;it -&amp;gt;&lt;/code&gt; implicit argument is now mandatory (while when using the closure syntax it could be omitted). Also, when the operator argument is not &lt;em&gt;single&lt;/em&gt; value, the lambda requires the round parentheses to define the argument e.g. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Channel
    .of( 1,2,3 ) 
    .map( it -&amp;gt; tuple(it * it,  it+1) )
    .view( (a,b) -&amp;gt; &amp;quot;the values are $a and $b&amp;quot; )
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Full support for Java streams API&lt;/h3&gt;&lt;p&gt;Since version 8, Java provides a &lt;a href=&quot;https://winterbe.com/posts/2014/07/31/java8-stream-tutorial-examples/&quot;&gt;stream library&lt;/a&gt; that is very powerful and implements some concepts and operators similar to Nextflow channels. &lt;/p&gt;&lt;p&gt;The main differences between the two are that Nextflow channels and the corresponding operators are &lt;em&gt;non-blocking&lt;/em&gt; i.e. their evaluation is performed asynchronously without blocking your program execution, while Java streams are executed in a synchronous manner (at least by default).&lt;/p&gt;&lt;p&gt;A Java stream looks like the following: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;assert (1..10).stream()
                .filter(e -&amp;gt; e % 2 == 0)
                .map(e -&amp;gt; e * 2)
                .toList() == [4, 8, 12, 16, 20]

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note, in the above example &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#filter-java.util.function.Predicate-&quot;&gt;filter&lt;/a&gt;, &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#map-java.util.function.Function-&quot;&gt;map&lt;/a&gt; and &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html#toList--&quot;&gt;toList&lt;/a&gt; methods are Java stream operator not the &lt;a href=&quot;https://www.nextflow.io/docs/latest/operator.html#filter&quot;&gt;Nextflow&lt;/a&gt; &lt;a href=&quot;https://www.nextflow.io/docs/latest/operator.html#map&quot;&gt;homonymous&lt;/a&gt; &lt;a href=&quot;https://www.nextflow.io/docs/latest/operator.html#tolist&quot;&gt;ones&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Java style method reference&lt;/h3&gt;&lt;p&gt;The new runtime also allows for the use of the &lt;code&gt;::&lt;/code&gt; operator to reference an object method. This can be useful to pass a method as an argument to a Nextflow operator in a similar manner to how it was already possible using a closure. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Channel
 .of( &amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;)
 .view( String::toUpperCase )
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above prints: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  A
  B
  C
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Because to &lt;a href=&quot;https://www.nextflow.io/docs/latest/operator.html#filter&quot;&gt;view&lt;/a&gt; operator applied the method &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#toUpperCase--&quot;&gt;toUpperCase&lt;/a&gt; to each element emitted by the channel. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The new Groovy runtime brings a lot of syntax sugar for Nextflow pipelines and allows the use of modern Java runtime which delivers better performance and resource usage. &lt;/p&gt;&lt;p&gt;The ones listed above are only a small selection which may be useful to everyday Nextflow developers. If you are curious to learn more about all the changes in the new Groovy parser you can find more details in &lt;a href=&quot;https://groovy-lang.org/releasenotes/groovy-3.0.html&quot;&gt;this link&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Finally, a big thanks to the Groovy community for their significant efforts in developing and maintaining this great programming environment. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>The Nextflow CLI - tricks and treats! </title>
      <link>https://www.nextflow.io/blog/2020/cli-docs-release.html</link>
      <pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2020/cli-docs-release.html</guid>
      	<description>
	&lt;p&gt;For most developers, the command line is synonymous with agility. While tools such as &lt;a href=&quot;https://tower.nf&quot;&gt;Nextflow Tower&lt;/a&gt; are opening up the ecosystem to a whole new set of users, the Nextflow CLI remains a bedrock for pipeline development. The CLI in Nextflow has been the core interface since the beginning; however, its full functionality was never extensively documented. Today we are excited to release the first iteration of the CLI documentation available on the &lt;a href=&quot;https://www.nextflow.io/docs/edge/cli.html&quot;&gt;Nextflow website&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;And given Halloween is just around the corner, in this blog post we&apos;ll take a look at 5 CLI tricks and examples which will make your life easier in designing, executing and debugging data pipelines. We are also giving away 5 limited-edition Nextflow hoodies and sticker packs so you can code in style this Halloween season!&lt;/p&gt;&lt;h3&gt;1. Invoke a remote pipeline execution with the latest revision&lt;/h3&gt;&lt;p&gt;Nextflow facilitates easy collaboration and re-use of existing pipelines in multiple ways. One of the simplest ways to do this is to use the URL of the Git repository.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run https://www.github.com/nextflow-io/hello
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When executing a pipeline using the run command, it first checks to see if it has been previously downloaded in the ~/.nextflow/assets directory, and if so, Nextflow uses this to execute the pipeline. If the pipeline is not already cached, Nextflow will download it, store it in the &lt;code&gt;$HOME/.nextflow/&lt;/code&gt; directory and then launch the execution. &lt;/p&gt;&lt;p&gt;How can we make sure that we always run the latest code from the remote pipeline? We simply need to add the &lt;code&gt;-latest&lt;/code&gt; option to the run command, and Nextflow takes care of the rest. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run nextflow-io/hello -latest 
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;2. Query work directories for a specific execution&lt;/h3&gt;&lt;p&gt;For every invocation of Nextflow, all the metadata about an execution is stored including task directories, completion status and time etc. We can use the &lt;code&gt;nextflow log&lt;/code&gt; command to generate a summary of this information for a specific run. &lt;/p&gt;&lt;p&gt;To see a list of work directories associated with a particular execution (for example, &lt;code&gt;tiny_leavitt&lt;/code&gt;), use: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow log tiny_leavitt 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To filter out specific process-level information from the logs of any execution, we simply need to use the fields (-f) option and specify the fields. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow log tiny_leavitt –f &amp;#39;process, hash, status, duration&amp;#39; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The hash is the name of the work directory where the process was executed; therefore, the location of a process work directory would be something like &lt;code&gt;work/74/68ff183&lt;/code&gt;. &lt;/p&gt;&lt;p&gt;The log command also has other child options including &lt;code&gt;-before&lt;/code&gt; and &lt;code&gt;-after&lt;/code&gt; to help with the chronological inspection of logs. &lt;/p&gt;&lt;h3&gt;3. Top-level configuration&lt;/h3&gt;&lt;p&gt;Nextflow emphasizes customization of pipelines and exposes multiple options to facilitate this. The configuration is applied to multiple Nextflow commands and is therefore a top-level option. In practice, this means specifying configuration options &lt;em&gt;before&lt;/em&gt; the command. &lt;/p&gt;&lt;p&gt;Nextflow CLI provides two kinds of config overrides - the soft override and the hard override. &lt;/p&gt;&lt;p&gt;The top-level soft override &quot;-c&quot; option allows us to change the previous config in an additive manner, overriding only the fields included the configuration file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow -c my.config run nextflow-io/hello
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;On the other hand, the hard override &lt;code&gt;-C&lt;/code&gt; completely replaces and ignores any additional configurations. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow –C my.config nextflow-io/hello
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Moreover, we can also use the config command to inspect the final inferred configuration and view any profiles.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow config -show-profiles 
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;4. Passing in an input parameter file&lt;/h3&gt;&lt;p&gt;Nextflow is designed to work across both research and production settings. In production especially, specifying multiple parameters for the pipeline on the command line becomes cumbersome. In these cases, environment variables or config files are commonly used which contain all input files, options and metadata. Love them or hate them, YAML and JSON are the standard formats for human and machines, respectively.&lt;/p&gt;&lt;p&gt;The Nextflow run option &lt;code&gt;-params-file&lt;/code&gt; can be used to pass in a file containing parameters in either format. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run nextflow-io/rnaseq -params-file run_42.yaml 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The YAML file could contain the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reads      : &amp;quot;s3://gatk-data/run_42/reads/*_R{1,2}_*.fastq.gz&amp;quot;
bwa_index  : &amp;quot;$baseDir/index/*.bwa-index.tar.gz&amp;quot;
paired_end : true
penalty    : 12
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;5. Specific workflow entry points&lt;/h3&gt;&lt;p&gt;The recently released &lt;a href=&quot;https://www.nextflow.io/blog/2020/dsl2-is-here.html&quot;&gt;DSL2&lt;/a&gt; adds powerful modularity to Nextflow and enables scripts to contain multiple workflows. By default, the unnamed workflow is assumed to be the main entry point for the script, however, with numerous named workflows, the entry point can be customized by using the &lt;code&gt;entry&lt;/code&gt; child-option of the run command. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run main.nf -entry workflow1 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This allows users to run a specific sub-workflow or a section of their entire workflow script. For more information, refer to the &lt;a href=&quot;https://www.nextflow.io/docs/latest/dsl2.html#implicit-workflow&quot;&gt;implicit workflow&lt;/a&gt; section of the documentation.&lt;/p&gt;&lt;p&gt;Additionally, as of version 20.09.1-edge, you can specify the script in a project to run other than &lt;code&gt;main.nf&lt;/code&gt; using the command line option &lt;code&gt;-main-script&lt;/code&gt;. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run http://github.com/my/pipeline -main-script my-analysis.nf 
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Bonus trick! Web dashboard launched from the CLI&lt;/h3&gt;&lt;p&gt;The tricks above highlight the functionality of the Nextflow CLI. However, for long-running workflows, monitoring becomes all the more crucial. With Nextflow Tower, we can invoke any Nextflow pipeline execution from the CLI and use the integrated dashboard to follow the workflow execution wherever we are. Sign-in to &lt;a href=&quot;https://tower.nf&quot;&gt;Tower&lt;/a&gt; using your GitHub credentials, obtain your token from the Getting Started page and export them into your terminal, &lt;code&gt;~/.bashrc&lt;/code&gt; or include them in your &lt;code&gt;nextflow.config&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export TOWER_ACCESS_TOKEN=my-secret-tower-key 
$ export NXF_VER=20.07.1 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next simply add the &quot;-with-tower&quot; child-option to any Nextflow run command. A URL with the monitoring dashboard will appear.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run nextflow-io/hello -with-tower 
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Nextflow Giveaway&lt;/h3&gt;&lt;p&gt;If you want to look stylish while you put the above tips into practice, or simply like free stuff, we are giving away five of our latest Nextflow hoodie and sticker packs. Retweet or like the Nextflow tweet about this article and we will draw and notify the winners on October 31st!&lt;/p&gt;&lt;h3&gt;About the Author&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/abhi18av/&quot;&gt;Abhinav Sharma&lt;/a&gt; is a Bioinformatics Engineer at &lt;a href=&quot;https://www.seqera.io&quot;&gt;Seqera Labs&lt;/a&gt; interested in Data Science and Cloud Engineering. He enjoys working on all things Genomics, Bioinformatics and Nextflow.&lt;/p&gt;&lt;h3&gt;Acknowledgements&lt;/h3&gt;&lt;p&gt;Shout out to &lt;a href=&quot;https://github.com/KevinSayers&quot;&gt;Kevin Sayers&lt;/a&gt; and &lt;a href=&quot;https://github.com/apeltzer&quot;&gt;Alexander Peltzer&lt;/a&gt; for their earlier efforts in documenting the CLI and which inspired this work.&lt;/p&gt;&lt;p&gt;&lt;em&gt;The latest CLI docs can be found in the edge release docs at &lt;a href=&quot;https://www.nextflow.io/docs/latest/cli.html&quot;&gt;https://www.nextflow.io/docs/latest/cli.html&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nextflow DSL 2 is here!</title>
      <link>https://www.nextflow.io/blog/2020/dsl2-is-here.html</link>
      <pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2020/dsl2-is-here.html</guid>
      	<description>
	&lt;p&gt;We are thrilled to announce the stable release of Nextflow DSL 2 as part of the latest 20.07.1 version! &lt;/p&gt;&lt;p&gt;Nextflow DSL 2 represents a major evolution of the Nextflow language and makes it possible to scale and modularise your data analysis pipeline while continuing to use the Dataflow programming paradigm that characterises the Nextflow processing model. &lt;/p&gt;&lt;p&gt;We spent more than one year collecting user feedback and making sure that DSL 2 would naturally fit the programming experience Nextflow developers are used to. &lt;/p&gt;&lt;h4&gt;DLS 2 in a nutshell&lt;/h4&gt;&lt;p&gt;Backward compatibility is a paramount value, for this reason the changes introduced in the syntax have been minimal and above all, guarantee the support of all existing applications. DSL 2 will be an opt-in feature for at least the next 12 to 18 months. After this transitory period, we plan to make it the default Nextflow execution mode. &lt;/p&gt;&lt;p&gt;As of today, to use DSL 2 in your Nextflow pipeline, you are required to use the following declaration at the top of your script: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow.enable.dsl=2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that the previous &lt;code&gt;nextflow.preview&lt;/code&gt; directive is still available, however, when using the above declaration the use of the final syntax is enforced.&lt;/p&gt;&lt;h4&gt;Nextflow modules&lt;/h4&gt;&lt;p&gt;A module file is nothing more than a Nextflow script containing one or more &lt;code&gt;process&lt;/code&gt; definitions that can be imported from another Nextflow script. &lt;/p&gt;&lt;p&gt;The only difference when compared with legacy syntax is that the process is not bound with specific input and output channels, as was previously required using the &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;into&lt;/code&gt; keywords respectively. Consider this example of the new syntax:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process INDEX {
  input:
    path transcriptome 
  output:
    path &amp;#39;index&amp;#39; 
  script:
    &amp;quot;&amp;quot;&amp;quot;
    salmon index --threads $task.cpus -t $transcriptome -i index
    &amp;quot;&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This allows the definition of workflow processes that can be included from any other script and invoked as a custom function within the new &lt;code&gt;workflow&lt;/code&gt; scope. This effectively allows for the composition of the pipeline logic and enables reuse of workflow components. We anticipate this to improve both the speed that users can develop new pipelines, and the robustness of these pipelines through the use of validated modules. &lt;/p&gt;&lt;p&gt;Any process input can be provided as a function argument using the usual channel semantics familiar to Nextflow developers. Moreover process outputs can either be assigned to a variable or accessed using the implicit &lt;code&gt;.out&lt;/code&gt; attribute in the scope implicitly defined by the process name itself. See the example below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;include { INDEX; FASTQC; QUANT; MULTIQC } from &amp;#39;./some/module/script.nf&amp;#39; 

read_pairs_ch = channel.fromFilePairs( params.reads)

workflow {
  INDEX( params.transcriptome )
  FASTQC( read_pairs_ch )
  QUANT( INDEX.out, read_pairs_ch )
  MULTIQC( QUANT.out.mix(FASTQC.out).collect(), multiqc_file )
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Also enhanced is the ability to use channels as inputs multiple times without the need to duplicate them (previously done with the special into operator) which makes the resulting pipeline code more concise, fluent and therefore readable!&lt;/p&gt;&lt;h4&gt;Sub-workflows&lt;/h4&gt;&lt;p&gt;Notably, the DSL 2 syntax allows for the definition of reusable processes as well as sub-workflow libraries. The only requirement is to provide a &lt;code&gt;workflow&lt;/code&gt; name that will be used to reference and declare the corresponding inputs and outputs using the new &lt;code&gt;take&lt;/code&gt; and &lt;code&gt;emit&lt;/code&gt; keywords. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;workflow RNASEQ {
  take:
    transcriptome
    read_pairs_ch
 
  main: 
    INDEX(transcriptome)
    FASTQC(read_pairs_ch)
    QUANT(INDEX.out, read_pairs_ch)

  emit: 
     QUANT.out.mix(FASTQC.out).collect()
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now named sub-workflows can be used in the same way as processes, allowing you to easily include and reuse multi-step workflows as part of larger workflows. Find more details &lt;a href=&quot;/docs/latest/dsl2.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h4&gt;More syntax sugar&lt;/h4&gt;&lt;p&gt;Another exciting feature of Nextflow DSL 2 is the ability to compose built-in operators, pipeline processes and sub-workflows with the pipe (|) operator! For example the last line in the above example could be written as: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;emit:
 QUANT.out | mix(FASTQC.out) | collect 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This syntax finally realizes the Nextflow vision of empowering developers to write complex data analysis applications with a simple but powerful language that mimics the expressiveness of the Unix pipe model but at the same time makes it possible to handle complex data structures and patterns as is required for highly parallelised and distributed computational workflows.&lt;/p&gt;&lt;p&gt;Another change is the introduction of &lt;code&gt;channel&lt;/code&gt; as an alternative name as a synonym of &lt;code&gt;Channel&lt;/code&gt; type identifier and therefore allows the use of &lt;code&gt;channel.fromPath&lt;/code&gt; instead of &lt;code&gt;Channel.fromPath&lt;/code&gt; and so on. This is a small syntax sugar to keep the capitazionation consistent with the rest of the language.&lt;/p&gt;&lt;p&gt;Moreover, several process inputs and outputs syntax shortcuts were removed when using the final version of DSL 2 to make it more predictable. For example, with DSL1, in a tuple input or output declaration the component type could be omitted, for example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input: 
  tuple foo, &amp;#39;bar&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;code&gt;foo&lt;/code&gt; identifier was implicitly considered an input value declaration instead the string &lt;code&gt;&amp;#39;bar&amp;#39;&lt;/code&gt; was considered a shortcut for &lt;code&gt;file(&amp;#39;bar&amp;#39;)&lt;/code&gt;. However, this was a bit confusing especially for new users and therefore using DSL 2, the fully qualified version must be used: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input: 
  tuple val(foo), path(&amp;#39;bar&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can find more detailed migration notes at &lt;a href=&quot;/docs/latest/dsl2.html#dsl2-migration-notes&quot;&gt;this link&lt;/a&gt;. &lt;/p&gt;&lt;h4&gt;What&apos;s next&lt;/h4&gt;&lt;p&gt;As always, reaching an important project milestone can be viewed as a major success, but at the same time the starting point for challenges and developments. Having a modularization mechanism opens new needs and possibilities. The first one of which will be focused on the ability to test and validate process modules independently using a unit-testing style approach. This will definitely help to make the resulting pipelines more resilient. &lt;/p&gt;&lt;p&gt;Another important area for the development of the Nextflow language will be the ability to better formalise pipeline inputs and outputs and further decouple for the process declaration. Nextflow currently strongly relies on the &lt;code&gt;publishDir&lt;/code&gt; constructor for the generation of the workflow outputs. &lt;/p&gt;&lt;p&gt;However in the new &lt;em&gt;module&lt;/em&gt; world, this approach results in &lt;code&gt;publishDir&lt;/code&gt; being tied to a single process definition. The plan is instead to extend this concept in a more general and abstract manner, so that it will be possible to capture and redirect the result of any process and sub-workflow based on semantic annotations instead of hardcoding it at the task level.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;We are extremely excited about today&apos;s release. This was a long awaited advancement and therefore we are very happy to make it available for general availability to all Nextflow users. We greatly appreciate all of the community feedback and ideas over the past year which have shaped DSL 2. &lt;/p&gt;&lt;p&gt;We are confident this represents a big step forward for the project and will enable the writing of a more scalable and complex data analysis pipeline and above all, a more enjoyable experience. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Easy provenance reporting</title>
      <link>https://www.nextflow.io/blog/2019/easy-provenance-report.html</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2019/easy-provenance-report.html</guid>
      	<description>
	&lt;p&gt;&lt;em&gt;Continuing our &lt;a href=&quot;blog/2019/demystifying-nextflow-resume.html&quot;&gt;series on understanding Nextflow resume&lt;/a&gt;, we wanted to delve deeper to show how you can report which tasks contribute to a given workflow output.&lt;/em&gt;&lt;/p&gt;&lt;h3&gt;Easy provenance reports&lt;/h3&gt;&lt;p&gt;When provided with a run name or session ID, the log command can return useful information about a pipeline execution. This can be composed to track the provenance of a workflow result.&lt;/p&gt;&lt;p&gt;When supplying a run name or session ID, the log command lists all the work directories used to compute the final result. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow log tiny_fermat

/data/.../work/7b/3753ff13b1fa5348d2d9b6f512153a
/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c
/data/.../work/f7/659c65ef60582d9713252bcfbcc310
/data/.../work/82/ba67e3175bd9e6479d4310e5a92f99
/data/.../work/e5/2816b9d4e7b402bfdd6597c2c2403d
/data/.../work/3b/3485d00b0115f89e4c202eacf82eba
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Using the option &lt;code&gt;-f&lt;/code&gt; (fields) it’s possible to specify which metadata should be printed by the log command. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow log tiny_fermat -f &amp;#39;process,exit,hash,duration&amp;#39;

index	0	7b/3753ff	2s
fastqc	0	c1/56a36d	9.3s
fastqc	0	f7/659c65	9.1s
quant	0	82/ba67e3	2.7s
quant	0	e5/2816b9	3.2s
multiqc	0	3b/3485d0	6.3s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The complete list of available fields can be retrieved with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow log -l
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The option &lt;code&gt;-F&lt;/code&gt; allows the specification of filtering criteria to print only a subset of tasks. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow log tiny_fermat -F &amp;#39;process =~ /fastqc/&amp;#39;

/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c
/data/.../work/f7/659c65ef60582d9713252bcfbcc310
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This can be useful to locate specific tasks work directories.&lt;/p&gt;&lt;p&gt;Finally, the &lt;code&gt;-t&lt;/code&gt; option allows for the creation of a basic custom HTML provenance report that can be generated by providing a template file, in any format of your choice. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;div&amp;gt;
&amp;lt;h2&amp;gt;${name}&amp;lt;/h2&amp;gt;
&amp;lt;div&amp;gt;
Script:
&amp;lt;pre&amp;gt;${script}&amp;lt;/pre&amp;gt;
&amp;lt;/div&amp;gt;

&amp;lt;ul&amp;gt;
    &amp;lt;li&amp;gt;Exit: ${exit}&amp;lt;/li&amp;gt;
    &amp;lt;li&amp;gt;Status: ${status}&amp;lt;/li&amp;gt;
    &amp;lt;li&amp;gt;Work dir: ${workdir}&amp;lt;/li&amp;gt;
    &amp;lt;li&amp;gt;Container: ${container}&amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;By saving the above snippet in a file named template.html, you can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow log tiny_fermat -t template.html &amp;gt; provenance.html
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Open it in your browser, et voilà! &lt;/p&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;This post introduces a little know Nextflow feature and it&apos;s intended to show how it can be used&lt;br/&gt;to produce a custom execution report reporting some - basic - provenance information. &lt;/p&gt;&lt;p&gt;In future releases we plan to support a more formal provenance specification and execution tracking features.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Troubleshooting Nextflow resume</title>
      <link>https://www.nextflow.io/blog/2019/troubleshooting-nextflow-resume.html</link>
      <pubDate>Mon, 1 Jul 2019 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2019/troubleshooting-nextflow-resume.html</guid>
      	<description>
	&lt;p&gt;&lt;em&gt;This two-part blog aims to help users understand Nextflow’s powerful caching mechanism. Part one describes how it works whilst part two will focus on execution provenance and troubleshooting. You can read part one &lt;a href=&quot;/blog/2019/demystifying-nextflow-resume.html&quot;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;&lt;h3&gt;Troubleshooting resume&lt;/h3&gt;&lt;p&gt;If your workflow execution is not resumed as expected, there exists several strategies to debug the problem.&lt;/p&gt;&lt;h4&gt;Modified input file(s)&lt;/h4&gt;&lt;p&gt;Make sure that there has been no change in your input files. Don’t forget the unique task hash is computed by taking into account the complete file path, the last modified timestamp and the file size. If any of these change, the workflow will be re-executed, even if the input content is the same. &lt;/p&gt;&lt;h4&gt;A process modifying one or more inputs&lt;/h4&gt;&lt;p&gt;A process should never alter input files. When this happens, the future execution of tasks will be invalidated for the same reason explained in the previous point.&lt;/p&gt;&lt;h4&gt;Inconsistent input file attributes&lt;/h4&gt;&lt;p&gt;Some shared file system, such as NFS, may report inconsistent file timestamp i.e. a different timestamp for the same file even if it has not been modified. There is an option to use the &lt;a href=&quot;https://www.nextflow.io/docs/latest/process.html#cache&quot;&gt;lenient mode of caching&lt;/a&gt; to avoid this problem.&lt;/p&gt;&lt;h4&gt;Race condition in a global variable&lt;/h4&gt;&lt;p&gt;Nextflow does its best to simplify parallel programming and to prevent race conditions and the access of shared resources. One of the few cases in which a race condition may arise is when using a global variable with two (or more) operators. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Channel
    .from(1,2,3)
    .map { it -&amp;gt; X=it; X+=2 }
    .println { &amp;quot;ch1 = $it&amp;quot; }

Channel
    .from(1,2,3)
    .map { it -&amp;gt; X=it; X*=2 }
    .println { &amp;quot;ch2 = $it&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The problem with this snippet is that the &lt;code&gt;X&lt;/code&gt; variable in the closure definition is defined in the global scope. Since operators are executed in parallel, the &lt;code&gt;X&lt;/code&gt; value can, therefore, be overwritten by the other &lt;code&gt;map&lt;/code&gt; invocation.&lt;/p&gt;&lt;p&gt;The correct implementation requires the use of the &lt;code&gt;def&lt;/code&gt; keyword to declare the variable local.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Channel
    .from(1,2,3)
    .map { it -&amp;gt; def X=it; X+=2 }
    .view { &amp;quot;ch1 = $it&amp;quot; }

Channel
    .from(1,2,3)
    .map { it -&amp;gt; def X=it; X*=2 }
    .view { &amp;quot;ch2 = $it&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;&lt;h4&gt;Non-deterministic input channels&lt;/h4&gt;&lt;p&gt;While dataflow channel ordering is guaranteed i.e. data is read in the same order in which it’s written in the channel, when a process declares as input two or more channels, each of which is the output of a different process, the overall input ordering is not consistent across different executions.&lt;/p&gt;&lt;p&gt;Consider the following snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process foo {
  input: set val(pair), file(reads) from reads_ch
  output: set val(pair), file(&amp;#39;*.bam&amp;#39;) into bam_ch
  &amp;quot;&amp;quot;&amp;quot;
  your_command --here
  &amp;quot;&amp;quot;&amp;quot;
}

process bar {
  input: set val(pair), file(reads) from reads_ch
  output: set val(pair), file(&amp;#39;*.bai&amp;#39;) into bai_ch
  &amp;quot;&amp;quot;&amp;quot;
  other_command --here
  &amp;quot;&amp;quot;&amp;quot;
}

process gather {
  input:
  set val(pair), file(bam) from bam_ch
  set val(pair), file(bai) from bai_ch
  &amp;quot;&amp;quot;&amp;quot;
  merge_command $bam $bai
  &amp;quot;&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The inputs declared in the gather process can be delivered in any order as the execution order of the process &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; is not deterministic due to parallel executions.&lt;/p&gt;&lt;p&gt;Therefore, the input of the third process needs to be synchronized using the &lt;code&gt;join&lt;/code&gt; operator or a similar approach. The third process should be written as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process gather {
  input:
  set val(pair), file(bam), file(bai) from bam_ch.join(bai_ch)
  &amp;quot;&amp;quot;&amp;quot;
  merge_command $bam $bai
  &amp;quot;&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4&gt;Still in trouble?&lt;/h4&gt;&lt;p&gt;These are most frequent causes of problems with the Nextflow resume mechanism. If you are still not able to resolve your problem, identify the first process not resuming correctly, then run your script twice using &lt;code&gt;-dump-hashes&lt;/code&gt;. You can then compare the resulting &lt;code&gt;.nextflow.log&lt;/code&gt; files (the first will be named &lt;code&gt;.nextflow.log.1&lt;/code&gt;). &lt;/p&gt;&lt;p&gt;Unfortunately, the information reported by &lt;code&gt;-dump-hashes&lt;/code&gt; can be quite cryptic, however, with the help of a good &lt;em&gt;diff&lt;/em&gt; tool it is possible to compare the two log files to identify the reason for the cache to be invalidated. &lt;/p&gt;&lt;h4&gt;The golden rule&lt;/h4&gt;&lt;p&gt;Never try to debug this kind of problem with production data! This issue can be annoying, but when it happens it should be able to be replicated in a consistent manner with any data.&lt;/p&gt;&lt;p&gt;Therefore, we always suggest Nextflow developers include in their pipeline project a small synthetic dataset to easily execute and test the complete pipeline execution in a few seconds. This is the golden rule for debugging and troubleshooting execution problems avoids getting stuck with production data.&lt;/p&gt;&lt;h4&gt;Resume by default?&lt;/h4&gt;&lt;p&gt;Given the majority of users always apply resume, we recently discussed having resume applied by the default. &lt;/p&gt;&lt;p&gt;Is there any situation where you do not use resume? Would a flag specifying &lt;code&gt;-no-cache&lt;/code&gt; be enough to satisfy these use cases? &lt;/p&gt;&lt;p&gt;We want to hear your thoughts on this. Help steer Nextflow development and vote in the twitter poll below.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Should -resume⏯️ be the default when launching a Nextflow pipeline?&lt;/p&gt;&amp;mdash; Nextflow (@nextflowio) &lt;a href=&quot;https://twitter.com/nextflowio/status/1145599932268785665?ref_src=twsrc%5Etfw&quot;&gt;July 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;br&gt; &lt;em&gt;In the following post of this series, we will show how to produce a provenance report using a built-in Nextflow command.&lt;/em&gt;&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Demystifying Nextflow resume</title>
      <link>https://www.nextflow.io/blog/2019/demystifying-nextflow-resume.html</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2019/demystifying-nextflow-resume.html</guid>
      	<description>
	&lt;p&gt;&lt;em&gt;This two-part blog aims to help users understand Nextflow’s powerful caching mechanism. Part one describes how it works whilst part two will focus on execution provenance and troubleshooting. You can read part two &lt;a href=&quot;/blog/2019/troubleshooting-nextflow-resume.html&quot;&gt;here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Task execution caching and checkpointing is an essential feature of any modern workflow manager and Nextflow provides an automated caching mechanism with every workflow execution. When using the &lt;code&gt;-resume&lt;/code&gt; flag, successfully completed tasks are skipped and the previously cached results are used in downstream tasks. But understanding the specifics of how it works and debugging situations when the behaviour is not as expected is a common source of frustration.&lt;/p&gt;&lt;p&gt;The mechanism works by assigning a unique ID to each task. This unique ID is used to create a separate execution directory, called the working directory, where the tasks are executed and the results stored. A task’s unique ID is generated as a 128-bit hash number obtained from a composition of the task’s: &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Inputs values&lt;/li&gt;
  &lt;li&gt;Input files&lt;/li&gt;
  &lt;li&gt;Command line string&lt;/li&gt;
  &lt;li&gt;Container ID&lt;/li&gt;
  &lt;li&gt;Conda environment&lt;/li&gt;
  &lt;li&gt;Environment modules&lt;/li&gt;
  &lt;li&gt;Any executed scripts in the bin directory&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;How does resume work?&lt;/h3&gt;&lt;p&gt;The &lt;code&gt;-resume&lt;/code&gt; command line option allows for the continuation of a workflow execution. It can be used in its most basic form with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run nextflow-io/hello -resume
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In practice, every execution starts from the beginning. However, when using resume, before launching a task, Nextflow uses the unique ID to check if:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the working directory exists&lt;/li&gt;
  &lt;li&gt;it contains a valid command exit status&lt;/li&gt;
  &lt;li&gt;it contains the expected output files.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;If these conditions are satisfied, the task execution is skipped and the previously computed outputs are applied. When a task requires recomputation, ie. the conditions above are not fulfilled, the downstream tasks are automatically invalidated.&lt;/p&gt;&lt;h3&gt;The working directory&lt;/h3&gt;&lt;p&gt;By default, the task work directories are created in the directory from where the pipeline is launched. This is often a scratch storage area that can be cleaned up once the computation is completed. A different location for the execution work directory can be specified using the command line option &lt;code&gt;-w&lt;/code&gt; e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run &amp;lt;script&amp;gt; -w /some/scratch/dir
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that if you delete or move the pipeline work directory, this will prevent to use the resume feature in subsequent runs. &lt;/p&gt;&lt;p&gt;Also note that the pipeline work directory is intended to be used as a temporary scratch area. The final workflow outputs are expected to be stored in a different location specified using the &lt;a href=&quot;https://www.nextflow.io/docs/latest/process.html#publishdir&quot;&gt;&lt;code&gt;publishDir&lt;/code&gt; directive&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;How is the hash calculated on input files?&lt;/h3&gt;&lt;p&gt;The hash provides a convenient way for Nextflow to determine if a task requires recomputation. For each input file, the hash code is computed with:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The complete file path&lt;/li&gt;
  &lt;li&gt;The file size&lt;/li&gt;
  &lt;li&gt;The last modified timestamp&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Therefore, even just performing a touch on a file will invalidate the task execution.&lt;/p&gt;&lt;h3&gt;How to ensure resume works as expected?&lt;/h3&gt;&lt;p&gt;It is good practice to organize each experiment in its own folder. An experiment’s input parameters can be specified using a Nextflow config file which also makes it simple to track and replicate an experiment over time. Note that you should avoid launching two (or more) Nextflow instances in the same directory concurrently.&lt;/p&gt;&lt;p&gt;The nextflow log command lists the executions run in the current folder:&lt;/p&gt;
&lt;style&gt;
pre {
    white-space: pre;
    overflow-x: auto;
}
&lt;/style&gt;
&lt;pre&gt;
$ nextflow log

TIMESTAMP            DURATION  RUN NAME          STATUS  REVISION ID  SESSION ID                            COMMAND                                    
2019-05-06 12:07:32  1.2s      focused_carson    ERR     a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run hello                         
2019-05-06 12:08:33  21.1s     mighty_boyd       OK      a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run rnaseq-nf -with-docker        
2019-05-06 12:31:15  1.2s      insane_celsius    ERR     b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf                     
2019-05-06 12:31:24  17s       stupefied_euclid  OK      b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf -resume -with-docker
&lt;/pre&gt;&lt;p&gt;You can use the resume command with either the session ID or the run name to recover a specific execution. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow run rnaseq-nf -resume mighty_boyd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;or &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run naseq-nf -resume 4dc656d2-c410-44c8-bc32-7dd0ea87bebf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Stay tuned for part two where we will discuss resume in more detail with respect to provenance and troubleshooting techniques!&lt;/em&gt;&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>One more step towards Nextflow modules</title>
      <link>https://www.nextflow.io/blog/2019/one-more-step-towards-modules.html</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2019/one-more-step-towards-modules.html</guid>
      	<description>
	&lt;p&gt;The ability to create components, libraries or module files has been among the most requested feature ever over the years. &lt;/p&gt;&lt;p&gt;For this reason, today we are very happy to announce that a preview implementation of the &lt;a href=&quot;https://github.com/nextflow-io/nextflow/issues/984&quot;&gt;modules feature&lt;/a&gt; has been merged on master branch of the project and included in the &lt;a href=&quot;https://github.com/nextflow-io/nextflow/releases/tag/v19.05.0-edge&quot;&gt;19.05.0-edge&lt;/a&gt; release. &lt;/p&gt;&lt;p&gt;The implementation of this feature has opened the possibility for many fantastic improvements to Nextflow and its syntax. We are extremely excited as it results in a radical new way of writing Nextflow applications! So much so, that we are referring to these changes as DSL 2.&lt;/p&gt;&lt;h4&gt;Enabling DSL 2 syntax&lt;/h4&gt;&lt;p&gt;Since this is still a preview technology and, above all, to not break any existing applications, to enable the new syntax you will need to add the following line at the beginning of your workflow script: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow.preview.dsl=2
&lt;/code&gt;&lt;/pre&gt;&lt;h4&gt;Module files&lt;/h4&gt;&lt;p&gt;A module file simply consists of one or more &lt;code&gt;process&lt;/code&gt; definitions, written with the usual syntax. The &lt;em&gt;only&lt;/em&gt; difference is that the &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;into&lt;/code&gt; clauses in the &lt;code&gt;input:&lt;/code&gt; and &lt;code&gt;output:&lt;/code&gt; definition blocks has to be omitted. For example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process INDEX {
  input:
    file transcriptome 
  output:
    file &amp;#39;index&amp;#39; 
  script:
    &amp;quot;&amp;quot;&amp;quot;
    salmon index --threads $task.cpus -t $transcriptome -i index
    &amp;quot;&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above snippet defines a process component that can be imported in the main application script using the &lt;code&gt;include&lt;/code&gt; statement shown below. &lt;/p&gt;&lt;p&gt;Also, module files can declare optional parameters using the usual &lt;code&gt;params&lt;/code&gt; idiom, as it can be done in any standard script file.&lt;/p&gt;&lt;p&gt;This approach, which is consistent with the current Nextflow syntax, makes very easy to migrate existing code to the new modules system, reducing it to a mere copy &amp;amp; pasting exercise in most cases. &lt;/p&gt;&lt;p&gt;You can see a complete module file &lt;a href=&quot;https://github.com/nextflow-io/rnaseq-nf/blob/66ebeea/modules/rnaseq.nf&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;Module inclusion&lt;/h3&gt;&lt;p&gt;A module file can be included into a Nextflow script using the &lt;code&gt;include&lt;/code&gt; statement. With this it becomes possible to reference any process defined in the module using the usual syntax for a function invocation, and specifying the expected input channels as they were function arguments. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow.preview.dsl=2
include &amp;#39;modules/rnaseq&amp;#39;

read_pairs_ch = Channel.fromFilePairs( params.reads, checkIfExists: true )
transcriptome_file = file( params.transcriptome )

INDEX( transcriptome_file )
FASTQC( read_pairs_ch )
QUANT( INDEX.out, read_pairs_ch )
MULTIQC( QUANT.out.mix(FASTQC.out).collect(), multiqc_file )
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notably, each process defines its own namespace in the script scope which allows the access of the process output channel(s) using the &lt;code&gt;.out&lt;/code&gt; attribute. This can be used then as any other Nextflow channel variable in your pipeline script.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;include&lt;/code&gt; statement gives also the possibility to include only a &lt;a href=&quot;https://www.nextflow.io/docs/edge/dsl2.html#selective-inclusion&quot;&gt;specific process&lt;/a&gt; or to include a process with a different &lt;a href=&quot;https://www.nextflow.io/docs/edge/dsl2.html#module-aliases&quot;&gt;name alias&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;Smart channel forking&lt;/h3&gt;&lt;p&gt;One of the most important changes of the new syntax is that any channel can be read as many times as you need removing the requirement to duplicate them using the &lt;code&gt;into&lt;/code&gt; operator. &lt;/p&gt;&lt;p&gt;For example, in the above snippet, the &lt;code&gt;read_pairs_ch&lt;/code&gt; channel has been used twice, as input both for the &lt;code&gt;FASTQC&lt;/code&gt; and &lt;code&gt;QUANT&lt;/code&gt; processes. Nextflow forks it behind the scene for you. &lt;/p&gt;&lt;p&gt;This makes the writing of workflow scripts much more fluent, readable and ... fun! No more channel names proliferation!&lt;/p&gt;&lt;h3&gt;Nextflow pipes!&lt;/h3&gt;&lt;p&gt;Finally, maybe our favourite one. The new DSL introduces the &lt;code&gt;|&lt;/code&gt; (pipe) operator which allows for the composition of Nextflow channels, processes and operators together seamlessly in a much more expressive way. &lt;/p&gt;&lt;p&gt;Consider the following example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process align {
  input:
    file seq
  output:
    file &amp;#39;result&amp;#39;

  &amp;quot;&amp;quot;&amp;quot;
    t_coffee -in=${seq} -out result
  &amp;quot;&amp;quot;&amp;quot;
}

Channel.fromPath(params.in) | splitFasta | align | view { it.text }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the last line, the &lt;code&gt;fromPath&lt;/code&gt; channel is piped to the &lt;a href=&quot;https://www.nextflow.io/docs/latest/operator.html#splitfasta&quot;&gt;&lt;code&gt;splitFasta&lt;/code&gt;&lt;/a&gt; operator whose result is used as input by the &lt;code&gt;align&lt;/code&gt; process. Then the output is finally printed by the &lt;a href=&quot;https://www.nextflow.io/docs/latest/operator.html#view&quot;&gt;&lt;code&gt;view&lt;/code&gt;&lt;/a&gt; operator.&lt;/p&gt;&lt;p&gt;This syntax finally realizes the Nextflow vision of empowering developers to write complex data analysis applications with a simple but powerful language that mimics the expressiveness of the Unix pipe model but at the same time makes it possible to handle complex data structures and patterns as is required for highly parallelised and distributed computational workflows. &lt;/p&gt;&lt;h4&gt;Conclusion&lt;/h4&gt;&lt;p&gt;This wave of improvements brings a radically new experience when it comes to writing Nextflow workflows. We are releasing it as a preview technology to allow users to try, test, provide their feedback and give us the possibility stabilise it. &lt;/p&gt;&lt;p&gt;We are also working to other important enhancements that will be included soon, such as remote modules, sub-workflows composition, simplified file path wrangling and more. Stay tuned!&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nextflow 19.04.0 stable release is out!</title>
      <link>https://www.nextflow.io/blog/2019/release-19.04.0-stable.html</link>
      <pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2019/release-19.04.0-stable.html</guid>
      	<description>
	&lt;p&gt;We are excited to announce the new Nextflow 19.04.0 stable release!&lt;/p&gt;&lt;p&gt;This version includes numerous bug fixes, enhancement and new features.&lt;/p&gt;&lt;h4&gt;Rich logging&lt;/h4&gt;&lt;p&gt;In this release, we are making the new interactive rich output using ANSI escape characters as the default logging option. This produces a much more readable and easy to follow log of the running workflow execution. &lt;/p&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/IrT6uo85yyVoOjPa6KVzT2FXQ.js&quot; id=&quot;asciicast-IrT6uo85yyVoOjPa6KVzT2FXQ&quot; async&gt;&lt;/script&gt;&lt;p&gt;The ANSI log is implicitly disabled when the nextflow is launched in the background i.e. when using the &lt;code&gt;-bg&lt;/code&gt; option. It can also be explicitly disabled using the &lt;code&gt;-ansi-log false&lt;/code&gt; option or setting the &lt;code&gt;NXF_ANSI_LOG=false&lt;/code&gt; variable in your launching environment. &lt;/p&gt;&lt;h4&gt;NCBI SRA data source&lt;/h4&gt;&lt;p&gt;The support for NCBI SRA archive was introduced in the &lt;a href=&quot;/blog/2019/release-19.03.0-edge.html&quot;&gt;previous edge release&lt;/a&gt;. Given the very positive reaction, we are graduating this feature into the stable release for general availability. &lt;/p&gt;&lt;h4&gt;Sharing&lt;/h4&gt;&lt;p&gt;This version includes also a new Git repository provider for the &lt;a href=&quot;https://gitea.io&quot;&gt;Gitea&lt;/a&gt; self-hosted source code management system, which is added to the already existing support for GitHub, Bitbucket and GitLab sharing platforms.&lt;/p&gt;&lt;h4&gt;Reports and metrics&lt;/h4&gt;&lt;p&gt;Finally, this version includes important enhancements and bug fixes for the task executions metrics collected by Nextflow. If you are using this feature we strongly suggest updating Nextflow to this version.&lt;/p&gt;&lt;p&gt;Remember that updating can be done with the &lt;code&gt;nextflow -self-update&lt;/code&gt; command.&lt;/p&gt;&lt;h3&gt;Changelog&lt;/h3&gt;&lt;p&gt;The complete list of changes and bug fixes is available on GitHub at &lt;a href=&quot;https://github.com/nextflow-io/nextflow/releases/tag/v19.04.0&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Contributions&lt;/h3&gt;&lt;p&gt;Special thanks to all people contributed to this release by reporting issues, improving the docs or submitting (patiently) a pull request (sorry if we have missed somebody):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/acerjanic&quot;&gt;Alex Cerjanic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/aunderwo&quot;&gt;Anthony Underwood&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pachiras&quot;&gt;Akira Sekiguchi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wflynny&quot;&gt;Bill Flynn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/glormph&quot;&gt;Jorrit Boekel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/olgabot&quot;&gt;Olga Botvinnik&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/olifly&quot;&gt;Ólafur Haukur Flygenring&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sven1103&quot;&gt;Sven Fillinger&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
	</description>
    </item>
    <item>
      <title>Edge release 19.03: The Sequence Read Archive &amp; more!</title>
      <link>https://www.nextflow.io/blog/2019/release-19.03.0-edge.html</link>
      <pubDate>Tue, 19 Mar 2019 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2019/release-19.03.0-edge.html</guid>
      	<description>
	&lt;p&gt;It&apos;s time for the monthly Nextflow release for March, &lt;em&gt;edge&lt;/em&gt; version 19.03. This is another great release with some cool new features, bug fixes and improvements.&lt;/p&gt;&lt;h3&gt;SRA channel factory&lt;/h3&gt;&lt;p&gt;This sees the introduction of the long-awaited sequence read archive (SRA) channel factory. The &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/sra&quot;&gt;SRA&lt;/a&gt; is a key public repository for sequencing data and run in coordination between The National Center for Biotechnology Information (NCBI), The European Bioinformatics Institute (EBI) and the DNA Data Bank of Japan (DDBJ).&lt;/p&gt;&lt;p&gt;This feature originates all the way back in &lt;a href=&quot;https://github.com/nextflow-io/nextflow/issues/89&quot;&gt;2015&lt;/a&gt; and was worked on during a 2018 Nextflow hackathon. It was brought to fore again thanks to the release of Phil Ewels&apos; excellent &lt;a href=&quot;https://ewels.github.io/sra-explorer/&quot;&gt;SRA Explorer&lt;/a&gt;. The SRA channel factory allows users to pull read data in FASTQ format directly from SRA by referencing a study, accession ID or even a keyword. It works in a similar way to &lt;a href=&quot;https://www.nextflow.io/docs/latest/channel.html#fromfilepairs&quot;&gt;&lt;code&gt;fromFilePairs&lt;/code&gt;&lt;/a&gt;, returning a sample ID and files (single or pairs of files) for each sample.&lt;/p&gt;&lt;p&gt;The code snippet below creates a channel containing 24 samples from a chromatin dynamics study and runs FASTQC on the resulting files.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Channel
    .fromSRA(&amp;#39;SRP043510&amp;#39;)
    .set{reads}

process fastqc {
    input:
    set sample_id, file(reads_file) from reads

    output:
    file(&amp;quot;fastqc_${sample_id}_logs&amp;quot;) into fastqc_ch

    script:
    &amp;quot;&amp;quot;&amp;quot;
    mkdir fastqc_${sample_id}_logs
    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads_file}
    &amp;quot;&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href=&quot;https://www.nextflow.io/docs/edge/channel.html#fromsra&quot;&gt;documentation&lt;/a&gt; for more details. When combined with downstream processes, you can quickly open a firehose of data on your workflow!&lt;/p&gt;&lt;h3&gt;Edge release&lt;/h3&gt;&lt;p&gt;Note that this is a monthly edge release. To use it simply execute the following command prior to running Nextflow: &lt;code&gt;
export NXF_VER=19.03.0-edge
&lt;/code&gt;&lt;/p&gt;&lt;h3&gt;If you need help&lt;/h3&gt;&lt;p&gt;Please don’t hesitate to use our very active &lt;a href=&quot;https://gitter.im/nextflow-io/nextflow&quot;&gt;Gitter&lt;/a&gt; channel or create a thread in the &lt;a href=&quot;https://groups.google.com/forum/#!forum/nextflow&quot;&gt;Google discussion group&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Reporting Issues&lt;/h3&gt;&lt;p&gt;Experiencing issues introduced by this release? Please report them in our &lt;a href=&quot;https://github.com/nextflow-io/nextflow/issues&quot;&gt;issue tracker&lt;/a&gt;. Make sure to fill in the fields of the issue template.&lt;/p&gt;&lt;h3&gt;Contributions&lt;/h3&gt;&lt;p&gt;Special thanks to the contributors of this release:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Akira Sekiguchi - &lt;a href=&quot;https://github.com/pachiras&quot;&gt;pachiras&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jon Haitz Legarreta Gorroño - &lt;a href=&quot;https://github.com/jhlegarreta&quot;&gt;jhlegarreta&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jonathan Leitschuh - &lt;a href=&quot;https://github.com/JLLeitschuh&quot;&gt;JLLeitschuh&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Kevin Sayers - &lt;a href=&quot;https://github.com/KevinSayers&quot;&gt;KevinSayers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lukas Jelonek - &lt;a href=&quot;https://github.com/lukasjelonek&quot;&gt;lukasjelonek&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Paolo Di Tommaso - &lt;a href=&quot;https://github.com/pditommaso&quot;&gt;pditommaso&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Toni Hermoso Pulido - &lt;a href=&quot;https://github.com/toniher&quot;&gt;toniher&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Philippe Hupé &lt;a href=&quot;https://github.com/phupe&quot;&gt;phupe&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/phue&quot;&gt;phue&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Complete changes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Fix Nextflow hangs submitting jobs to AWS batch #1024&lt;/li&gt;
  &lt;li&gt;Fix process builder incomplete output [2fe1052c]&lt;/li&gt;
  &lt;li&gt;Fix Grid executor reports invalid queue status #1045&lt;/li&gt;
  &lt;li&gt;Fix Script execute permission is lost in container #1060&lt;/li&gt;
  &lt;li&gt;Fix K8s serviceAccount is not honoured #1049&lt;/li&gt;
  &lt;li&gt;Fix K8s kuberun login path #1072&lt;/li&gt;
  &lt;li&gt;Fix K8s imagePullSecret and imagePullPolicy #1062&lt;/li&gt;
  &lt;li&gt;Fix Google Storage docs #1023&lt;/li&gt;
  &lt;li&gt;Fix Env variable NXF_CONDA_CACHEDIR is ignored #1051&lt;/li&gt;
  &lt;li&gt;Fix failing task due to legacy sleep command [3e150b56]&lt;/li&gt;
  &lt;li&gt;Fix SplitText operator should accept a closure parameter #1021&lt;/li&gt;
  &lt;li&gt;Add Channel.fromSRA factory method #1070&lt;/li&gt;
  &lt;li&gt;Add voluntary/involuntary context switches to metrics #1047&lt;/li&gt;
  &lt;li&gt;Add noHttps option to singularity config #1041&lt;/li&gt;
  &lt;li&gt;Add docker-daemon Singularity support #1043 [dfef1391]&lt;/li&gt;
  &lt;li&gt;Use peak_vmem and peak_rss as default output in the trace file instead of rss and vmem #1020&lt;/li&gt;
  &lt;li&gt;Improve ansi log rendering #996 [33038a18]&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Breaking changes:&lt;/h3&gt;&lt;p&gt;None known.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Bringing Nextflow to Google Cloud Platform with WuXi NextCODE</title>
      <link>https://www.nextflow.io/blog/2018/bringing-nextflow-to-google-cloud-wuxinextcode.html</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2018/bringing-nextflow-to-google-cloud-wuxinextcode.html</guid>
      	<description>
	&lt;div class=&apos;text-muted&apos; style=&apos;margin-bottom:2em&apos;&gt;
&lt;i&gt;This is a guest post authored by Halli Bjornsson, Head of Product Development Operations at WuXi NextCODE and Jonathan Sheffi, Product Manager, Biomedical Data at Google Cloud.
&lt;/i&gt;
&lt;/div&gt;&lt;p&gt;Google Cloud and WuXi NextCODE are dedicated to advancing the state of the art in biomedical informatics, especially through open source, which allows developers to collaborate broadly and deeply.&lt;/p&gt;&lt;p&gt;WuXi NextCODE is itself a user of Nextflow, and Google Cloud has many customers that use Nextflow. Together, we’ve collaborated to deliver Google Cloud Platform (GCP) support for Nextflow using the &lt;a href=&quot;https://cloud.google.com/genomics/pipelines&quot;&gt;Google Pipelines API&lt;/a&gt;. Pipelines API is a managed computing service that allows the execution of containerized workloads on GCP.&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;column&quot;&gt;
  &lt;img src=&apos;/img/google-cloud.svg&apos; style=&apos;width:80%; padding:1.7em; &apos;&gt;
  &lt;/div&gt;
  &lt;div class=&quot;column&quot;&gt;
  &lt;img src=&apos;/img/wuxi-nextcode.jpeg&apos; style=&apos;width:80%; padding:1em&apos;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
.column {
  float: left;
  width: 50%;
}

.row:after {
  content: &quot;&quot;;
  display: table;
  clear: both;
}
&lt;/style&gt;&lt;p&gt;Nextflow now provides built-in support for Google Pipelines API which allows the seamless deployment of a Nextflow pipeline in the cloud, offloading the process executions as pipelines running on Google&apos;s scalable infrastructure with a few commands. This makes it even easier for customers and partners like WuXi NextCODE to process biomedical data using Google Cloud.&lt;/p&gt;&lt;h3&gt;Get started!&lt;/h3&gt;&lt;p&gt;This feature is currently available in the Nextflow edge channel. Follow these steps to get started:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Install Nextflow from the edge channel exporting the variables shown below and then running the usual Nextflow installer Bash snippet:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;export NXF_VER=18.12.0-edge
export NXF_MODE=google
curl https://get.nextflow.io | bash
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://console.cloud.google.com/flows/enableapi?apiid=genomics.googleapis.com,compute.googleapis.com,storage-api.googleapis.com&quot;&gt;Enable the Google Genomics API for your GCP projects&lt;/a&gt;. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/docs/authentication/production#obtaining_and_providing_service_account_credentials_manually&quot;&gt;Download and set credentials for your Genomics API-enabled project&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Change your &lt;code&gt;nextflow.config&lt;/code&gt; file to use the Google Pipelines executor and specify the required config values for it as &lt;a href=&quot;/docs/edge/google.html#google-pipelines&quot;&gt;described in the documentation&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Finally, run your script with Nextflow like usual, specifying a Google Storage bucket as the pipeline work directory with the &lt;code&gt;-work-dir&lt;/code&gt; option. For example:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;nextflow run rnaseq-nf -work-dir gs://your-bucket/scratch
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;br&gt; You can find more detailed info about available configuration settings and deployment options at &lt;a href=&quot;/docs/edge/google.html&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We’re thrilled to make this contribution available to the Nextflow community!&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Goodbye zero, Hello Apache!</title>
      <link>https://www.nextflow.io/blog/2018/goodbye-zero-hello-apache.html</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2018/goodbye-zero-hello-apache.html</guid>
      	<description>
	&lt;p&gt;Today marks an important milestone in the Nextflow project. We are thrilled to announce three important changes to better meet users’ needs and ground the project on a solid foundation upon which to build a vibrant ecosystem of tools and data analysis applications for genomic research and beyond.&lt;/p&gt;&lt;h3&gt;Apache license&lt;/h3&gt;&lt;p&gt;Nextflow was originally licensed as GPLv3 open source software more than five years ago. GPL is designed to promote the adoption and spread of open source software and culture. On the other hand it has also some controversial side-effects, such as the one on &lt;a href=&quot;https://copyleft.org/guide/comprehensive-gpl-guidech5.html&quot; target=&quot;_blank&quot; &gt;derivate works&lt;/a&gt; and &lt;a href=&quot;https://opensource.com/law/14/7/lawsuit-threatens-break-new-ground-gpl-and-software-licensing-issues&quot; target=&quot;_blank&quot;&gt;legal implications&lt;/a&gt; which make the use of GPL released software a headache in many organisations. We have previously discussed these concerns in &lt;a href=&quot;/blog/2018/clarification-about-nextflow-license.html&quot; target=&quot;_blank&quot;&gt;this blog post&lt;/a&gt; and, after community feedback, have opted to change the project license to Apache 2.0.&lt;/p&gt;&lt;p&gt;This is a popular permissive free software license written by the &lt;a href=&quot;https://www.apache.org/&quot; target=&quot;_blank&quot; &gt;Apache Software Foundation&lt;/a&gt; (ASF). Software distributed with this license requires the preservation of the copyright notice and disclaimer. It allows the freedom to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software without dictating the licence terms of the resulting applications and derivative works. We are sure this licensing model addresses the concerns raised by the Nextflow community and will boost further project developments.&lt;/p&gt;&lt;h3&gt;New release schema&lt;/h3&gt;&lt;p&gt;In the time since Nextflow was open sourced, we have released 150 versions which have been used by many organizations to deploy critical production workflows on a large range of computational platforms and under heavy loads and stress conditions.&lt;/p&gt;&lt;p&gt;For example, at the Centre for Genomic Regulation (CRG) alone, Nextflow has been used to deploy data intensive computation workflows since 2014, and it has orchestrated the execution of over 12 million jobs totalling 1.4 million CPU-hours.&lt;/p&gt;&lt;p&gt;&lt;img src=&apos;/img/nextflow-release-schema-01.png&apos; alt=&quot;Nextflow release schema&quot; style=&apos;float:right; width: 240pt; margin-top: -20px; margin-left: 20px&apos; /&gt;&lt;/p&gt;&lt;p&gt;This extensive use across different execution environments has resulted in a reliable software package, and it&apos;s therefore finally time to declare Nextflow stable and drop the zero from the version number!&lt;/p&gt;&lt;p&gt;From today onwards, Nextflow will use a 3 monthly time-based &lt;em&gt;stable&lt;/em&gt; release cycle. Today&apos;s release is numbered as &lt;strong&gt;18.10&lt;/strong&gt;, the next one will be on January 2019, numbered as 19.01, and so on. This gives our users a more predictable release cadence and allows us to better focus on new feature development and scheduling. &lt;/p&gt;&lt;p&gt;Along with the 3-months stable release cycle, we will provide a monthly &lt;em&gt;edge&lt;/em&gt; release, which will include access to the latest experimental features and developments. As such, it should only be used for evaluation and testing purposes.&lt;/p&gt;&lt;h3&gt;Commercial support&lt;/h3&gt;&lt;p&gt;Finally, for organisations requiring commercial support, we have recently incorporated &lt;a href=&apos;https://www.seqera.io/&apos; target=&apos;_blank&apos;&gt;Seqera Labs&lt;/a&gt;, a spin-off of the Centre for Genomic Regulation.&lt;/p&gt;&lt;p&gt;Seqera Labs will foster Nextflow adoption as professional open source software by providing commercial support services and exploring new innovative products and solutions. &lt;/p&gt;&lt;p&gt;It&apos;s important to highlight that Seqera Labs will not close or make Nextflow a commercial project. Nextflow is and will continue to be owned by the CRG and the other contributing organisations and individuals.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The Nextflow project has reached an important milestone. In the last five years it has grown and managed to become a stable technology used by thousands of people daily to deploy large scale workloads for life science data analysis applications and beyond. The project is now exiting from the experimental stage.&lt;/p&gt;&lt;p&gt;With the above changes we want to fulfil the needs of researchers, for a reliable tool enabling scalable and reproducible data analysis, along with the demand of production oriented users, who require reliable support and services for critical deployments. &lt;/p&gt;&lt;p&gt;Above all, our aim is to strengthen the community effort around the Nextflow ecosystem and make it a sustainable and solid technology in the long run.&lt;/p&gt;&lt;h3&gt;Credits&lt;/h3&gt;&lt;p&gt;We want to say thank you to all the people who have supported and contributed to this project to this stage. First of all to Cedric Notredame for his long term commitment to the project within the Comparative Bioinformatics group at CRG. The Open Bioinformatics Foundation (OBF) in the name of Chris Fields and The Ontario Institute for Cancer Research (OICR), namely Dr Lincoln Stein, for supporting the Nextflow change of license. The CRG TBDO department, and in particular Salvatore Cappadona for his continued support and advice. Finally, the user community who with their feedback and constructive criticism contribute everyday to make this project more stable, useful and powerful.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nextflow meets Dockstore</title>
      <link>https://www.nextflow.io/blog/2018/nextflow-meets-dockstore.html</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2018/nextflow-meets-dockstore.html</guid>
      	<description>
	&lt;div class=&apos;text-muted&apos; style=&apos;margin-bottom:2em&apos;&gt;
&lt;i&gt;This post is co-authored with Denis Yuen, lead of the Dockstore project at the Ontario Institute for Cancer Research&lt;/i&gt;
&lt;/div&gt;&lt;p&gt;One key feature of Nextflow is the ability to automatically pull and execute a workflow application directly from a sharing platform such as GitHub. We realised this was critical to allow users to properly track code changes and releases and, above all, to enable the &lt;a href=&quot;/blog/2016/best-practice-for-reproducibility.html&quot;&gt;seamless sharing of workflow projects&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Nextflow never wanted to implement its own centralised workflow registry because we thought that in order for a registry to be viable and therefore useful, it should be technology agnostic and it should be driven by a consensus among the wider user community. &lt;/p&gt;&lt;p&gt;This is exactly what the &lt;a href=&quot;https://dockstore.org/&quot;&gt;Dockstore&lt;/a&gt; project is designed for and for this reason we are thrilled to announce that Dockstore has just released the support for Nextflow workflows in its latest release! &lt;/p&gt;&lt;h3&gt;Dockstore in a nutshell&lt;/h3&gt;&lt;p&gt;Dockstore is an open platform that collects and catalogs scientific data analysis tools and workflows, starting from the genomics community. It’s developed by the &lt;a href=&quot;https://oicr.on.ca/&quot;&gt;OICR&lt;/a&gt; in collaboration with &lt;a href=&quot;https://ucscgenomics.soe.ucsc.edu/&quot;&gt;UCSC&lt;/a&gt; and it is based on the &lt;a href=&quot;https://www.ga4gh.org/&quot;&gt;GA4GH&lt;/a&gt; open standards and the FAIR principles i.e. the idea to make research data and applications findable, accessible, interoperable and reusable (&lt;a href=&quot;https://www.nature.com/articles/sdata201618&quot;&gt;FAIR&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;&lt;img src=&apos;/img/dockstore.png&apos; alt=&quot;Dockstore logo&quot; style=&apos;float:right; width: 150pt; padding: .5em;&apos; /&gt;&lt;/p&gt;&lt;p&gt;In Dockstore’s initial release of support for Nextflow, users will be able to register and display Nextflow workflows. Many of Dockstore’s cross-language features will be available such as &lt;a href=&quot;https://dockstore.org/search?descriptorType=nfl&amp;searchMode=files&quot;&gt;searching&lt;/a&gt;, displaying metadata information on authorship from Nextflow’s config (&lt;a href=&quot;https://www.nextflow.io/docs/latest/config.html?highlight=author#scope-manifest&quot;&gt;author and description&lt;/a&gt;), displaying the &lt;a href=&quot;https://dockstore.org/workflows/github.com/nf-core/hlatyping:1.1.1?tab=tools&quot;&gt;Docker images&lt;/a&gt; used by a workflow, and limited support for displaying a visualization of the &lt;a href=&quot;https://dockstore.org/workflows/github.com/nf-core/hlatyping:1.1.1?tab=dag&quot;&gt;workflow structure&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The Dockstore team will initially work to on-board the high-quality &lt;a href=&quot;https://github.com/nf-core&quot;&gt;nf-core&lt;/a&gt; workflows curated by the Nextflow community. However, all developers that develop Nextflow workflows will be able to login, contribute, and maintain workflows starting with our standard &lt;a href=&quot;https://docs.dockstore.org/docs/publisher-tutorials/workflows/&quot;&gt;workflow tutorials&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Moving forward, the Dockstore team hopes to engage more with the Nextflow community and integrate Nextflow code in order to streamline the process of publishing Nextflow workflows and draw better visualizations of Nextflow workflows. Dockstore also hopes to work with a cloud vendor to add browser based launch-with support for Nextflow workflows. &lt;/p&gt;&lt;p&gt;Finally, support for Nextflow workflows in Dockstore will also enable the possibility of cloud platforms that implement &lt;a href=&quot;https://github.com/ga4gh/workflow-execution-service-schemas&quot;&gt;GA4GH WES&lt;/a&gt; to run Nextflow workflows. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;We welcome the support for Nextflow workflows in the Dockstore platform. This is a valuable contribution and presents great opportunities for workflow developers and the wider scientific community.&lt;/p&gt;&lt;p&gt;We invite all Nextflow developers to register their data analysis applications in the Dockstore platform to make them accessible and reusable to a wider community of researchers.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Clarification about the Nextflow license</title>
      <link>https://www.nextflow.io/blog/2018/clarification-about-nextflow-license.html</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2018/clarification-about-nextflow-license.html</guid>
      	<description>
	&lt;p&gt;Over past week there was some discussion on social media regarding the Nextflow license and its impact on users&apos; workflow applications. &lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet tw-align-center&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;… don’t use Nextflow, yo. &lt;a href=&quot;https://t.co/Paip5W1wgG&quot;&gt;https://t.co/Paip5W1wgG&lt;/a&gt;&lt;/p&gt;&amp;mdash; Konrad Rudolph 👨‍🔬💻 (@klmr) &lt;a href=&quot;https://twitter.com/klmr/status/1016606226103357440?ref_src=twsrc%5Etfw&quot;&gt;July 10, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet tw-align-center&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This is certainly disappointing. An argument in favor of writing workflows in &lt;a href=&quot;https://twitter.com/commonwl?ref_src=twsrc%5Etfw&quot;&gt;@commonwl&lt;/a&gt;, which is independent of the execution engine. &lt;a href=&quot;https://t.co/mIbdLQQxmf&quot;&gt;https://t.co/mIbdLQQxmf&lt;/a&gt;&lt;/p&gt;&amp;mdash; John Didion (@jdidion) &lt;a href=&quot;https://twitter.com/jdidion/status/1016612435938160640?ref_src=twsrc%5Etfw&quot;&gt;July 10, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet tw-align-center&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;GPL is generally considered toxic to companies due to fear of the viral nature of the license.&lt;/p&gt;&amp;mdash; Jeff Gentry (@geoffjentry) &lt;a href=&quot;https://twitter.com/geoffjentry/status/1016656901139025921?ref_src=twsrc%5Etfw&quot;&gt;July 10, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;h3&gt;What&apos;s the problem with GPL?&lt;/h3&gt;&lt;p&gt;Nextflow has been released under the GPLv3 license since its early days &lt;a href=&quot;https://github.com/nextflow-io/nextflow/blob/c080150321e5000a2c891e477bb582df07b7f75f/src/main/groovy/nextflow/Nextflow.groovy&quot;&gt;over 5 years ago&lt;/a&gt;. GPL is a very popular open source licence used by many projects (like, for example, &lt;a href=&quot;https://www.kernel.org/doc/html/v4.17/process/license-rules.html&quot;&gt;Linux&lt;/a&gt; and &lt;a href=&quot;https://git-scm.com/about/free-and-open-source&quot;&gt;Git&lt;/a&gt;) and it has been designed to promote the adoption and spread of open source software and culture. &lt;/p&gt;&lt;p&gt;With this idea in mind, GPL requires the author of a piece of software, &lt;em&gt;derived&lt;/em&gt; from a GPL licensed application or library, to distribute it using the same license i.e. GPL itself.&lt;/p&gt;&lt;p&gt;This is generally good, because this requirement incentives the growth of the open source ecosystem and the adoption of open source software more widely. &lt;/p&gt;&lt;p&gt;However, this is also a reason for concern by some users and organizations because it&apos;s perceived as too strong requirement by copyright holders (who may not want to disclose their code) and because it can be difficult to interpret what a *derived* application is. See for example &lt;a href=&quot;http://ivory.idyll.org/blog/2015-on-licensing-in-bioinformatics.html&quot;&gt;this post by Titus Brown&lt;/a&gt; at this regard. &lt;/p&gt;&lt;h4&gt;What&apos;s the impact of the Nextflow license on my application?&lt;/h4&gt;&lt;p&gt;If you are not distributing your application, based on Nextflow, it doesn&apos;t affect you in any way. If you are distributing an application that requires Nextflow to be executed, technically speaking your application is dynamically linking to the Nextflow runtime and it uses routines provided by it. For this reason your application should be released as GPLv3. See &lt;a href=&quot;https://www.gnu.org/licenses/gpl-faq.en.html#GPLStaticVsDynamic&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.gnu.org/licenses/gpl-faq.en.html#IfInterpreterIsGPL&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;However, this was not our original intention. We don’t consider workflow applications to be subject to the GPL copyleft obligations of the GPL even though they may link dynamically to Nextflow functionality through normal calls and we are not interested to enforce the license requirement to third party workflow developers and organizations. Therefore you can distribute your workflow application using the license of your choice. For other kind of derived applications the GPL license should be used, though. &lt;/b&gt;&lt;/p&gt;&lt;h3&gt;That&apos;s all?&lt;/h3&gt;&lt;p&gt;No. We are aware that this is not enough and the GPL licence can impose some limitation in the usage of Nextflow to some users and organizations. For this reason we are working with the CRG legal department to move Nextflow to a more permissive open source license. This is primarily motivated by our wish to make it more adaptable and compatible with all the different open source ecosystems, but also to remove any remaining legal uncertainty that using Nextflow through linking with its functionality may cause. &lt;/p&gt;&lt;p&gt;We are expecting that this decision will be made over the summer so stay tuned and continue to enjoy Nextflow.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Conda support has landed!</title>
      <link>https://www.nextflow.io/blog/2018/conda-support-has-landed.html</link>
      <pubDate>Tue, 5 Jun 2018 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2018/conda-support-has-landed.html</guid>
      	<description>
	&lt;p&gt;Nextflow aims to ease the development of large scale, reproducible workflows allowing&lt;br/&gt;developers to focus on the main application logic and to rely on best community tools and best practices.&lt;/p&gt;&lt;p&gt;For this reason we are very excited to announce that the latest Nextflow version (&lt;code&gt;0.30.0&lt;/code&gt;) finally provides built-in support for &lt;a href=&quot;https://conda.io/docs/&quot;&gt;Conda&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Conda is a popular package manager that simplifies the installation of software packages and the configuration of complex software environments. Above all, it provides access to large tool and software package collections maintained by domain specific communities such as &lt;a href=&quot;https://bioconda.github.io&quot;&gt;Bioconda&lt;/a&gt; and &lt;a href=&quot;https://biobuilds.org/&quot;&gt;BioBuild&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The native integration with Nextflow allows researchers to develop workflow applications in a rapid and easy repeatable manner, reusing community tools, whilst taking advantage of the configuration flexibility, portability and scalability provided by Nextflow.&lt;/p&gt;&lt;h3&gt;How it works&lt;/h3&gt;&lt;p&gt;Nextflow automatically creates and activates the Conda environment(s) given the dependencies specified by each process.&lt;/p&gt;&lt;p&gt;Dependencies are specified by using the &lt;a href=&quot;/docs/latest/process.html#conda&quot;&gt;conda&lt;/a&gt; directive, providing either the names of the required Conda packages, the path of a Conda environment yaml file or the path of an existing Conda environment directory.&lt;/p&gt;&lt;p&gt;Conda environments are stored on the file system. By default Nextflow instructs Conda to save the required environments in the pipeline work directory. You can specify the directory where the Conda environments are stored using the &lt;code&gt;conda.cacheDir&lt;/code&gt; configuration property.&lt;/p&gt;&lt;h4&gt;Use Conda package names&lt;/h4&gt;&lt;p&gt;The simplest way to use one or more Conda packages consists in specifying their names using the &lt;code&gt;conda&lt;/code&gt; directive. Multiple package names can be specified by separating them with a space. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process foo {
    conda &amp;quot;bwa samtools multiqc&amp;quot;

    &amp;quot;&amp;quot;&amp;quot;
    your_command --here
    &amp;quot;&amp;quot;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Using the above definition a Conda environment that includes BWA, Samtools and MultiQC tools is created and activated when the process is executed.&lt;/p&gt;&lt;p&gt;The usual Conda package syntax and naming conventions can be used. The version of a package can be specified after the package name as shown here: &lt;code&gt;bwa=0.7.15&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The name of the channel where a package is located can be specified prefixing the package with the channel name as shown here: &lt;code&gt;bioconda::bwa=0.7.15&lt;/code&gt;.&lt;/p&gt;&lt;h4&gt;Use Conda environment files&lt;/h4&gt;&lt;p&gt;When working in a project requiring a large number of dependencies it can be more convenient to consolidate all required tools using a Conda environment file. This is a file that lists the required packages and channels, structured using the YAML format. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: my-env
channels:
  - bioconda
  - conda-forge
  - defaults
dependencies:
  - star=2.5.4a
  - bwa=0.7.15
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The path of the environment file can be specified using the &lt;code&gt;conda&lt;/code&gt; directive:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process foo {
  conda &amp;#39;/some/path/my-env.yaml&amp;#39;

  &amp;#39;&amp;#39;&amp;#39;
  your_command --here
  &amp;#39;&amp;#39;&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note: the environment file name &lt;strong&gt;must&lt;/strong&gt; end with a &lt;code&gt;.yml&lt;/code&gt; or &lt;code&gt;.yaml&lt;/code&gt; suffix otherwise it won&apos;t be properly recognized. Also relative paths are resolved against the workflow launching directory. &lt;/p&gt;&lt;p&gt;The suggested approach is to store the the Conda environment file in your project root directory and reference it in the &lt;code&gt;nextflow.config&lt;/code&gt; directory using the &lt;code&gt;baseDir&lt;/code&gt; variable as shown below: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process.conda = &amp;quot;$baseDir/my-env.yaml&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This guarantees that the environment paths is correctly resolved independently of the execution path. &lt;/p&gt;&lt;p&gt;See the &lt;a href=&quot;/docs/latest/conda.html&quot;&gt;documentation&lt;/a&gt; for more details on how to configure and use Conda environments in your Nextflow workflow. &lt;/p&gt;&lt;h3&gt;Bonus!&lt;/h3&gt;&lt;p&gt;This release includes also a better support for &lt;a href=&quot;https://biocontainers.pro/&quot;&gt;Biocontainers&lt;/a&gt;. So far, Nextflow users were able to use container images provided by the Biocontainers community. However, it was not possible to collect process metrics and runtime statistics within those images due to the usage of a legacy version of the &lt;code&gt;ps&lt;/code&gt; system tool that is not compatible with the one expected by Nextflow. &lt;/p&gt;&lt;p&gt;The latest version of Nextflow does not require the &lt;code&gt;ps&lt;/code&gt; tool any more to fetch execution metrics and runtime statistics, therefore this information is collected and correctly reported when using Biocontainers images.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;We are very excited by this new feature bringing the ability to use popular Conda tool collections, such as Bioconda, directly into Nextflow workflow applications. &lt;/p&gt;&lt;p&gt;Nextflow developers have now yet another option to transparently manage the dependencies in their workflows along with &lt;a href=&quot;/docs/latest/process.html#module&quot;&gt;Environment Modules&lt;/a&gt; and &lt;a href=&quot;/docs/latest/docker.html&quot;&gt;containers&lt;/a&gt; &lt;a href=&quot;/docs/latest/singularity.html&quot;&gt;technology&lt;/a&gt;, giving them great configuration flexibility. &lt;/p&gt;&lt;p&gt;The resulting workflow applications can easily be reconfigured and deployed across a range of different platforms choosing the best technology according to the requirements of the target system. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nextflow turns five! Happy birthday!</title>
      <link>https://www.nextflow.io/blog/2018/nextflow-turns-5.html</link>
      <pubDate>Tue, 3 Apr 2018 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2018/nextflow-turns-5.html</guid>
      	<description>
	&lt;p&gt;Nextflow is growing up. The past week marked five years since the &lt;a href=&quot;https://github.com/nextflow-io/nextflow/commit/c080150321e5000a2c891e477bb582df07b7f75f&quot;&gt;first commit&lt;/a&gt; of the project on GitHub. Like a parent reflecting on their child attending school for the first time, we know reaching this point hasn’t been an entirely solo journey, despite Paolo&apos;s best efforts!&lt;/p&gt;&lt;p&gt;A lot has happened recently and we thought it was time to highlight some of the recent evolutions. We also take the opportunity to extend the warmest of thanks to all those who have contributed to the development of Nextflow as well as the fantastic community of users who consistently provide ideas, feedback and the occasional late night banter on the &lt;a href=&quot;https://gitter.im/nextflow-io/nextflow&quot;&gt;Gitter channel&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Here are a few neat developments churning out of the birthday cake mix.&lt;/p&gt;&lt;h3&gt;nf-core&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://nf-core.github.io/&quot;&gt;nf-core&lt;/a&gt; is a community effort to provide a home for high quality, production-ready, curated analysis pipelines built using Nextflow. The project has been initiated and is being led by &lt;a href=&quot;https://github.com/ewels&quot;&gt;Phil Ewels&lt;/a&gt; of &lt;a href=&quot;http://multiqc.info/&quot;&gt;MultiQC&lt;/a&gt; fame. The principle is that &lt;em&gt;nf-core&lt;/em&gt; pipelines can be used out-of-the-box or as inspiration for something different.&lt;/p&gt;&lt;p&gt;As well as being a place for best-practise pipelines, other features of &lt;em&gt;nf-core&lt;/em&gt; include the &lt;a href=&quot;https://github.com/nf-core/cookiecutter&quot;&gt;cookie cutter template tool&lt;/a&gt; which provides a fast way to create a dependable workflow using many of Nextflow’s sweet capabilities such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Outline:&lt;/em&gt; Skeleton pipeline script.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data:&lt;/em&gt; Reference Genome implementation (AWS iGenomes).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Configuration:&lt;/em&gt; Robust configuration setup.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Containers:&lt;/em&gt; Skeleton files for Docker image generation.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Reporting:&lt;/em&gt; HTML email functionality and and HTML results output.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Documentation:&lt;/em&gt; Installation, Usage, Output, Troubleshooting, etc.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Continuous Integration:&lt;/em&gt; Skeleton files for automated testing using Travis CI.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;There is also a Python package with helper tools for Nextflow.&lt;/p&gt;&lt;p&gt;You can find more information about the community via the project &lt;a href=&quot;https://nf-core.github.io&quot;&gt;website&lt;/a&gt;, &lt;a href=&quot;https://github.com/nf-core&quot;&gt;GitHub repository&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/nf_core&quot;&gt;Twitter account&lt;/a&gt; or join the dedicated &lt;a href=&quot;https://gitter.im/nf-core/Lobby&quot;&gt;Gitter&lt;/a&gt; chat.&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;nf-core logo&apos; width=&apos;560&apos; src=&apos;/img/nf-core-logo-min.png&apos; style=&apos;margin:1em auto; cursor: pointer;&apos; onclick=&quot;window.open(&apos;https://nf-core.github.io&apos;)&quot; /&gt;&lt;/p&gt;&lt;h3&gt;Kubernetes has landed&lt;/h3&gt;&lt;p&gt;As of version 0.28.0 Nextflow now has support for Kubernetes. If you don’t know much about Kubernetes, at its heart it is an open-source platform for the management and deployment of containers at scale. Google led the initial design and it is now maintained by the Cloud Native Computing Foundation. I found the &lt;a href=&quot;https://www.youtube.com/watch?v=4ht22ReBjno&quot;&gt;The Illustrated Children&apos;s Guide to Kubernetes&lt;/a&gt; particularly useful in explaining the basic vocabulary and concepts.&lt;/p&gt;&lt;p&gt;Kubernetes looks be one of the key technologies for the application of containers in the cloud as well as for building Infrastructure as a Service (IaaS) and Platform and a Service (PaaS) applications. We have been approached by many users who wish to use Nextflow with Kubernetes to be able to deploy workflows across both academic and commercial settings. With enterprise versions of Kubernetes such as Red Hat&apos;s &lt;a href=&quot;https://www.openshift.com/&quot;&gt;OpenShift&lt;/a&gt;, it was becoming apparent there was a need for native execution with Nextflow.&lt;/p&gt;&lt;p&gt;The new command &lt;code&gt;nextflow kuberun&lt;/code&gt; launches the Nextflow driver as a &lt;em&gt;pod&lt;/em&gt; which is then able to run workflow tasks as other pods within a Kubernetes cluster. You can read more in the documentation on Kubernetes support for Nextflow &lt;a href=&quot;https://www.nextflow.io/docs/latest/kubernetes.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;Nextflow and Kubernetes&apos; width=&apos;760&apos; src=&apos;/img/nextflow-kubernetes-min.png&apos; style=&apos;margin:1em auto&apos;/&gt;&lt;/p&gt;&lt;h3&gt;Improved reporting and notifications&lt;/h3&gt;&lt;p&gt;Following the hackathon in September we wrote about the addition of HTML trace reports that allow for the generation HTML detailing resource usage (CPU time, memory, disk i/o etc).&lt;/p&gt;&lt;p&gt;Thanks to valuable feedback there has continued to be many improvements to the reports as tracked through the Nextflow GitHub issues page. Reports are now able to display &lt;a href=&quot;https://github.com/nextflow-io/nextflow/issues/547&quot;&gt;thousands of tasks&lt;/a&gt; and include extra information such as the &lt;a href=&quot;https://github.com/nextflow-io/nextflow/issues/521&quot;&gt;container engine used&lt;/a&gt;. Tasks can be filtered and an &lt;a href=&quot;https://github.com/nextflow-io/nextflow/issues/534&quot;&gt;overall progress bar&lt;/a&gt; has been added.&lt;/p&gt;&lt;p&gt;You can explore a &lt;a href=&quot;/misc/nf-trace-report2.html&quot;&gt;real-world HTML report&lt;/a&gt; and more information on HTML reports can be found in the &lt;a href=&quot;https://www.nextflow.io/docs/latest/tracing.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;There has also been additions to workflow notifications. Currently these can be configured to automatically send a notification email when a workflow execution terminates. You can read more about how to setup notifications in the &lt;a href=&quot;https://www.nextflow.io/docs/latest/mail.html?highlight=notification#workflow-notification&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Syntax-tic!&lt;/h3&gt;&lt;p&gt;Writing workflows no longer has to be done in monochrome. There is now syntax highlighting for Nextflow in the popular &lt;a href=&quot;https://atom.io&quot;&gt;Atom editor&lt;/a&gt; as well as in &lt;a href=&quot;https://code.visualstudio.com&quot;&gt;Visual Studio Code&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;Nextflow syntax highlighting with Atom&apos; width=&apos;360&apos; src=&apos;/img/atom-min.png&apos; style=&apos;margin:1em auto; cursor: pointer;&apos; onclick=&quot;window.open(this.src)&quot; /&gt; &lt;img alt=&apos;Nextflow syntax highlighting with VSCode&apos; width=&apos;360&apos; src=&apos;/img/vscode-min.png&apos; style=&apos;margin:1em auto; cursor: pointer;&apos; onclick=&quot;window.open(this.src)&quot; /&gt;&lt;/p&gt;&lt;p&gt;You can find the Atom plugin by searching for Nextflow in Atoms package installer or clicking &lt;a href=&quot;https://atom.io/packages/language-nextflow&quot;&gt;here&lt;/a&gt;. The Visual Studio plugin can be downloaded &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=nextflow.nextflow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;On a related note, Nextflow is now an official language on GitHub!&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;GitHub nextflow syntax&apos; width=&apos;760&apos; src=&apos;/img/github-nf-syntax-min.png&apos; style=&apos;margin:1em auto&apos;/&gt;&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Nextflow developments are progressing faster than ever and with the help of the community, there are a ton of great new features on the way. If you have any suggestions of your killer NF idea then please drop us a line, open an issue or even better, join in the fun.&lt;/p&gt;&lt;p&gt;Over the coming months Nextflow will be reaching out with several training and presentation sessions across the US and Europe. We hope to see as many of you as possible on the road.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Running CAW with Singularity and Nextflow</title>
      <link>https://www.nextflow.io/blog/2017/caw-and-singularity.html</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2017/caw-and-singularity.html</guid>
      	<description>
	&lt;p&gt;&lt;i&gt;This is a guest post authored by Maxime Garcia from the Science for Life Laboratory in Sweden. Max describes how they deploy complex cancer data analysis pipelines using Nextflow and Singularity. We are very happy to share their experience across the Nextflow community.&lt;/i&gt;&lt;/p&gt;&lt;h3&gt;The CAW pipeline&lt;/h3&gt;&lt;p&gt;&lt;img src=&apos;/img/CAW_logo.png&apos; alt=&quot;Cancer Analysis Workflow logo&quot; style=&apos;float:right&apos; /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://opensource.scilifelab.se/projects/sarek/&quot;&gt;Cancer Analysis Workflow&lt;/a&gt; (CAW for short) is a Nextflow based analysis pipeline developed for the analysis of tumour: normal pairs. It is developed in collaboration with two infrastructures within &lt;a href=&quot;https://www.scilifelab.se/&quot;&gt;Science for Life Laboratory&lt;/a&gt;: &lt;a href=&quot;https://ngisweden.scilifelab.se/&quot;&gt;National Genomics Infrastructure&lt;/a&gt; (NGI), in The Stockholm &lt;a href=&quot;https://www.scilifelab.se/facilities/ngi-stockholm/&quot;&gt;Genomics Applications Development Facility&lt;/a&gt; to be precise and &lt;a href=&quot;https://www.nbis.se/&quot;&gt;National Bioinformatics Infrastructure Sweden&lt;/a&gt; (NBIS).&lt;/p&gt;&lt;p&gt;CAW is based on &lt;a href=&quot;https://software.broadinstitute.org/gatk/best-practices/&quot;&gt;GATK Best Practices&lt;/a&gt; for the preprocessing of FastQ files, then uses various variant calling tools to look for somatic SNVs and small indels (&lt;a href=&quot;https://github.com/broadinstitute/mutect/&quot;&gt;MuTect1&lt;/a&gt;, &lt;a href=&quot;https://github.com/broadgsa/gatk-protected/&quot;&gt;MuTect2&lt;/a&gt;, &lt;a href=&quot;https://github.com/Illumina/strelka/&quot;&gt;Strelka&lt;/a&gt;, &lt;a href=&quot;https://github.com/ekg/freebayes/&quot;&gt;Freebayes&lt;/a&gt;), (&lt;a href=&quot;https://github.com/broadgsa/gatk-protected/&quot;&gt;GATK HaplotyeCaller&lt;/a&gt;), for structural variants(&lt;a href=&quot;https://github.com/Illumina/manta/&quot;&gt;Manta&lt;/a&gt;) and for CNVs (&lt;a href=&quot;https://github.com/Crick-CancerGenomics/ascat/&quot;&gt;ASCAT&lt;/a&gt;). Annotation tools (&lt;a href=&quot;http://snpeff.sourceforge.net/&quot;&gt;snpEff&lt;/a&gt;, &lt;a href=&quot;https://www.ensembl.org/info/docs/tools/vep/index.html&quot;&gt;VEP&lt;/a&gt;) are also used, and finally &lt;a href=&quot;http://multiqc.info/&quot;&gt;MultiQC&lt;/a&gt; for handling reports.&lt;/p&gt;&lt;p&gt;We are currently working on a manuscript, but you&apos;re welcome to look at (or even contribute to) our &lt;a href=&quot;https://github.com/SciLifeLab/CAW/&quot;&gt;github repository&lt;/a&gt; or talk with us on our &lt;a href=&quot;https://gitter.im/SciLifeLab/CAW/&quot;&gt;gitter channel&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Singularity and UPPMAX&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://singularity.lbl.gov/&quot;&gt;Singularity&lt;/a&gt; is a tool package software dependencies into a contained environment, much like Docker. It&apos;s designed to run on HPC environments where Docker is often a problem due to its requirement for administrative privileges.&lt;/p&gt;&lt;p&gt;We&apos;re based in Sweden, and &lt;a href=&quot;https://uppmax.uu.se/&quot;&gt;Uppsala Multidisciplinary Center for Advanced Computational Science&lt;/a&gt; (UPPMAX) provides Computational infrastructures for all Swedish researchers. Since we&apos;re analyzing sensitive data, we are using secure clusters (with a two factor authentication), set up by UPPMAX: &lt;a href=&quot;https://www.uppmax.uu.se/projects-and-collaborations/snic-sens/&quot;&gt;SNIC-SENS&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In my case, since we&apos;re still developing the pipeline, I am mainly using the research cluster &lt;a href=&quot;https://www.uppmax.uu.se/resources/systems/the-bianca-cluster/&quot;&gt;Bianca&lt;/a&gt;. So I can only transfer files and data in one specific repository using SFTP.&lt;/p&gt;&lt;p&gt;UPPMAX provides computing resources for Swedish researchers for all scientific domains, so getting software updates can occasionally take some time. Typically, &lt;a href=&quot;http://modules.sourceforge.net/&quot;&gt;Environment Modules&lt;/a&gt; are used which allow several versions of different tools - this is good for reproducibility and is quite easy to use. However, the approach is not portable across different clusters outside of UPPMAX.&lt;/p&gt;&lt;h3&gt;Why use containers?&lt;/h3&gt;&lt;p&gt;The idea of using containers, for improved portability and reproducibility, and more up to date tools, came naturally to us, as it is easily managed within Nextflow. We cannot use &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; on our secure cluster, so we wanted to run CAW with &lt;a href=&quot;http://singularity.lbl.gov/&quot;&gt;Singularity&lt;/a&gt; images instead.&lt;/p&gt;&lt;h3&gt;How was the switch made?&lt;/h3&gt;&lt;p&gt;We were already using Docker containers for our continuous integration testing with Travis, and since we use many tools, I took the approach of making (almost) a container for each process. Because this process is quite slow, repetitive and I&lt;s&gt;&apos;m lazy&lt;/s&gt; like to automate everything, I made a simple NF &lt;a href=&quot;https://github.com/SciLifeLab/CAW/blob/master/buildContainers.nf&quot;&gt;script&lt;/a&gt; to build and push all docker containers. Basically it&apos;s just &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;pull&lt;/code&gt; for all containers, with some configuration possibilities.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t ${repository}/${container}:${tag} ${baseDir}/containers/${container}/.

docker push ${repository}/${container}:${tag}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Since Singularity can directly pull images from DockerHub, I made the build script to pull all containers from DockerHub to have local Singularity image files.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity pull --name ${container}-${tag}.img docker://${repository}/${container}:${tag}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After this, it&apos;s just a matter of moving all containers to the secure cluster we&apos;re using, and using the right configuration file in the profile. I&apos;ll spare you the details of the SFTP transfer. This is what the configuration file for such Singularity images looks like: &lt;a href=&quot;https://github.com/SciLifeLab/CAW/blob/master/configuration/singularity-path.config&quot;&gt;&lt;code&gt;singularity-path.config&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/*
vim: syntax=groovy
-*- mode: groovy;-*-
 * -------------------------------------------------
 * Nextflow config file for CAW project
 * -------------------------------------------------
 * Paths to Singularity images for every process
 * No image will be pulled automatically
 * Need to transfer and set up images before
 * -------------------------------------------------
 */

singularity {
  enabled = true
  runOptions = &amp;quot;--bind /scratch&amp;quot;
}

params {
  containerPath=&amp;#39;containers&amp;#39;
  tag=&amp;#39;1.2.3&amp;#39;
}

process {
  $ConcatVCF.container      = &amp;quot;${params.containerPath}/caw-${params.tag}.img&amp;quot;
  $RunMultiQC.container     = &amp;quot;${params.containerPath}/multiqc-${params.tag}.img&amp;quot;
  $IndelRealigner.container = &amp;quot;${params.containerPath}/gatk-${params.tag}.img&amp;quot;
  // I&amp;#39;m not putting the whole file here
  // you probably already got the point
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This approach ran (almost) perfectly on the first try, except a process failing due to a typo on a container name...&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;This switch was completed a couple of months ago and has been a great success. We are now using Singularity containers in almost all of our Nextflow pipelines developed at NGI. Even if we do enjoy the improved control, we must not forgot that:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;With great power comes great responsibility!&lt;/p&gt;
&lt;/blockquote&gt;&lt;h3&gt;Credits&lt;/h3&gt;&lt;p&gt;Thanks to &lt;a href=&quot;https://github.com/Hammarn&quot;&gt;Rickard Hammarén&lt;/a&gt; and &lt;a href=&quot;http://phil.ewels.co.uk/&quot;&gt;Phil Ewels&lt;/a&gt; for comments and suggestions for improving the post.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Scaling with AWS Batch</title>
      <link>https://www.nextflow.io/blog/2017/scaling-with-aws-batch.html</link>
      <pubDate>Wed, 8 Nov 2017 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2017/scaling-with-aws-batch.html</guid>
      	<description>
	&lt;p&gt;The latest Nextflow release (0.26.0) includes built-in support for &lt;a href=&quot;https://aws.amazon.com/batch/&quot;&gt;AWS Batch&lt;/a&gt;, a managed computing service that allows the execution of containerised workloads over the Amazon EC2 Container Service (ECS).&lt;/p&gt;&lt;p&gt;This feature allows the seamless deployment of Nextflow pipelines in the cloud by offloading the process executions as managed Batch jobs. The service takes care to spin up the required computing instances on-demand, scaling up and down the number and composition of the instances to best accommodate the actual workload resource needs at any point in time. &lt;/p&gt;&lt;p&gt;AWS Batch shares with Nextflow the same vision regarding workflow containerisation i.e. each compute task is executed in its own Docker container. This dramatically simplifies the workflow deployment through the download of a few container images. This common design background made the support for AWS Batch a natural extension for Nextflow.&lt;/p&gt;&lt;h3&gt;Batch in a nutshell&lt;/h3&gt;&lt;p&gt;Batch is organised in &lt;em&gt;Compute Environments&lt;/em&gt;, &lt;em&gt;Job queues&lt;/em&gt;, &lt;em&gt;Job definitions&lt;/em&gt; and &lt;em&gt;Jobs&lt;/em&gt;. &lt;/p&gt;&lt;p&gt;The &lt;em&gt;Compute Environment&lt;/em&gt; allows you to define the computing resources required for a specific workload (type). You can specify the minimum and maximum number of CPUs that can be allocated, the EC2 provisioning model (On-demand or Spot), the AMI to be used and the allowed instance types. &lt;/p&gt;&lt;p&gt;The &lt;em&gt;Job queue&lt;/em&gt; definition allows you to bind a specific task to one or more Compute Environments. &lt;/p&gt;&lt;p&gt;Then, the &lt;em&gt;Job definition&lt;/em&gt; is a template for one or more jobs in your workload. This is required to specify the Docker image to be used in running a particular task along with other requirements such as the container mount points, the number of CPUs, the amount of memory and the number of retries in case of job failure. &lt;/p&gt;&lt;p&gt;Finally the &lt;em&gt;Job&lt;/em&gt; binds a Job definition to a specific Job queue and allows you to specify the actual task command to be executed in the container. &lt;/p&gt;&lt;p&gt;The job input and output data management is delegated to the user. This means that if you only use Batch API/tools you will need to take care to stage the input data from a S3 bucket (or a different source) and upload the results to a persistent storage location. &lt;/p&gt;&lt;p&gt;This could turn out to be cumbersome in complex workflows with a large number of tasks and above all it makes it difficult to deploy the same applications across different infrastructure. &lt;/p&gt;&lt;h3&gt;How to use Batch with Nextflow&lt;/h3&gt;&lt;p&gt;Nextflow streamlines the use of AWS Batch by smoothly integrating it in its workflow processing model and enabling transparent interoperability with other systems. &lt;/p&gt;&lt;p&gt;To run Nextflow you will need to set-up in your AWS Batch account a &lt;a href=&quot;http://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html&quot;&gt;Compute Environment&lt;/a&gt; defining the required computing resources and associate it to a &lt;a href=&quot;http://docs.aws.amazon.com/batch/latest/userguide/job_queues.html&quot;&gt;Job Queue&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Nextflow takes care to create the required &lt;em&gt;Job Definitions&lt;/em&gt; and &lt;em&gt;Job&lt;/em&gt; requests as needed. This spares some Batch configurations steps. &lt;/p&gt;&lt;p&gt;In the &lt;code&gt;nextflow.config&lt;/code&gt;, file specify the &lt;code&gt;awsbatch&lt;/code&gt; executor, the Batch &lt;code&gt;queue&lt;/code&gt; and the container to be used in the usual manner. You may also need to specify the AWS region and access credentials if they are not provided by other means. For example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process.executor = &amp;#39;awsbatch&amp;#39;
process.queue = &amp;#39;my-batch-queue&amp;#39;
process.container = your-org/your-docker:image
aws.region = &amp;#39;eu-west-1&amp;#39;
aws.accessKey = &amp;#39;xxx&amp;#39;
aws.secretKey = &amp;#39;yyy&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each process can eventually use a different queue and Docker image (see Nextflow documentation for details). The container image(s) must be published in a Docker registry that is accessible from the instances run by AWS Batch eg. &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt;, &lt;a href=&quot;https://quay.io/&quot;&gt;Quay&lt;/a&gt; or &lt;a href=&quot;https://aws.amazon.com/ecr/&quot;&gt;ECS Container Registry&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The Nextflow process can be launched either in a local computer or a EC2 instance. The latter is suggested for heavy or long running workloads. &lt;/p&gt;&lt;p&gt;Note that input data should be stored in the S3 storage. In the same manner the pipeline execution must specify a S3 bucket as a working directory by using the &lt;code&gt;-w&lt;/code&gt; command line option.&lt;/p&gt;&lt;p&gt;A final caveat about custom containers and computing AMI. Nextflow automatically stages input data and shares tasks intermediate results by using the S3 bucket specified as a work directory. For this reason it needs to use the &lt;code&gt;aws&lt;/code&gt; command line tool which must be installed either in your process container or be present in a custom AMI that can be mounted and accessed by the Docker containers. &lt;/p&gt;&lt;p&gt;You may also need to create a custom AMI because the default image used by AWS Batch only provides 22 GB of storage which may not be enough for real world analysis pipelines. &lt;/p&gt;&lt;p&gt;See the documentation to learn &lt;a href=&quot;/docs/latest/awscloud.html#custom-ami&quot;&gt;how to create a custom AMI&lt;/a&gt; with larger storage and how to setup the AWS CLI tools. &lt;/p&gt;&lt;h3&gt;An example&lt;/h3&gt;&lt;p&gt;In order to validate Nextflow integration with AWS Batch, we used a simple RNA-Seq pipeline. &lt;/p&gt;&lt;p&gt;This pipeline takes as input a metadata file from the Encode project corresponding to a &lt;a href=&quot;https://www.encodeproject.org/search/?type=Experiment&amp;award.project=ENCODE&amp;replicates.library.biosample.donor.organism.scientific_name=Homo+sapiens&amp;files.file_type=fastq&amp;files.run_type=paired-ended&amp;replicates.library.nucleic_acid_term_name=RNA&amp;replicates.library.depleted_in_term_name=rRNA&quot;&gt;search returning all human RNA-seq paired-end datasets&lt;/a&gt; (the metadata file has been additionally filtered to retain only data having a SRA ID). &lt;/p&gt;&lt;p&gt;The pipeline automatically downloads the FASTQ files for each sample from the EBI ENA database, it assesses the overall quality of sequencing data using FastQC and then runs &lt;a href=&quot;https://combine-lab.github.io/salmon/&quot;&gt;Salmon&lt;/a&gt; to perform the quantification over the human transcript sequences. Finally all the QC and quantification outputs are summarised using the &lt;a href=&quot;http://multiqc.info/&quot;&gt;MultiQC&lt;/a&gt; tool. &lt;/p&gt;&lt;p&gt;For the sake of this benchmark we used the first 38 samples out of the full 375 samples dataset.&lt;/p&gt;&lt;p&gt;The pipeline was executed both on AWS Batch cloud and in the CRG internal Univa cluster, using &lt;a href=&quot;/blog/2016/more-fun-containers-hpc.html&quot;&gt;Singularity&lt;/a&gt; as containers runtime. &lt;/p&gt;&lt;p&gt;It&apos;s worth noting that with the exception of the two configuration changes detailed below, we used exactly the same pipeline implementation at &lt;a href=&quot;https://github.com/nextflow-io/rnaseq-encode-nf&quot;&gt;this GitHub repository&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The AWS deploy used the following configuration profile: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws.region = &amp;#39;eu-west-1&amp;#39;
aws.client.storageEncryption = &amp;#39;AES256&amp;#39;
process.queue = &amp;#39;large&amp;#39;
executor.name = &amp;#39;awsbatch&amp;#39;
executor.awscli = &amp;#39;/home/ec2-user/miniconda/bin/aws&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;While for the cluster deployment the following configuration was used: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;executor = &amp;#39;crg&amp;#39;
singularity.enabled = true
process.container = &amp;quot;docker://nextflow/rnaseq-nf&amp;quot;
process.queue = &amp;#39;cn-el7&amp;#39;
process.time = &amp;#39;90 min&amp;#39;
process.$quant.time = &amp;#39;4.5 h&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Results&lt;/h3&gt;&lt;p&gt;The AWS Batch Compute environment was configured to use a maximum of 132 CPUs as the number of CPUs that were available in the queue for local cluster deployment. &lt;/p&gt;&lt;p&gt;The two executions ran in roughly the same time: 2 hours and 24 minutes when running in the CRG cluster and 2 hours and 37 minutes when using AWS Batch. &lt;/p&gt;&lt;p&gt;It must be noted that 14 jobs failed in the Batch deployment, presumably because one or more spot instances were retired. However Nextflow was able to re-schedule the failed jobs automatically and the overall pipeline execution completed successfully, also showing the benefits of a truly fault tolerant environment.&lt;/p&gt;&lt;p&gt;The overall cost for running the pipeline with AWS Batch was &lt;strong&gt;$5.47&lt;/strong&gt; ($ 3.28 for EC2 instances, $1.88 for EBS volume and $0.31 for S3 storage). This means that with ~ $55 we could have performed the same analysis on the full Encode dataset. &lt;/p&gt;&lt;p&gt;It is more difficult to estimate the cost when using the internal cluster, because we don&apos;t have access to such detailed cost accounting. However, as a user, we can estimate it roughly comes out at $0.01 per CPU-Hour. The pipeline needed around 147 CPU-Hour to carry out the analysis, hence with an estimated cost of &lt;strong&gt;$1.47&lt;/strong&gt; just for the computation. &lt;/p&gt;&lt;p&gt;The execution report for the Batch execution is available at &lt;a href=&quot;https://cdn.rawgit.com/nextflow-io/rnaseq-encode-nf/db303a81/benchmark/aws-batch/report.html&quot;&gt;this link&lt;/a&gt; and the one for cluster is available &lt;a href=&quot;https://cdn.rawgit.com/nextflow-io/rnaseq-encode-nf/db303a81/benchmark/crg-cluster/report.html&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;This post shows how Nextflow integrates smoothly with AWS Batch and how it can be used to deploy and execute real world genomics pipeline in the cloud with ease. &lt;/p&gt;&lt;p&gt;The auto-scaling ability provided by AWS Batch along with the use of spot instances make the use of the cloud even more cost effective. Running on a local cluster may still be cheaper, even if it is non trivial to account for all the real costs of a HPC infrastructure. However the cloud allows flexibility and scalability not possible with common on-premises clusters. &lt;/p&gt;&lt;p&gt;We also demonstrate how the same Nextflow pipeline can be &lt;em&gt;transparently&lt;/em&gt; deployed in two very different computing infrastructure, using different containerisation technologies by simply providing a separate configuration profile.&lt;/p&gt;&lt;p&gt;This approach enables the interoperability across different deployment sites, reduces operational and maintenance costs and guarantees consistent results over time.&lt;/p&gt;&lt;h3&gt;Credits&lt;/h3&gt;&lt;p&gt;This post is co-authored with &lt;a href=&quot;https://twitter.com/fstrozzi&quot;&gt;Francesco Strozzi&lt;/a&gt;, who also helped to write the pipeline used for the benchmark in this post and contributed to and tested the AWS Batch integration. Thanks to &lt;a href=&quot;https://github.com/emi80&quot;&gt;Emilio Palumbo&lt;/a&gt; that helped to set-up and configure the AWS Batch environment and &lt;a href=&quot;https://gitter.im/skptic&quot;&gt;Evan Floden&lt;/a&gt; for the comments. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nexflow Hackathon 2017</title>
      <link>https://www.nextflow.io/blog/2017/nextflow-hack17.html</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2017/nextflow-hack17.html</guid>
      	<description>
	&lt;p&gt;Last week saw the inaugural Nextflow meeting organised at the Centre for Genomic Regulation (CRG) in Barcelona. The event combined talks, demos, a tutorial/workshop for beginners as well as two hackathon sessions for more advanced users. &lt;/p&gt;&lt;p&gt;Nearly 50 participants attended over the two days which included an entertaining tapas course during the first evening!&lt;/p&gt;&lt;p&gt;One of the main objectives of the event was to bring together Nextflow users to work together on common interest projects. There were several proposals for the hackathon sessions and in the end five diverse ideas were chosen for communal development ranging from new pipelines through to the addition of new features in Nextflow. &lt;/p&gt;&lt;p&gt;The proposals and outcomes of each the projects, which can be found in the issues section of &lt;a href=&quot;https://github.com/nextflow-io/hack17&quot;&gt;this GitHub repository&lt;/a&gt;, have been summarised below.&lt;/p&gt;&lt;h3&gt;Nextflow HTML tracing reports&lt;/h3&gt;&lt;p&gt;The HTML tracing project aims to generate a rendered version of the Nextflow trace file to enable fast sorting and visualisation of task/process execution statistics. &lt;/p&gt;&lt;p&gt;Currently the data in the trace includes information such as CPU duration, memory usage and completion status of each task, however wading through the file is often not convenient when a large number of tasks have been executed. &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/ewels&quot;&gt;Phil Ewels&lt;/a&gt; proposed the idea and led the coordination effort with the outcome being a very impressive working prototype which can be found in the Nextflow branch &lt;code&gt;html-trace&lt;/code&gt;. &lt;/p&gt;&lt;p&gt;An image of the example report is shown below with the interactive HTML available &lt;a href=&quot;/misc/nf-trace-report.html&quot;&gt;here&lt;/a&gt;. It is expected to be merged into the main branch of Nextflow with documentation in a near-future release.&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;Nextflow HTML execution report&apos; width=&apos;760&apos; src=&apos;/img/nf-trace-report-min.png&apos; style=&apos;margin:1em auto&apos;/&gt;&lt;/p&gt;&lt;h3&gt;Nextflow pipeline for 16S microbial data&lt;/h3&gt;&lt;p&gt;The H3Africa Bioinformatics Network have been developing several pipelines which are used across the participating centers. The diverse computing resources available across the nodes has led to members wanting workflow solutions with a particular focus on portability. &lt;/p&gt;&lt;p&gt;With this is mind, Scott Hazelhurst proposed a project for a 16S Microbial data analysis pipeline which had &lt;a href=&quot;https://github.com/h3abionet/h3abionet16S/tree/master&quot;&gt;previously been developed using CWL&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The participants made a new &lt;a href=&quot;https://github.com/h3abionet/h3abionet16S/tree/nextflow&quot;&gt;branch&lt;/a&gt; of the original pipeline and ported it into Nextflow. &lt;/p&gt;&lt;p&gt;The pipeline will continue to be developed with the goal of acting as a comparison between CWL and Nextflow. It is thought this can then be extended to other pipelines by both those who are already familiar with Nextflow as well as used as a tool for training newer users.&lt;/p&gt;&lt;h3&gt;Nextflow modules prototyping&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Toolboxing&lt;/em&gt; allows users to incorporate software into their pipelines in an efficient and reproducible manner. Various software repositories are becoming increasing popular, highlighted by the over 5,000 tools available in the &lt;a href=&quot;https://toolshed.g2.bx.psu.edu/&quot;&gt;Galaxy Toolshed&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Projects such as &lt;a href=&quot;http://biocontainers.pro/&quot;&gt;Biocontainers&lt;/a&gt; aim to wrap up the execution environment using containers. &lt;a href=&quot;https://github.com/skptic&quot;&gt;Myself&lt;/a&gt; and &lt;a href=&quot;https://github.com/viklund&quot;&gt;Johan Viklund&lt;/a&gt; wished to piggyback off existing repositories and settled on &lt;a href=&quot;https://dockstore.org&quot;&gt;Dockstore&lt;/a&gt; which is an open platform compliant with the &lt;a href=&quot;http://genomicsandhealth.org&quot;&gt;GA4GH&lt;/a&gt; initiative. &lt;/p&gt;&lt;p&gt;The majority of tools in Dockstore are written in the CWL and therefore we required a parser between the CWL CommandLineTool class and Nextflow processes. Johan was able to develop a parser which generates Nextflow processes for several Dockstore tools. &lt;/p&gt;&lt;p&gt;As these resources such as Dockstore become mature and standardised, it will be possible to automatically generate a &lt;em&gt;Nextflow Store&lt;/em&gt; and enable efficient incorporation of tools into workflows.&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/pditommaso/7ccdb6e8af80133a25f259ae801371bf.js&quot;&gt;&lt;/script&gt;&lt;p&gt;&lt;em&gt;Example showing a Nextflow process generated from the Dockstore CWL repository for the tool BAMStats.&lt;/em&gt;&lt;/p&gt;&lt;h3&gt;Nextflow pipeline for de novo assembly of nanopore reads&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Nanopore_sequencing&quot;&gt;Nanopore sequencing&lt;/a&gt; is an exciting and emerging technology which promises to change the landscape of nucleotide sequencing. &lt;/p&gt;&lt;p&gt;With keen interest in Nanopore specific pipelines, &lt;a href=&quot;https://github.com/HadrienG&quot;&gt;Hadrien Gourlé&lt;/a&gt; lead the hackathon project for &lt;em&gt;Nanoflow&lt;/em&gt;. &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/HadrienG/nanoflow&quot;&gt;Nanoflow&lt;/a&gt; is a de novo assembler of bacterials genomes from nanopore reads using Nextflow. &lt;/p&gt;&lt;p&gt;During the two days the participants developed the pipeline for adapter trimming as well as assembly and consensus sequence generation using either &lt;a href=&quot;https://github.com/marbl/canu&quot;&gt;Canu&lt;/a&gt; and &lt;a href=&quot;https://github.com/lh3/miniasm&quot;&gt;Miniasm&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The future plans are to finalise the pipeline to include a polishing step and a genome annotation step. &lt;/p&gt;&lt;h3&gt;Nextflow AWS Batch integration&lt;/h3&gt;&lt;p&gt;Nextflow already has experimental support for &lt;a href=&quot;https://aws.amazon.com/batch/&quot;&gt;AWS Batch&lt;/a&gt; and the goal of this project proposed by &lt;a href=&quot;https://github.com/fstrozzi&quot;&gt;Francesco Strozzi&lt;/a&gt; was to improve this support, add features and test the implementation on real world pipelines. &lt;/p&gt;&lt;p&gt;Earlier work from &lt;a href=&quot;https://github.com/pditommaso&quot;&gt;Paolo Di Tommaso&lt;/a&gt; in the Nextflow repository, highlighted several challenges to using AWS Batch with Nextflow. &lt;/p&gt;&lt;p&gt;The major obstacle described by &lt;a href=&quot;https://github.com/tdudgeon&quot;&gt;Tim Dudgeon&lt;/a&gt; was the requirement for each Docker container to have a version of the Amazon Web Services Command Line tools (aws-cli) installed. &lt;/p&gt;&lt;p&gt;A solution was to install the AWS CLI tools on a custom AWS image that is used by the Docker host machine, and then mount the directory that contains the necessary items into each of the Docker containers as a volume. Early testing suggests this approach works with the hope of providing a more elegant solution in future iterations. &lt;/p&gt;&lt;p&gt;The code and documentation for AWS Batch has been prepared and will be tested further before being rolled into an official Nextflow release in the near future.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The event was seen as an overwhelming success and special thanks must be made to all the participants. As the Nextflow community continues to grow, it would be fantastic to make these types meetings more regular occasions. &lt;/p&gt;&lt;p&gt;In the meantime we have put together a short video containing some of the highlights of the two days. &lt;/p&gt;&lt;p&gt;We hope to see you all again in Barcelona soon or at new events around the world!&lt;/p&gt;&lt;p&gt;&lt;iframe width=&quot;760&quot; height=&quot;428&quot; src=&quot;https://www.youtube.com/embed/s7SqYMRiY8w?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nextflow and the Common Workflow Language</title>
      <link>https://www.nextflow.io/blog/2017/nextflow-and-cwl.html</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2017/nextflow-and-cwl.html</guid>
      	<description>
	&lt;p&gt;The Common Workflow Language (&lt;a href=&quot;http://www.commonwl.org/&quot;&gt;CWL&lt;/a&gt;) is a specification for defining workflows in a declarative manner. It has been implemented to varying degrees by different software packages. Nextflow and CWL share a common goal of enabling portable reproducible workflows. &lt;/p&gt;&lt;p&gt;We are currently investigating the automatic conversion of CWL workflows into Nextflow scripts to increase the portability of workflows. This work is being developed as the &lt;a href=&quot;https://github.com/nextflow-io/cwl2nxf&quot;&gt;cwl2nxf&lt;/a&gt; project, currently in early prototype stage. &lt;/p&gt;&lt;p&gt;Our first phase of the project was to determine mappings of CWL to Nextflow and familiarize ourselves with how the current implementation of the converter supports a number of CWL specific features. &lt;/p&gt;&lt;h3&gt;Mapping CWL to Nextflow&lt;/h3&gt;&lt;p&gt;Inputs in the CWL workflow file are initially parsed as &lt;em&gt;channels&lt;/em&gt; or other Nextflow input types. Each step specified in the workflow is then parsed independently. At the time of writing subworkflows are not supported, each step must be a CWL &lt;code&gt;CommandLineTool&lt;/code&gt; file. &lt;/p&gt;&lt;p&gt;The image below shows an example of the major components in the CWL files and then post-conversion (click to zoom). &lt;/p&gt;&lt;p&gt;&lt;a href=&apos;/img/cwl2nxf-min.png&apos; target=&apos;_blank&apos;&gt; &lt;img alt=&apos;Nextflow CWL conversion&apos; width=&apos;760&apos; src=&apos;/img/cwl2nxf-min.png&apos; style=&apos;margin:1em auto&apos;/&gt; &lt;/a&gt;&lt;/p&gt;&lt;p&gt;CWL and Nextflow share a similar structure of defining inputs and outputs as shown above. &lt;/p&gt;&lt;p&gt;A notable difference between the two is how tasks are defined. CWL requires either a separate file for each task or a sub-workflow. CWL also requires the explicit mapping of each command line option for an executed tool. This is done using YAML meta-annotation to indicate the position, prefix, etc. for each command line option.&lt;/p&gt;&lt;p&gt;In Nextflow a task command is defined as a separated component in the &lt;code&gt;process&lt;/code&gt; definition and it is ultimately a multiline string which is interpreted by a command script by the underlying system. Input parameters can be used in the command string with a simple variable interpolation mechanism. This is beneficial as it simplifies porting existing BASH scripts to Nextflow with minimal refactoring. &lt;/p&gt;&lt;p&gt;These examples highlight some of the differences between the two approaches, and the difficulties converting complex use cases such as scatter, CWL expressions, and conditional command line inclusion. &lt;/p&gt;&lt;h3&gt;Current status&lt;/h3&gt;&lt;p&gt;The cwl2nxf is a Groovy based tool with a limited conversion ability. It parses the YAML documents and maps the various CWL objects to Nextflow. Conversion examples are provided as part of the repository along with documentation for each example specifying the mapping. &lt;/p&gt;&lt;p&gt;This project was initially focused on developing an understanding of how to translate CWL to Nextflow. A number of CWL specific features such as scatter, secondary files and simple JavaScript expressions were analyzed and implemented. &lt;/p&gt;&lt;p&gt;The GitHub repository includes instructions on how to build cwl2nxf and an example usage. The tool can be executed as either just a parser printing the converted CWL to stdout, or by specifying an output file which will generate the Nextflow script file and if necessary a config file. &lt;/p&gt;&lt;p&gt;The tool takes in a CWL workflow file and the YAML inputs file. It does not currently work with a standalone &lt;code&gt;CommandLineTool&lt;/code&gt;. The following example show how to run it: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java -jar build/libs/cwl2nxf-*.jar rnatoy.cwl samp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;br&gt; See the GitHub &lt;a href=&quot;https://github.com/nextflow-io/cwl2nxf&quot;&gt;repository&lt;/a&gt; for further details. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;We are continuing to investigate ways to improve the interoperability of Nextflow with CWL. Although still an early prototype, the cwl2nxf tool provides some level of conversion of CWL to Nextflow. &lt;/p&gt;&lt;p&gt;We are also planning to explore &lt;a href=&quot;https://github.com/common-workflow-language/cwlavro&quot;&gt;CWL Avro&lt;/a&gt;, which may provide a more efficient way to parse and handle CWL objects for conversion to Nextflow.&lt;/p&gt;&lt;p&gt;Additionally, a number of workflows in the GitHub repository have been implemented in both CWL and Nextflow which can be used as a comparison of the two languages. &lt;/p&gt;&lt;p&gt;The Nextflow team will be presenting a short talk and participating in the Codefest at &lt;a href=&quot;https://www.open-bio.org/wiki/BOSC_2017&quot;&gt;BOSC 2017&lt;/a&gt;. We are interested in hearing from the community regarding CWL to Nextflow conversion, and would like to encourage anyone interested to contribute to the cwl2nxf project. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nextflow workshop is coming!</title>
      <link>https://www.nextflow.io/blog/2017/nextflow-workshop.html</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2017/nextflow-workshop.html</guid>
      	<description>
	&lt;p&gt;We are excited to announce the first Nextflow workshop that will take place at the Barcelona Biomedical Research Park building (&lt;a href=&quot;https://www.prbb.org/&quot;&gt;PRBB&lt;/a&gt;) on 14-15th September 2017. &lt;/p&gt;&lt;p&gt;This event is open to everybody who is interested in the problem of computational workflow reproducibility. Leading experts and users will discuss the current state of the Nextflow technology and how it can be applied to manage -omics analyses in a reproducible manner. Best practices will be introduced on how to deploy real-world large-scale genomic applications for precision medicine.&lt;/p&gt;&lt;p&gt;During the hackathon, organized for the second day, participants will have the opportunity to learn how to write self-contained, replicable data analysis pipelines along with Nextflow expert developers. &lt;/p&gt;&lt;p&gt;More details at &lt;a href=&quot;http://www.crg.eu/en/event/coursescrg-nextflow-reproducible-silico-genomics&quot;&gt;this link&lt;/a&gt;. The registration form is &lt;a href=&quot;http://apps.crg.es/content/internet/events/webforms/17502&quot;&gt;available here&lt;/a&gt; (deadline 15th Jun). &lt;/p&gt;&lt;h3&gt;Schedule (draft)&lt;/h3&gt;&lt;h4&gt;Thursday, 14 September&lt;/h4&gt;
&lt;table border=1 cellpadding=9 width=&apos;90%&apos;&gt; 
&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;10.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Welcome &amp; introduction&lt;br&gt;
    &lt;i&gt;Cedric Notredame&lt;br&gt;
    Comparative Bioinformatics, CRG, Spain&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt; 

&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;10.15&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Nextflow: a quick review&lt;br&gt;
    &lt;i&gt;Paolo Di Tommaso&lt;br&gt;
    Comparative Bioinformatics, CRG, Spain&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;10.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Standardising Swedish genomics analyses using Nextflow&lt;br&gt;
    &lt;i&gt;Phil Ewels&lt;br&gt;
    National Genomics Infrastructure, SciLifeLab, Sweden&lt;/i&gt;
    &lt;/td&gt;
&lt;/tr&gt;   

&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;11.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Building Pipelines to Support African Bioinformatics: the H3ABioNet Pipelines Project&lt;br&gt;
    &lt;i&gt;Scott Hazelhurst&lt;br&gt;
    University of the Witwatersrand, Johannesburg, South Africa&lt;/i&gt;
    &lt;/td&gt;
&lt;/tr&gt;   
 
&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;11.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;coffee break&lt;/i&gt;
    &lt;/td&gt;
&lt;/tr&gt;   
 
&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;12.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Using Nextflow for Large Scale Benchmarking of Phylogenetic methods and tools&lt;br&gt;
    &lt;i&gt;Frédéric Lemoine&lt;br&gt;
    Evolutionary Bioinformatics, Institut Pasteur, France&lt;/i&gt;
    &lt;/td&gt;
&lt;/tr&gt;  

&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;12.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Nextflow for chemistry - crossing the divide&lt;br&gt;
    &lt;i&gt;Tim Dudgeon&lt;br&gt;
    Informatics Matters Ltd, UK&lt;/i&gt;
    &lt;/td&gt;
&lt;/tr&gt;    
 
&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;12.50&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;From zero to Nextflow @ CRG&apos;s Biocore&lt;br&gt;
    &lt;i&gt;Luca Cozzuto&lt;br&gt;
    Bioinformatics Core Facility, CRG, Spain&lt;/i&gt;
    &lt;/td&gt;
&lt;/tr&gt;   
 
&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;13.10&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;(to be determined)&lt;/td&gt;
&lt;/tr&gt; 

&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;13.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Lunch&lt;/td&gt;
&lt;/tr&gt; 

&lt;tr&gt;
&lt;td valign=&apos;top&apos;&gt;14.30&lt;br&gt;18.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Hackathon &amp; course&lt;/td&gt;
&lt;/tr&gt; 
 
&lt;/table&gt;&lt;h4&gt;Friday, 15 September&lt;/h4&gt;
&lt;table border=1 cellpadding=9 width=&apos;90%&apos;&gt; 
&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;9.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Computational workflows for omics analyses at the IARC&lt;br&gt;
    &lt;i&gt;Matthieu Foll&lt;br&gt;
    International Agency for Research on Cancer (IARC), France&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt; 

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;10.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Medical Genetics at Oslo University Hospital&lt;br&gt;
    &lt;i&gt;Hugues Fontanelle&lt;br&gt;
    Oslo University Hospital, Norway&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt; 

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;10.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Inside-Out: reproducible analysis of external data, inside containers with Nextflow&lt;br&gt;
    &lt;i&gt;Evan Floden&lt;br&gt;
    Comparative Bioinformatics, CRG, Spain&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;11.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;coffee break&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;11.30&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;(title to be defined)&lt;br&gt;
    &lt;i&gt;Johnny Wu&lt;br&gt;
    Roche Sequencing, Pleasanton, USA&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;12.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Standardizing life sciences datasets to improve studies reproducibility in the EOSC&lt;br&gt;
    &lt;i&gt;Jordi Rambla&lt;br&gt;
    European Genome-Phenome Archive, CRG&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;12.20&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Unbounded by Economics&lt;br&gt;
    &lt;i&gt;Brendan Bouffler&lt;br&gt;
    AWS Research Cloud Program, UK&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;12.40&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Challenges with large-scale portable computational workflows&lt;br&gt;
    &lt;i&gt;Paolo Di Tommaso&lt;br&gt;
    Comparative Bioinformatics, CRG, Spain&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;13.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Lunch&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt; 
&lt;td valign=&apos;top&apos;&gt;14.00&lt;br&gt;18.00&lt;/td&gt; 
&lt;td valign=&apos;top&apos;&gt;Hackathon&lt;/i&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;&lt;p&gt;&lt;br&gt; See you in Barcelona!&lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;Nextflow workshop&apos; width=&apos;640&apos; src=&apos;/img/nf-workshop.png&apos; style=&apos;padding-top: 1em&apos;/&gt;&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Nextflow published in Nature Biotechnology</title>
      <link>https://www.nextflow.io/blog/2017/nextflow-nature-biotech-paper.html</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2017/nextflow-nature-biotech-paper.html</guid>
      	<description>
	&lt;p&gt;We are excited to announce the publication of our work &lt;em&gt;&lt;a href=&quot;http://rdcu.be/qZVo&quot;&gt;Nextflow enables reproducible computational workflows&lt;/a&gt;&lt;/em&gt; in Nature Biotechnology.&lt;/p&gt;&lt;p&gt;The article provides a description of the fundamental components and principles of Nextflow. We illustrate how the unique combination of containers, pipeline sharing and portable deployment provides tangible advantages to researchers wishing to generate reproducible computational workflows.&lt;/p&gt;&lt;p&gt;Reproducibility is a &lt;a href=&quot;http://www.nature.com/news/reproducibility-1.17552&quot;&gt;major challenge&lt;/a&gt; in today&apos;s scientific environment. We show how three bioinformatics data analyses produce different results when executed on different execution platforms and how Nextflow, along with software containers, can be used to control numerical stability, enabling consistent and replicable results across different computing platforms. As complex omics analyses enter the clinical setting, ensuring that results remain stable brings on extra importance.&lt;/p&gt;&lt;p&gt;Since its first release three years ago, the Nextflow user base has grown in an organic fashion. From the beginning it has been our own demands in a workflow tool and those of our users that have driven the development of Nextflow forward. The publication forms an important milestone in the project and we would like to extend a warm thank you to all those who have been early users and contributors. &lt;/p&gt;&lt;p&gt;We kindly ask if you use Nextflow in your own work to cite the following article:&lt;/p&gt;
&lt;div style=&apos;padding: 0.7em 1.5em; background-color: #eee&apos;&gt;
Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., &amp; Notredame, C. (2017). 
&lt;i&gt;Nextflow enables reproducible computational workflows.&lt;/i&gt; Nature Biotechnology, 35(4), 316–319.
&lt;a href=&apos;http://www.nature.com/nbt/journal/v35/n4/full/nbt.3820.html&apos;&gt;doi:10.1038/nbt.3820&lt;/a&gt;
&lt;/div&gt;
	</description>
    </item>
    <item>
      <title>More fun with containers in HPC</title>
      <link>https://www.nextflow.io/blog/2016/more-fun-containers-hpc.html</link>
      <pubDate>Tue, 20 Dec 2016 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2016/more-fun-containers-hpc.html</guid>
      	<description>
	&lt;p&gt;Nextflow was one of the &lt;a href=&quot;https://www.nextflow.io/blog/2014/nextflow-meets-docker.html&quot;&gt;first workflow framework&lt;/a&gt; to provide built-in support for Docker containers. A couple of years ago we also started to experiment with the deployment of containerised bioinformatic pipelines at CRG, using Docker technology (see &lt;a href=&quot;https://www.nextflow.io/blog/2014/using-docker-in-hpc-cluster.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.nextplatform.com/2016/01/28/crg-goes-with-the-genomics-flow/&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;We found that by isolating and packaging the complete computational workflow environment with the use of Docker images, radically simplifies the burden of maintaining complex dependency graphs of real workload data analysis pipelines.&lt;/p&gt;&lt;p&gt;Even more importantly, the use of containers enables replicable results with minimal effort for the system configuration. The entire computational environment can be archived in a self-contained executable format, allowing the replication of the associated analysis at any point in time.&lt;/p&gt;&lt;p&gt;This ability is the main reason that drove the rapid adoption of Docker in the bioinformatic community and its support in many projects, like for example &lt;a href=&quot;https://galaxyproject.org&quot;&gt;Galaxy&lt;/a&gt;, &lt;a href=&quot;http://commonwl.org&quot;&gt;CWL&lt;/a&gt;, &lt;a href=&quot;http://bioboxes.org&quot;&gt;Bioboxes&lt;/a&gt;, &lt;a href=&quot;https://dockstore.org&quot;&gt;Dockstore&lt;/a&gt; and many others. &lt;/p&gt;&lt;p&gt;However, while the popularity of Docker spread between the developers, its adaption in research computing infrastructures continues to remain very low and it&apos;s very unlikely that this trend will change in the future.&lt;/p&gt;&lt;p&gt;The reason for this resides in the Docker architecture, which requires a daemon running with root permissions on each node of a computing cluster. Such a requirement raises many security concerns, thus good practices would prevent its use in shared HPC cluster or supercomputer environments.&lt;/p&gt;&lt;h3&gt;Introducing Singularity&lt;/h3&gt;&lt;p&gt;Alternative implementations, such as &lt;a href=&quot;http://singularity.lbl.gov&quot;&gt;Singularity&lt;/a&gt;, have fortunately been promoted by the interested in containers technology. &lt;/p&gt;&lt;p&gt;Singularity is a containers engine developed at the Berkeley Lab and designed for the needs of scientific workloads. The main differences with Docker are: containers are file based, no root escalation is allowed nor root permission is needed to run a container (although a privileged user is needed to create a container image), and there is no separate running daemon.&lt;/p&gt;&lt;p&gt;These, along with other features, such as support for autofs mounts, makes Singularity a container engine better suited to the requirements of HPC clusters and supercomputers.&lt;/p&gt;&lt;p&gt;Moreover, although Singularity uses a container image format different to that of Docker, they provide a conversion tool that allows Docker images to be converted to the Singularity format.&lt;/p&gt;&lt;h3&gt;Singularity in the wild&lt;/h3&gt;&lt;p&gt;We integrated Singularity support in Nextflow framework and tested it in the CRG computing cluster and the BSC &lt;a href=&quot;https://www.bsc.es/discover-bsc/the-centre/marenostrum&quot;&gt;MareNostrum&lt;/a&gt; supercomputer.&lt;/p&gt;&lt;p&gt;The absence of a separate running daemon or image gateway made the installation straightforward when compared to Docker or other solutions.&lt;/p&gt;&lt;p&gt;To evaluate the performance of Singularity we carried out the &lt;a href=&quot;https://peerj.com/articles/1273/&quot;&gt;same benchmarks&lt;/a&gt; we performed for Docker and compared the results of the two engines.&lt;/p&gt;&lt;p&gt;The benchmarks consisted in the execution of three Nextflow based genomic pipelines:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nextflow-io/rnatoy/tree/peerj5515&quot;&gt;Rna-toy&lt;/a&gt;: a simple pipeline for RNA-Seq data analysis.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nextflow-io/nmdp-flow/tree/peerj5515/&quot;&gt;Nmdp-Flow&lt;/a&gt;: an assembly-based variant calling pipeline.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/cbcrg/piper-nf/tree/peerj5515&quot;&gt;Piper-NF&lt;/a&gt;: a pipeline for the detection and mapping of long non-coding RNAs.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;In order to repeat the analyses, we converted the container images we used to perform the Docker benchmarks to Singularity image files by using the &lt;a href=&quot;https://github.com/singularityware/docker2singularity&quot;&gt;docker2singularity&lt;/a&gt; tool &lt;em&gt;(this is not required anymore, see the update below)&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;The only change needed to run these pipelines with Singularity was to replace the Docker specific settings with the following ones in the configuration file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity.enabled = true
process.container = &amp;#39;&amp;lt;the image file path&amp;gt;&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each pipeline was executed 10 times, alternately by using Docker and Singularity as container engine. The results are shown in the following table (time in minutes):&lt;/p&gt;
&lt;style&gt; 
table#benchmark { width: 100%; margin-bottom: 1em; margin-top: 1em; border-top: 1px solid #999; border-bottom: 1px solid #999 }
table#benchmark th { text-align: center; background-color: #eee; padding: 2px 5px }
table#benchmark td { text-align: right; padding: 2px 5px; padding-right: 15px }
table#benchmark .r { text-align: right }
table#benchmark .l { text-align: left }
&lt;/style&gt;
&lt;table id=&apos;benchmark&apos;&gt;
&lt;tr&gt;
&lt;th class=&apos;l&apos;&gt;Pipeline&lt;/th&gt;
&lt;th &gt;Tasks&lt;/th&gt;
&lt;th colspan=2 &gt;Mean task time&lt;/th&gt;
&lt;th colspan=2 &gt;Mean execution time&lt;/th&gt;
&lt;th colspan=2 &gt;Execution time std dev&lt;/th&gt;
&lt;th colspan=2 &gt;Ratio&lt;/th&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;th&gt;&amp;nbsp;&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&lt;/th&gt;
&lt;th&gt;Singularity&lt;/th&gt;
&lt;th&gt;Docker&lt;/th&gt;
&lt;th&gt;Singularity&lt;/th&gt;
&lt;th&gt;Docker&lt;/th&gt;
&lt;th&gt;Singularity&lt;/th&gt;
&lt;th&gt;Docker&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&lt;/th&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class=&apos;l&apos;&gt;RNA-Seq&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;73.7&lt;/td&gt;
&lt;td&gt;73.6&lt;/td&gt;
&lt;td&gt;663.6&lt;/td&gt;
&lt;td&gt;662.3&lt;/td&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;3.1&lt;/td&gt;
&lt;td&gt;0.998&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class=&apos;l&apos;&gt;Variant call&lt;/td&gt;
&lt;td&gt;48&lt;/td&gt;
&lt;td&gt;22.1&lt;/td&gt;
&lt;td&gt;22.4&lt;/td&gt;
&lt;td&gt;1061.2&lt;/td&gt;
&lt;td&gt;1074.4&lt;/td&gt;
&lt;td&gt;43.1&lt;/td&gt;
&lt;td&gt;38.5&lt;/td&gt;
&lt;td&gt;1.012&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class=&apos;l&apos;&gt;Piper-NF&lt;/td&gt;
&lt;td&gt;98&lt;/td&gt;
&lt;td&gt;1.2&lt;/td&gt;
&lt;td&gt;1.3&lt;/td&gt;
&lt;td&gt;120.0&lt;/td&gt;
&lt;td&gt;124.5&lt;/td&gt;
&lt;td&gt;6.9 &lt;/td&gt;
&lt;td&gt;2.8&lt;/td&gt;
&lt;td&gt;1.038&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;&lt;p&gt;The benchmark results show that there isn&apos;t any significative difference in the execution times of containerised workflows between Docker and Singularity. In two cases Singularity was slightly faster and a third one it was almost identical although a little slower than Docker.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;In our evaluation Singularity proved to be an easy to install, stable and performant container engine.&lt;/p&gt;&lt;p&gt;The only minor drawback, we found when compared to Docker, was the need to define the host path mount points statically when the Singularity images were created. In fact, even if Singularity supports user mount points to be defined dynamically when the container is launched, this feature requires the overlay file system which was not supported by the kernel available in our system.&lt;/p&gt;&lt;p&gt;Docker surely will remain the &lt;em&gt;de facto&lt;/em&gt; standard engine and image format for containers due to its popularity and &lt;a href=&quot;http://www.coscale.com/blog/docker-usage-statistics-increased-adoption-by-enterprises-and-for-production-use&quot;&gt;impressive growth&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;However, in our opinion, Singularity is the tool of choice for the execution of containerised workloads in the context of HPC, thanks to its focus on system security and its simpler architectural design.&lt;/p&gt;&lt;p&gt;The transparent support provided by Nextflow for both Docker and Singularity technology guarantees the ability to deploy your workflows in a range of different platforms (cloud, cluster, supercomputer, etc). Nextflow transparently manages the deployment of the containerised workload according to the runtime available in the target system.&lt;/p&gt;&lt;h4&gt;Credits&lt;/h4&gt;&lt;p&gt;Thanks to Gabriel Gonzalez (CRG), Luis Exposito (CRG) and Carlos Tripiana Montes (BSC) for the support installing Singularity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; Singularity, since version 2.3.x, is able to pull and run Docker images from the Docker Hub. This greatly simplifies the interoperability with existing Docker containers. You only need to prefix the image name with the &lt;code&gt;docker://&lt;/code&gt; pseudo-protocol to download it as a Singularity image, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity pull --size 1200 docker://nextflow/rnatoy
&lt;/code&gt;&lt;/pre&gt;
	</description>
    </item>
    <item>
      <title>Enabling elastic computing with Nextflow </title>
      <link>https://www.nextflow.io/blog/2016/enabling-elastic-computing-nextflow.html</link>
      <pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2016/enabling-elastic-computing-nextflow.html</guid>
      	<description>
	&lt;p class=&quot;text-muted&quot; style=&apos;font-size: 1.2em; padding-bottom: 10px&apos;&gt;
&lt;i&gt;Learn how to deploy an elastic computing cluster in the AWS cloud with Nextflow &lt;/i&gt;
&lt;/p&gt;&lt;p&gt;In the &lt;a href=&quot;/blog/2016/deploy-in-the-cloud-at-snap-of-a-finger.html&quot;&gt;previous post&lt;/a&gt; I introduced the new cloud native support for AWS provided by Nextflow. &lt;/p&gt;&lt;p&gt;It allows the creation of a computing cluster in the cloud in a no-brainer way, enabling the deployment of complex computational pipelines in a few commands. &lt;/p&gt;&lt;p&gt;This solution is characterised by using a lean application stack which does not require any third party component installed in the EC2 instances other than a Java VM and the Docker engine (the latter it&apos;s only required in order to deploy pipeline binary dependencies). &lt;/p&gt;&lt;p&gt;&lt;img alt=&apos;Nextflow cloud deployment&apos; width=&apos;640&apos; height=&apos;448&apos; src=&apos;/img/cloud-deployment.png&apos; /&gt;&lt;/p&gt;&lt;p&gt;Each EC2 instance runs a script, at bootstrap time, that mounts the &lt;a href=&quot;https://aws.amazon.com/efs/&quot;&gt;EFS&lt;/a&gt; storage and downloads and launches the Nextflow cluster daemon. This daemon is self-configuring, it automatically discovers the other running instances and joins them forming the computing cluster. &lt;/p&gt;&lt;p&gt;The simplicity of this stack makes it possible to setup the cluster in the cloud in just a few minutes, a little more time than is required to spin up the EC2 VMs. This time does not depend on the number of instances launched, as they configure themself independently. &lt;/p&gt;&lt;p&gt;This also makes it possible to add or remove instances as needed, realising the &lt;a href=&quot;http://www.nextplatform.com/2016/09/21/three-great-lies-cloud-computing/&quot;&gt;long promised elastic scalability&lt;/a&gt; of cloud computing. &lt;/p&gt;&lt;p&gt;This ability is even more important for bioinformatic workflows, which frequently crunch not homogenous datasets and are composed of tasks with very different computing requirements (eg. a few very long running tasks and many short-lived tasks in the same workload).&lt;/p&gt;&lt;h3&gt;Going elastic&lt;/h3&gt;&lt;p&gt;The Nextflow support for the cloud features an elastic cluster which is capable of resizing itself to adapt to the actual computing needs at runtime, thus spinning up new EC2 instances when jobs wait for too long in the execution queue, or terminating instances that are not used for a certain amount of time. &lt;/p&gt;&lt;p&gt;In order to enable the cluster autoscaling you will need to specify the autoscale properties in the &lt;code&gt;nextflow.config&lt;/code&gt; file. For example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cloud {
  imageId = &amp;#39;ami-4b7daa32&amp;#39;
  instanceType = &amp;#39;m4.xlarge&amp;#39;

  autoscale {
     enabled = true
     minInstances = 5
     maxInstances = 10 
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above configuration enables the autoscaling features so that the cluster will include at least 5 nodes. If at any point one or more tasks spend more than 5 minutes without being processed, the number of instances needed to fullfil the pending tasks, up to limit specified by the &lt;code&gt;maxInstances&lt;/code&gt; attribute, are launched. On the other hand, if these instances are idle, they are terminated before reaching the 60 minutes instance usage boundary. &lt;/p&gt;&lt;p&gt;The autoscaler launches instances by using the same AMI ID and type specified in the &lt;code&gt;cloud&lt;/code&gt; configuration. However it is possible to define different attributes as shown below: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cloud {
  imageId = &amp;#39;ami-4b7daa32&amp;#39;
  instanceType = &amp;#39;m4.large&amp;#39;

  autoscale {
     enabled = true
     maxInstances = 10 
     instanceType = &amp;#39;m4.2xlarge&amp;#39;
     spotPrice = 0.05 
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The cluster is first created by using instance(s) of type &lt;code&gt;m4.large&lt;/code&gt;. Then, when new computing nodes are required the autoscaler launches instances of type &lt;code&gt;m4.2xlarge&lt;/code&gt;. Also, since the &lt;code&gt;spotPrice&lt;/code&gt; attribute is specified, &lt;a href=&quot;https://aws.amazon.com/ec2/spot/&quot;&gt;EC2 spot&lt;/a&gt; instances are launched, instead of regular on-demand ones, bidding for the price specified. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Nextflow implements an easy though effective cloud scheduler that is able to scale dynamically to meet the computing needs of deployed workloads taking advantage of the &lt;em&gt;elastic&lt;/em&gt; nature of the cloud platform. &lt;/p&gt;&lt;p&gt;This ability, along the support for spot/preemptible instances, allows a cost effective solution for the execution of your pipeline in the cloud. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Deploy your computational pipelines in the cloud at the snap-of-a-finger </title>
      <link>https://www.nextflow.io/blog/2016/deploy-in-the-cloud-at-snap-of-a-finger.html</link>
      <pubDate>Thu, 1 Sep 2016 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2016/deploy-in-the-cloud-at-snap-of-a-finger.html</guid>
      	<description>
	&lt;p class=&quot;text-muted&quot; style=&apos;font-size: 1.2em; padding-bottom: 10px&apos;&gt;
&lt;i&gt;Learn how to deploy and run a computational pipeline in the Amazon AWS cloud with ease 
thanks to Nextflow and Docker containers&lt;/i&gt;
&lt;/p&gt;&lt;p&gt;Nextflow is a framework that simplifies the writing of parallel and distributed computational pipelines in a portable and reproducible manner across different computing platforms, from a laptop to a cluster of computers. &lt;/p&gt;&lt;p&gt;Indeed, the original idea, when this project started three years ago, was to implement a tool that would allow researchers in &lt;a href=&quot;http://www.crg.eu/es/programmes-groups/comparative-bioinformatics&quot;&gt;our lab&lt;/a&gt; to smoothly migrate their data analysis applications in the cloud when needed - without having to change or adapt their code. &lt;/p&gt;&lt;p&gt;However to date Nextflow has been used mostly to deploy computational workflows within on-premise computing clusters or HPC data-centers, because these infrastructures are easier to use and provide, on average, cheaper cost and better performance when compared to a cloud environment. &lt;/p&gt;&lt;p&gt;A major obstacle to efficient deployment of scientific workflows in the cloud is the lack of a performant POSIX compatible shared file system. These kinds of applications are usually made-up by putting together a collection of tools, scripts and system commands that need a reliable file system to share with each other the input and output files as they are produced, above all in a distributed cluster of computers. &lt;/p&gt;&lt;p&gt;The recent availability of the &lt;a href=&quot;https://aws.amazon.com/efs/&quot;&gt;Amazon Elastic File System&lt;/a&gt; (EFS), a fully featured NFS based file system hosted on the AWS infrastructure represents a major step in this context, unlocking the deployment of scientific computing in the cloud and taking it to the next level. &lt;/p&gt;&lt;h3&gt;Nextflow support for the cloud&lt;/h3&gt;&lt;p&gt;Nextflow could already be deployed in the cloud, either using tools such as &lt;a href=&quot;https://github.com/gc3-uzh-ch/elasticluster&quot;&gt;ElastiCluster&lt;/a&gt; or &lt;a href=&quot;https://aws.amazon.com/hpc/cfncluster/&quot;&gt;CfnCluster&lt;/a&gt;, or by using custom deployment scripts. However the procedure was still cumbersome and, above all, it was not optimised to fully take advantage of cloud elasticity i.e. the ability to (re)shape the computing cluster dynamically as the computing needs change over time. &lt;/p&gt;&lt;p&gt;For these reasons, we decided it was time to provide Nextflow with a first-class support for the cloud, integrating the Amazon EFS and implementing an optimised native cloud scheduler, based on &lt;a href=&quot;https://ignite.apache.org/&quot;&gt;Apache Ignite&lt;/a&gt;, with a full support for cluster auto-scaling and spot/preemptible instances. &lt;/p&gt;&lt;p&gt;In practice this means that Nextflow can now spin-up and configure a fully featured computing cluster in the cloud with a single command, after that you need only to login to the master node and launch the pipeline execution as you would do in your on-premise cluster. &lt;/p&gt;&lt;h3&gt;Demo !&lt;/h3&gt;&lt;p&gt;Since a demo is worth a thousands words, I&apos;ve record a short screencast showing how Nextflow can setup a cluster in the cloud and mount the Amazon EFS shared file system. &lt;/p&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/9vupd4d72ivaz6h56pajjjkop.js&quot; id=&quot;asciicast-9vupd4d72ivaz6h56pajjjkop&quot; async&gt;&lt;/script&gt;
&lt;p class=&quot;text-muted&quot; style=&apos;font-size: 0.9em; position: relative; top:-15px&apos; &gt;
Note: in this screencast it has been cut the Ec2 instances startup delay. It required around 
5 minutes to launch them and setup the cluster.   
&lt;/p&gt;&lt;p&gt;Let&apos;s recap the steps showed in the demo: &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;The user provides the cloud parameters (such as the VM image ID and the instance type)  in the &lt;code&gt;nextflow.config&lt;/code&gt; file.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;To configure the EFS file system you need to provide your EFS storage ID and the mount path  by using the &lt;code&gt;sharedStorageId&lt;/code&gt; and &lt;code&gt;sharedStorageMount&lt;/code&gt; properties. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;To use &lt;a href=&quot;https://aws.amazon.com/ec2/spot/&quot;&gt;EC2 Spot&lt;/a&gt; instances, just specify the price  you want to bid by using the &lt;code&gt;spotPrice&lt;/code&gt; property.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The AWS access and secret keys are provided by using the usual environment variables. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The &lt;code&gt;nextflow cloud create&lt;/code&gt; launches the requested number of instances, configures the user and  access key, mounts the EFS storage and setups the Nextflow cluster automatically.  Any Linux AMI can be used, it is only required that the &lt;a href=&quot;https://cloudinit.readthedocs.io/en/latest/&quot;&gt;cloud-init&lt;/a&gt;  package, a Java 7+ runtime and the Docker engine are present. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;When the cluster is ready, you can SSH in the master node and launch the pipeline execution  as usual with the &lt;code&gt;nextflow run &amp;lt;pipeline name&amp;gt;&lt;/code&gt; command.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;For the sake of this demo we are using &lt;a href=&quot;https://github.com/pditommaso/paraMSA&quot;&gt;paraMSA&lt;/a&gt;,  a pipeline for generating multiple sequence alignments and bootstrap replicates developed  in our lab. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Nextflow automatically pulls the pipeline code from its GitHub repository when the  execution is launched. This repository includes also a dataset which is used by default.&lt;br/&gt; &lt;a href=&quot;https://github.com/pditommaso/paraMSA#dependencies-&quot;&gt;The many bioinformatic tools used by the pipeline&lt;/a&gt;  are packaged using a Docker image, which is downloaded automatically on each computing node. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The pipeline results are uploaded automatically in the S3 bucket specified  by the &lt;code&gt;--output s3://cbcrg-eu/para-msa-results&lt;/code&gt; command line option. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;When the computation is completed, the cluster can be safely shutdown and the  EC2 instances terminated with the &lt;code&gt;nextflow cloud shutdown&lt;/code&gt; command. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Try it yourself&lt;/h3&gt;&lt;p&gt;&lt;s&gt;We are releasing the Nextflow integrated cloud support in the upcoming version &lt;code&gt;0.22.0&lt;/code&gt;&lt;/s&gt;. &lt;/p&gt;&lt;p&gt;Nextflow integrated cloud support is available from version &lt;code&gt;0.22.0&lt;/code&gt;. To use it just make sure to have this or an higher version of Nextflow. &lt;/p&gt;&lt;p&gt;Bare in mind that Nextflow requires a Unix-like operating system and a Java runtime version 7+ (Windows 10 users which have installed the &lt;a href=&quot;https://blogs.windows.com/buildingapps/2016/03/30/run-bash-on-ubuntu-on-windows/&quot;&gt;Ubuntu subsystem&lt;/a&gt; should be able to run it, at their risk..). &lt;/p&gt;&lt;p&gt;Once you have installed it, you can follow the steps in the above demo. For your convenience we made publicly available the EC2 image &lt;s&gt;&lt;code&gt;ami-43f49030&lt;/code&gt;&lt;/s&gt; &lt;code&gt;ami-4b7daa32&lt;/code&gt;&lt;sup&gt;* &lt;/sup&gt; (EU Ireland region) used to record this screencast. &lt;/p&gt;&lt;p&gt;Also make sure you have the following the following variables defined in your environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;AWS_ACCESS_KEY_ID=&amp;quot;&amp;lt;your aws access key&amp;gt;&amp;quot;
AWS_SECRET_ACCESS_KEY=&amp;quot;&amp;lt;your aws secret key&amp;gt;&amp;quot;
AWS_DEFAULT_REGION=&amp;quot;&amp;lt;your aws region&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Referes to the &lt;a href=&apos;/docs/latest/awscloud.html&apos;&gt;documentation&lt;/a&gt; for configuration details. &lt;/p&gt;&lt;p&gt;* Update: the AMI has been updated with Java 8 on Sept 2017. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Nextflow provides state of the art support for cloud and containers technologies making it possibile to create computing clusters in the cloud and deploy computational workflows in a no-brainer way, with just two commands on your terminal. &lt;/p&gt;&lt;p&gt;In an upcoming post I will describe the autoscaling capabilities implemented by the Nextflow scheduler that allows, along with the use of spot/preemptible instances, a cost effective solution for the execution of your pipeline in the cloud. &lt;/p&gt;&lt;h4&gt;Credits&lt;/h4&gt;&lt;p&gt;Thanks to &lt;a href=&quot;https://github.com/skptic&quot;&gt;Evan Floden&lt;/a&gt; for reviewing this post and for writing the &lt;a href=&quot;https://github.com/skptic/paraMSA/&quot;&gt;paraMSA&lt;/a&gt; pipeline.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Docker for dunces &amp; Nextflow for nunces</title>
      <link>https://www.nextflow.io/blog/2016/docker-for-dunces-nextflow-for-nunces.html</link>
      <pubDate>Fri, 10 Jun 2016 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2016/docker-for-dunces-nextflow-for-nunces.html</guid>
      	<description>
	&lt;p&gt;&lt;em&gt;Below is a step-by-step guide for creating &lt;a href=&quot;http://www.docker.io&quot;&gt;Docker&lt;/a&gt; images for use with &lt;a href=&quot;http://www.nextflow.io&quot;&gt;Nextflow&lt;/a&gt; pipelines. This post was inspired by recent experiences and written with the hope that it may encourage others to join in the virtualization revolution.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Modern science is built on collaboration. Recently I became involved with one such venture between several groups across Europe. The aim was to annotate long non-coding RNA (lncRNA) in farm animals and I agreed to help with the annotation based on RNA-Seq data. The basic procedure relies on mapping short read data from many different tissues to a genome, generating transcripts and then determining if they are likely to be lncRNA or protein coding genes.&lt;/p&gt;&lt;p&gt;During several successful &apos;hackathon&apos; meetings the best approach was decided and implemented in a joint effort. I undertook the task of wrapping the procedure up into a Nextflow pipeline with a view to replicating the results across our different institutions and to allow the easy execution of the pipeline by researchers anywhere. &lt;/p&gt;&lt;p&gt;Creating the Nextflow pipeline (&lt;a href=&quot;http://www.github.com/cbcrg/lncrna-annotation-nf&quot;&gt;here&lt;/a&gt;) in itself was not a difficult task. My collaborators had documented their work well and were on hand if anything was not clear. However installing and keeping aligned all the pipeline dependencies across different the data centers was still a challenging task. &lt;/p&gt;&lt;p&gt;The pipeline is typical of many in bioinformatics, consisting of binary executions, BASH scripting, R, Perl, BioPerl and some custom Perl modules. We found the BioPerl modules in particular where very sensitive to the various versions in the &lt;em&gt;long&lt;/em&gt; dependency tree. The solution was to turn to &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; containers. &lt;/p&gt;&lt;p&gt;I have taken this opportunity to document the process of developing the Docker side of a Nextflow + Docker pipeline in a step-by-step manner.&lt;/p&gt;&lt;h3&gt;Docker Installation&lt;/h3&gt;&lt;p&gt;By far the most challenging issue is the installation of Docker. For local installations, the &lt;a href=&quot;https://docs.docker.com/engine/installation&quot;&gt;process is relatively straight forward&lt;/a&gt;. However difficulties arise as computing moves to a cluster. Owing to security concerns, many HPC administrators have been reluctant to install Docker system-wide. This is changing and Docker developers have been responding to many of these concerns with &lt;a href=&quot;https://blog.docker.com/2016/02/docker-engine-1-10-security/&quot;&gt;updates addressing these issues&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;That being the case, local installations are usually perfectly fine for development. One of the golden rules in Nextflow development is to have a small test dataset that can run the full pipeline in minutes with few computational resources, ie can run on a laptop.&lt;/p&gt;&lt;p&gt;If you have Docker and Nextflow installed and you wish to view the working pipeline, you can perform the following commands to obtain everything you need and run the full lncrna annotation pipeline on a test dataset. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker pull cbcrg/lncrna_annotation
nextflow run cbcrg/lncrna-annotation-nf -profile test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;[If the following does not work, there could be a problem with your Docker installation.]&lt;/p&gt;&lt;p&gt;The first command will download the required Docker image in your computer, while the second will launch Nextflow which automatically download the pipeline repository and run it using the test data included with it.&lt;/p&gt;&lt;h3&gt;The Dockerfile&lt;/h3&gt;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; contains all the instructions required by Docker to build the Docker image. It provides a transparent and consistent way to specify the base operating system and installation of all software, libraries and modules.&lt;/p&gt;&lt;p&gt;We begin by creating a file &lt;code&gt;Dockerfile&lt;/code&gt; in the Nextflow project directory. The Dockerfile begins with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Set the base image to debian jessie
FROM debian:jessie

# File Author / Maintainer
MAINTAINER Evan Floden &amp;lt;evanfloden@gmail.com&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This sets the base distribution for our Docker image to be Debian v8.4, a lightweight Linux distribution that is ideally suited for the task. We must also specify the maintainer of the Docker image.&lt;/p&gt;&lt;p&gt;Next we update the repository sources and install some essential tools such as &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;perl&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install --yes --no-install-recommends \
    wget \
    locales \
    vim-tiny \
    git \
    cmake \
    build-essential \
    gcc-multilib \
    perl \
    python ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notice that we use the command &lt;code&gt;RUN&lt;/code&gt; before each line. The &lt;code&gt;RUN&lt;/code&gt; instruction executes commands as if they are performed from the Linux shell.&lt;/p&gt;&lt;p&gt;Also is good practice to group as many as possible commands in the same &lt;code&gt;RUN&lt;/code&gt; statement. This reduces the size of the final Docker image. See &lt;a href=&quot;https://blog.replicated.com/2016/02/05/refactoring-a-dockerfile-for-image-size/&quot;&gt;here&lt;/a&gt; for these details and &lt;a href=&quot;https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/&quot;&gt;here&lt;/a&gt; for more best practices. &lt;/p&gt;&lt;p&gt;Next we can specify the install of the required perl modules using &lt;a href=&quot;http://search.cpan.org/~miyagawa/Menlo-1.9003/script/cpanm-menlo&quot;&gt;cpan minus&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Install perl modules
RUN cpanm --force CPAN::Meta \
    YAML \
    Digest::SHA \
    Module::Build \
    Data::Stag \
    Config::Simple \
    Statistics::Lite ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can give the instructions to download and install software from GitHub using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Install Star Mapper
RUN wget -qO- https://github.com/alexdobin/STAR/archive/2.5.2a.tar.gz | tar -xz \ 
    &amp;amp;&amp;amp; cd STAR-2.5.2a \
    &amp;amp;&amp;amp; make STAR
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can add custom Perl modules and specify environmental variables such as &lt;code&gt;PERL5LIB&lt;/code&gt; as below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Install FEELnc
RUN wget -q https://github.com/tderrien/FEELnc/archive/a6146996e06f8a206a0ae6fd59f8ca635c7d9467.zip \
    &amp;amp;&amp;amp; unzip a6146996e06f8a206a0ae6fd59f8ca635c7d9467.zip \ 
    &amp;amp;&amp;amp; mv FEELnc-a6146996e06f8a206a0ae6fd59f8ca635c7d9467 /FEELnc \
    &amp;amp;&amp;amp; rm a6146996e06f8a206a0ae6fd59f8ca635c7d9467.zip

ENV FEELNCPATH /FEELnc
ENV PERL5LIB $PERL5LIB:${FEELNCPATH}/lib/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;R and R libraries can be installed as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Install R
RUN echo &amp;quot;deb http://cran.rstudio.com/bin/linux/debian jessie-cran3/&amp;quot; &amp;gt;&amp;gt;  /etc/apt/sources.list &amp;amp;&amp;amp;\
apt-key adv --keyserver keys.gnupg.net --recv-key 381BA480 &amp;amp;&amp;amp;\
apt-get update --fix-missing &amp;amp;&amp;amp; \
apt-get -y install r-base

# Install R libraries
RUN R -e &amp;#39;install.packages(&amp;quot;ROCR&amp;quot;, repos=&amp;quot;http://cloud.r-project.org/&amp;quot;); install.packages(&amp;quot;randomForest&amp;quot;,repos=&amp;quot;http://cloud.r-project.org/&amp;quot;)&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For the complete working Dockerfile of this project see &lt;a href=&quot;https://github.com/cbcrg/lncRNA-Annotation-nf/blob/master/Dockerfile&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Building the Docker Image&lt;/h3&gt;&lt;p&gt;Once we start working on the Dockerfile, we can build it anytime using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t skptic/lncRNA_annotation .
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This builds the image from the Dockerfile and assigns a tag (i.e. a name) for the image. If there are no errors, the Docker image is now in you local Docker repository ready for use.&lt;/p&gt;&lt;h3&gt;Testing the Docker Image&lt;/h3&gt;&lt;p&gt;We find it very helpful to test our images as we develop the Docker file. Once built, it is possible to launch the Docker image and test if the desired software was correctly installed. For example, we can test if FEELnc and its dependencies were successfully installed by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -ti lncrna_annotation

cd FEELnc/test

FEELnc_filter.pl -i transcript_chr38.gtf -a annotation_chr38.gtf \
&amp;gt; -b transcript_biotype=protein_coding &amp;gt; candidate_lncRNA.gtf

exit # remember to exit the Docker image
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Tagging the Docker Image&lt;/h3&gt;&lt;p&gt;Once you are confident your image is built correctly, you can tag it, allowing you to push it to &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Dockerhub.io&lt;/a&gt;. Dockerhub is an online repository for docker images which allows anyone to pull public images and run them.&lt;/p&gt;&lt;p&gt;You can view the images in your local repository with the &lt;code&gt;docker images&lt;/code&gt; command and tag using &lt;code&gt;docker tag&lt;/code&gt; with the image ID and the name.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker images

REPOSITORY                               TAG                 IMAGE ID            CREATED             SIZE
lncrna_annotation                        latest              d8ec49cbe3ed        2 minutes ago       821.5 MB

docker tag d8ec49cbe3ed cbcrg/lncrna_annotation:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now when we check our local images we can see the updated tag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker images

REPOSITORY                               TAG                 IMAGE ID            CREATED             SIZE
cbcrg/lncrna_annotation                 latest              d8ec49cbe3ed        2 minutes ago       821.5 MB
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Pushing the Docker Image to Dockerhub&lt;/h3&gt;&lt;p&gt;If you have not previously, sign up for a Dockerhub account &lt;a href=&quot;https://hub.docker.com/&quot;&gt;here&lt;/a&gt;. From the command line, login to Dockerhub and push your image.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker login --username=cbcrg
docker push cbcrg/lncrna_annotation
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can test if you image has been correctly pushed and is publicly available by removing your local version using the IMAGE ID of the image and pulling the remote:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker rmi -f d8ec49cbe3ed

# Ensure the local version is not listed.
docker images

docker pull cbcrg/lncrna_annotation
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We are now almost ready to run our pipeline. The last step is to set up the Nexflow config.&lt;/p&gt;&lt;h3&gt;Nextflow Configuration&lt;/h3&gt;&lt;p&gt;Within the &lt;code&gt;nextflow.config&lt;/code&gt; file in the main project directory we can add the following line which links the Docker image to the Nexflow execution. The images can be: &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;General (same docker image for all processes):&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;process {    
    container = &amp;#39;cbcrg/lncrna_annotation&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Specific to a profile (specified by &lt;code&gt;-profile crg&lt;/code&gt; for example):&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;profile {
    crg {
        container = &amp;#39;cbcrg/lncrna_annotation&amp;#39;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Specific to a given process within a pipeline: &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$processName.container = &amp;#39;cbcrg/lncrna_annotation&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In most cases it is easiest to use the same Docker image for all processes. One further thing to consider is the inclusion of the sha256 hash of the image in the container reference. I have &lt;a href=&quot;http://www.nextflow.io/blog/2016/best-practice-for-reproducibility.html&quot;&gt;previously written about this&lt;/a&gt;, but briefly, including a hash ensures that not a single byte of the operating system or software is different.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    process {    
        container = &amp;#39;cbcrg/lncrna_annotation@sha256:9dfe233b...&amp;#39;
    }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;All that is left now to run the pipeline. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run lncRNA-Annotation-nf -profile test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Whilst I have explained this step-by-step process in a linear, consequential manner, in reality the development process is often more circular with changes in the Docker images reflecting changes in the pipeline.&lt;/p&gt;&lt;h3&gt;CircleCI and Nextflow&lt;/h3&gt;&lt;p&gt;Now that you have a pipeline that successfully runs on a test dataset with Docker, a very useful step is to add a continuous development component to the pipeline. With this, whenever you push a modification of the pipeline to the GitHub repo, the test data set is run on the &lt;a href=&quot;http://www.circleci.com&quot;&gt;CircleCI&lt;/a&gt; servers (using Docker). &lt;/p&gt;&lt;p&gt;To include CircleCI in the Nexflow pipeline, create a file named &lt;code&gt;circle.yml&lt;/code&gt; in the project directory. We add the following instructions to the file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine:
    java:
        version: oraclejdk8
    services:
        - docker

dependencies:
    override:

test:
    override:
        - docker pull cbcrg/lncrna_annotation
        - curl -fsSL get.nextflow.io | bash
        - ./nextflow run . -profile test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next you can sign up to CircleCI, linking your GitHub account.&lt;/p&gt;&lt;p&gt;Within the GitHub README.md you can add a badge with the following: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;![CircleCI status](https://circleci.com/gh/cbcrg/lncRNA-Annotation-nf.png?style=shield)
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Tips and Tricks&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;File permissions&lt;/strong&gt;: When a process is executed by a Docker container, the UNIX user running the process is not you. Therefore any files that are used as an input should have the appropriate file permissions. For example, I had to change the permissions of all the input data in the test data set with:&lt;/p&gt;&lt;p&gt; find &lt;data-path&gt; -type f -exec chmod 644 {} \;  find &lt;data-path&gt; -type d -exec chmod 755 {} \;&lt;/p&gt;&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This was my first time building a Docker image and after a bit of trial-and-error the process was surprising straight forward. There is a wealth of information available for Docker and the almost seamless integration with Nextflow is fantastic. Our collaboration team is now looking forward to applying the pipeline to different datasets and publishing the work, knowing our results will be completely reproducible across any platform.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Workflows &amp; publishing: best practice for reproducibility </title>
      <link>https://www.nextflow.io/blog/2016/best-practice-for-reproducibility.html</link>
      <pubDate>Wed, 13 Apr 2016 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2016/best-practice-for-reproducibility.html</guid>
      	<description>
	&lt;p&gt;Publication time acts as a snapshot for scientific work. Whether a project is ongoing or not, work which was performed months ago must be described, new software documented, data collated and figures generated. &lt;/p&gt;&lt;p&gt;The monumental increase in data and pipeline complexity has led to this task being performed to many differing standards, or &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0080278&quot;&gt;lack of thereof&lt;/a&gt;. We all agree it is not good enough to simply note down the software version number. But what practical measures can be taken?&lt;/p&gt;&lt;p&gt;The recent publication describing &lt;em&gt;Kallisto&lt;/em&gt; &lt;a href=&quot;https://doi.org/10.1038/nbt.3519&quot;&gt;(Bray et al. 2016)&lt;/a&gt; provides an excellent high profile example of the growing efforts to ensure reproducible science in computational biology. The authors provide a GitHub &lt;a href=&quot;https://github.com/pachterlab/kallisto_paper_analysis&quot;&gt;repository&lt;/a&gt; that &lt;em&gt;“contains all the analysis to reproduce the results in the kallisto paper”&lt;/em&gt;. &lt;/p&gt;&lt;p&gt;They should be applauded and indeed - in the Twittersphere - they were. The corresponding author Lior Pachter stated that the publication could be reproduced starting from raw reads in the NCBI Sequence Read Archive through to the results, which marks a fantastic accomplishment.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Hoping people will notice &lt;a href=&quot;https://t.co/qiu3LFozMX&quot;&gt;https://t.co/qiu3LFozMX&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/yarbsalocin&quot;&gt;@yarbsalocin&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hjpimentel&quot;&gt;@hjpimentel&lt;/a&gt; &lt;a href=&quot;https://twitter.com/pmelsted&quot;&gt;@pmelsted&lt;/a&gt; reproducing ALL the &lt;a href=&quot;https://twitter.com/hashtag/kallisto?src=hash&quot;&gt;#kallisto&lt;/a&gt; paper from SRA→results&lt;/p&gt;&amp;mdash; Lior Pachter (@lpachter) &lt;a href=&quot;https://twitter.com/lpachter/status/717279998424457216&quot;&gt;April 5, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;p&gt;They achieve this utilising the workflow framework &lt;a href=&quot;https://bitbucket.org/snakemake/snakemake/wiki/Home&quot;&gt;Snakemake&lt;/a&gt;. Increasingly, we are seeing scientists applying workflow frameworks to their pipelines, which is great to see. There is a learning curve, but I have personally found the payoffs in productivity to be immense.&lt;/p&gt;&lt;p&gt;As both users and developers of Nextflow, we have long discussed best practice to ensure reproducibility of our work. As a community, we are at the beginning of that conversation - there are still many ideas to be aired and details ironed out - nevertheless we wished to provide a &lt;em&gt;state-of-play&lt;/em&gt; as we see it and to describe what is possible with Nextflow in this regard.&lt;/p&gt;&lt;h3&gt;Guaranteed Reproducibility&lt;/h3&gt;&lt;p&gt;This is our goal. It is one thing for a pipeline to be able to be reproduced in your own hands, on your machine, yet is another for this to be guaranteed so that anyone anywhere can reproduce it. What I mean by guaranteed is that when a given pipeline is executed, there is only one result which can be output. Envisage what I term the &lt;em&gt;reproducibility triangle&lt;/em&gt;: consisting of data, code and compute environment.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/reproducibility-triangle.png&quot; alt=&quot;Reproducibility Triangle&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; The Reproducibility Triangle. &lt;em&gt;Data&lt;/em&gt;: raw data such as sequencing reads, genomes and annotations but also metadata such as experimental design. &lt;em&gt;Code&lt;/em&gt;: scripts, binaries and libraries/dependencies. &lt;em&gt;Environment&lt;/em&gt;: operating system.&lt;/p&gt;&lt;p&gt;If there is any change to one of these then the reproducibililty is no longer guaranteed. For years there have been solutions to each of these individual components. But they have lived a somewhat discrete existence: data in databases such as the SRA and Ensembl, code on GitHub and compute environments in the form of virtual machines. We think that in the future science must embrace solutions that integrate each of these components natively and holistically.&lt;/p&gt;&lt;h3&gt;Implementation&lt;/h3&gt;&lt;p&gt;Nextflow provides a solution to reproduciblility through version control and sandboxing.&lt;/p&gt;&lt;h4&gt;Code&lt;/h4&gt;&lt;p&gt;Version control is provided via &lt;a href=&quot;http://www.nextflow.io/docs/latest/sharing.html&quot;&gt;native integration with GitHub&lt;/a&gt; and other popular code management platforms such as Bitbucket and GitLab. Pipelines can be pulled, executed, developed, collaborated on and shared. For example, the command below will pull a specific version of a &lt;a href=&quot;https://github.com/cbcrg/kallisto-nf&quot;&gt;simple Kallisto + Sleuth pipeline&lt;/a&gt; from GitHub and execute it. The &lt;code&gt;-r&lt;/code&gt; parameter can be used to specify a specific tag, branch or revision that was previously defined in the Git repository. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run cbcrg/kallisto-nf -r v0.9
&lt;/code&gt;&lt;/pre&gt;&lt;h4&gt;Environment&lt;/h4&gt;&lt;p&gt;Sandboxing during both development and execution is another key concept; version control alone does not ensure that all dependencies nor the compute environment are the same.&lt;/p&gt;&lt;p&gt;A simplified implementation of this places all binaries, dependencies and libraries within the project repository. In Nextflow, any binaries within the the &lt;code&gt;bin&lt;/code&gt; directory of a repository are added to the path. Also, within the Nextflow &lt;a href=&quot;https://github.com/cbcrg/kallisto-nf/blob/master/nextflow.config&quot;&gt;config file&lt;/a&gt;, environmental variables such as &lt;code&gt;PERL5LIB&lt;/code&gt; can be defined so that they are automatically added during the task executions. &lt;/p&gt;&lt;p&gt;This can be taken a step further with containerisation such as &lt;a href=&quot;http://www.nextflow.io/docs/latest/docker.html&quot;&gt;Docker&lt;/a&gt;. We have recently published &lt;a href=&quot;https://doi.org/10.7717/peerj.1273&quot;&gt;work&lt;/a&gt; about this: briefly a &lt;a href=&quot;https://github.com/cbcrg/kallisto-nf/blob/master/Dockerfile&quot;&gt;dockerfile&lt;/a&gt; containing the instructions on how to build the docker image resides inside a repository. This provides a specification for the operating system, software, libraries and dependencies to be run.&lt;/p&gt;&lt;p&gt;The images themself also have content-addressable identifiers in the form of &lt;a href=&quot;https://docs.docker.com/engine/userguide/containers/dockerimages/#image-digests&quot;&gt;digests&lt;/a&gt;, which ensure not a single byte of information, from the operating system through to the libraries pulled from public repos, has been changed. This container digest can be specified in the &lt;a href=&quot;https://github.com/cbcrg/kallisto-nf/blob/master/nextflow.config&quot;&gt;pipeline config file&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process {
    container = &amp;quot;cbcrg/kallisto-nf@sha256:9f84012739...&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When doing so Nextflow automatically pulls the specified image from the Docker Hub and manages the execution of the pipeline tasks from within the container in a transparent manner, i.e. without having to adapt or modify your code. &lt;/p&gt;&lt;h4&gt;Data&lt;/h4&gt;&lt;p&gt;Data is currently one of the more challenging aspect to address. &lt;em&gt;Small data&lt;/em&gt; can be easily version controlled within git-like repositories. For larger files the &lt;a href=&quot;https://git-lfs.github.com/&quot;&gt;Git Large File Storage&lt;/a&gt;, for which Nextflow provides built-in support, may be one solution. Ultimately though, the real home of scientific data is in publicly available, programatically accessible databases. &lt;/p&gt;&lt;p&gt;Providing out-of-box solutions is difficult given the hugely varying nature of the data and meta-data within these databases. We are currently looking to incorporate the most highly used ones, such as the &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/sra&quot;&gt;SRA&lt;/a&gt; and &lt;a href=&quot;http://www.ensembl.org/index.html&quot;&gt;Ensembl&lt;/a&gt;. In the long term we have an eye on initiatives, such as &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/bioproject/&quot;&gt;NCBI BioProject&lt;/a&gt;, with the idea there is a single identifier for both the data and metadata that can be referenced in a workflow.&lt;/p&gt;&lt;p&gt;Adhering to the practices above, one could imagine one line of code which would appear within a publication. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run [user/repo] -r [version] --data[DB_reference:data_reference] -with-docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The result would be guaranteed to be reproduced by whoever wished. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;With this approach the reproducilbility triangle is complete. But it must be noted that this does not guard against conceptial or implemenation errors. It does not replace proper documentation. What it does is to provide transparency to a result.&lt;/p&gt;&lt;p&gt;The assumption that the deterministic nature of computation makes results insusceptible to irreproducbility is clearly false. We consider Nextflow with its other features such its polyglot nature, out-of-the-box portability and native support across HPC and Cloud environments to be an ideal solution in our everyday work. We hope to see more scientists adopt this approach to their workflows. &lt;/p&gt;&lt;p&gt;The recent efforts by the &lt;em&gt;Kallisto&lt;/em&gt; authors highlight the appetite for increasing these standards and we encourage the community at large to move towards ensuring this becomes the normal state of affairs for publishing in science.&lt;/p&gt;&lt;h3&gt;References&lt;/h3&gt;&lt;p&gt;Bray, Nicolas L., Harold Pimentel, Páll Melsted, and Lior Pachter. 2016. “Near-Optimal Probabilistic RNA-Seq Quantification.” Nature Biotechnology, April. Nature Publishing Group. doi:10.1038/nbt.3519.&lt;/p&gt;&lt;p&gt;Di Tommaso P, Palumbo E, Chatzou M, Prieto P, Heuer ML, Notredame C. (2015) &quot;The impact of Docker containers on the performance of genomic pipelines.&quot; PeerJ 3:e1273 doi.org:10.7717/peerj.1273.&lt;/p&gt;&lt;p&gt;Garijo D, Kinnings S, Xie L, Xie L, Zhang Y, Bourne PE, et al. (2013) &quot;Quantifying Reproducibility in Computational Biology: The Case of the Tuberculosis Drugome.&quot; PLoS ONE 8(11): e80278. doi:10.1371/journal.pone.0080278&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Error recovery and automatic resource management with Nextflow</title>
      <link>https://www.nextflow.io/blog/2016/error-recovery-and-automatic-resources-management.html</link>
      <pubDate>Thu, 11 Feb 2016 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2016/error-recovery-and-automatic-resources-management.html</guid>
      	<description>
	&lt;p&gt;Recently a new feature has been added to Nextflow that allows failing jobs to be rescheduled, automatically increasing the amount of computational resources requested. &lt;/p&gt;&lt;h2&gt;The problem&lt;/h2&gt;&lt;p&gt;Nextflow provides a mechanism that allows tasks to be automatically re-executed when a command terminates with an error exit status. This is useful to handle errors caused by temporary or even permanent failures (i.e. network hiccups, broken disks, etc.) that may happen in a cloud based environment. &lt;/p&gt;&lt;p&gt;However in an HPC cluster these events are very rare. In this scenario error conditions are more likely to be caused by a peak in computing resources, allocated by a job exceeding the original resource requested. This leads to the batch scheduler killing the job which in turn stops the overall pipeline execution. &lt;/p&gt;&lt;p&gt;In this context automatically re-executing the failed task is useless because it would simply replicate the same error condition. A common solution consists of increasing the resource request for the needs of the most consuming job, even though this will result in a suboptimal allocation of most of the jobs that are less resource hungry. &lt;/p&gt;&lt;p&gt;Moreover it is also difficult to predict such upper limit. In most cases the only way to determine it is by using a painful fail-and-retry approach. &lt;/p&gt;&lt;p&gt;Take in consideration, for example, the following Nextflow process: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process align {
    executor &amp;#39;sge&amp;#39; 
    memory 1.GB 
    errorStrategy &amp;#39;retry&amp;#39; 

    input: 
    file &amp;#39;seq.fa&amp;#39; from sequences 

    script:
    &amp;#39;&amp;#39;&amp;#39;
    t_coffee -in seq.fa 
    &amp;#39;&amp;#39;&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above definition will execute as many jobs as there are fasta files emitted by the &lt;code&gt;sequences&lt;/code&gt; channel. Since the &lt;code&gt;retry&lt;/code&gt; &lt;em&gt;error strategy&lt;/em&gt; is specified, if the task returns a non-zero error status, Nextflow will reschedule the job execution requesting the same amount of memory and disk storage. In case the error is generated by &lt;code&gt;t_coffee&lt;/code&gt; that it needs more than one GB of memory for a specific alignment, the task will continue to fail, stopping the pipeline execution as a consequence. &lt;/p&gt;&lt;h2&gt;Increase job resources automatically&lt;/h2&gt;&lt;p&gt;A better solution can be implemented with Nextflow which allows resources to be defined in a dynamic manner. By doing this it is possible to increase the memory request when rescheduling a failing task execution. For example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process align { 
    executor &amp;#39;sge&amp;#39;
    memory { 1.GB * task.attempt }
    errorStrategy &amp;#39;retry&amp;#39; 

    input: 
    file &amp;#39;seq.fa&amp;#39; from sequences 

    script:
    &amp;#39;&amp;#39;&amp;#39;
    t_coffee -in seq.fa 
    &amp;#39;&amp;#39;&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the above example the memory requirement is defined by using a dynamic rule. The &lt;code&gt;task.attempt&lt;/code&gt; attribute represents the current task attempt (&lt;code&gt;1&lt;/code&gt; the first time the task is executed, &lt;code&gt;2&lt;/code&gt; the second and so on). &lt;/p&gt;&lt;p&gt;The task will then request one GB of memory. In case of an error it will be rescheduled requesting 2 GB and so on, until it is executed successfully or the limit of times a task can be retried is reached, forcing the termination of the pipeline. &lt;/p&gt;&lt;p&gt;It is also possible to define the &lt;code&gt;errorStrategy&lt;/code&gt; directive in a dynamic manner. This is useful to re-execute failed jobs only if a certain condition is verified. &lt;/p&gt;&lt;p&gt;For example the Univa Grid Engine batch scheduler returns the exit status &lt;code&gt;140&lt;/code&gt; when a job is terminated because it&apos;s using more resources than the ones requested. &lt;/p&gt;&lt;p&gt;By checking this exit status we can reschedule only the jobs that fail by exceeding the resources allocation. This can be done with the following directive declaration: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;errorStrategy { task.exitStatus == 140 ? &amp;#39;retry&amp;#39; : &amp;#39;terminate&amp;#39; }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this way a failed task is rescheduled only when it returns the &lt;code&gt;140&lt;/code&gt; exit status. In all other cases the pipeline execution is terminated. &lt;/p&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Nextflow provides a very flexible mechanism for defining the job resource request and handling error events. It makes it possible to automatically reschedule failing tasks under certain conditions and to define job resource requests in a dynamic manner so that they can be adapted to the actual job&apos;s needs and to optimize the overall resource utilisation. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Developing a bioinformatics pipeline across multiple environments</title>
      <link>https://www.nextflow.io/blog/2016/developing-bioinformatics-pipeline-across-multiple-environments.html</link>
      <pubDate>Thu, 4 Feb 2016 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2016/developing-bioinformatics-pipeline-across-multiple-environments.html</guid>
      	<description>
	&lt;p&gt;As a new bioinformatics student with little formal computer science training, there are few things that scare me more than PhD committee meetings and having to run my code in a completely different operating environment. &lt;/p&gt;&lt;p&gt;Recently my work landed me in the middle of the phylogenetic tree jungle and the computational requirements of my project far outgrew the resources that were available on our institute’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Univa_Grid_Engine&quot;&gt;Univa Grid Engine&lt;/a&gt; based cluster. Luckily for me, an opportunity arose to participate in a joint program at the MareNostrum HPC at the &lt;a href=&quot;http://www.bsc.es&quot;&gt;Barcelona Supercomputing Centre&lt;/a&gt; (BSC).&lt;/p&gt;&lt;p&gt;As one of the top 100 supercomputers in the world, the &lt;a href=&quot;https://www.bsc.es/discover-bsc/the-centre/marenostrum&quot;&gt;MareNostrum III&lt;/a&gt; dwarfs our cluster and consists of nearly 50&apos;000 processors. However it soon became apparent that with great power comes great responsibility and in the case of the BSC, great restrictions. These include no internet access, restrictive wall times for jobs, longer queues, fewer pre-installed binaries and an older version of bash. Faced with the possibility of having to rewrite my 16 bodged scripts for another queuing system I turned to Nextflow.&lt;/p&gt;&lt;p&gt;Straight off the bat I was able to reduce all my previous scripts to a single Nextflow script. Admittedly, the original code was not great, but the data processing model made me feel confident in what I was doing and I was able to reduce the volume of code to 25% of its initial amount whilst making huge improvements in the readability. The real benefits however came from the portability.&lt;/p&gt;&lt;p&gt;I was able to write the project on my laptop (Macbook Air), continuously test it on my local desktop machine (Linux) and then perform more realistic heavy lifting runs on the cluster, all managed from a single GitHub repository. The BSC uses the &lt;a href=&quot;https://en.wikipedia.org/wiki/Platform_LSF&quot;&gt;Load Sharing Facility&lt;/a&gt; (LSF) platform with longer queue times, but a large number of CPUs. My project on the other hand had datasets that require over 100&apos;000 tasks, but the tasks processes themselves run for a matter of seconds or minutes. We were able to marry these two competing interests deploying Nextflow in a &lt;a href=&quot;/blog/2015/mpi-like-execution-with-nextflow.html&quot;&gt;distributed execution manner that resemble the one of an MPI application&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In this configuration, the queuing system allocates the Nextflow requested resources and using the embedded &lt;a href=&quot;https://ignite.apache.org/&quot;&gt;Apache Ignite&lt;/a&gt; clustering engine, Nextflow handles the submission of processes to the individual nodes. &lt;/p&gt;&lt;p&gt;Here is some examples of how to run the same Nextflow project over multiple platforms.&lt;/p&gt;&lt;h4&gt;Local&lt;/h4&gt;&lt;p&gt;If I wished to launch a job locally I can run it with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run myproject.nf
&lt;/code&gt;&lt;/pre&gt;&lt;h4&gt;Univa Grid Engine (UGE)&lt;/h4&gt;&lt;p&gt;For the UGE I simply needed to specify the following in the &lt;code&gt;nextflow.config&lt;/code&gt; file: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process {
        executor=&amp;#39;uge&amp;#39;
        queue=&amp;#39;my_queue&amp;#39;
}  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And then launch the pipeline execution as we did before:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run myproject.nf     
&lt;/code&gt;&lt;/pre&gt;&lt;h4&gt;Load Sharing Facility (LSF)&lt;/h4&gt;&lt;p&gt;For running the same pipeline in the MareNostrum HPC enviroment, taking advantage of the MPI standard to deploy my workload, I first created a wrapper script (for example &lt;code&gt;bsc-wrapper.sh&lt;/code&gt;) declaring the resources that I want to reserve for the pipeline execution:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
#BSUB -oo logs/output_%J.out
#BSUB -eo logs/output_%J.err
#BSUB -J myProject
#BSUB -q bsc_ls
#BSUB -W 2:00
#BSUB -x
#BSUB -n 512
#BSUB -R &amp;quot;span[ptile=16]&amp;quot;
export NXF_CLUSTER_SEED=$(shuf -i 0-16777216 -n 1)
mpirun --pernode bin/nextflow run concMSA.nf -with-mpi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And then can execute it using &lt;code&gt;bsub&lt;/code&gt; as shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bsub &amp;lt; bsc-wrapper.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;By running Nextflow in this way and given the wrapper above, a single &lt;code&gt;bsub&lt;/code&gt; job will run on 512 cores in 32 computing nodes (512/16 = 32) with a maximum wall time of 2 hours. Thousands of Nextflow processes can be spawned during this and the execution can be monitored in the standard manner from a single Nextflow output and error files. If any errors occur the execution can of course to continued with &lt;a href=&quot;/docs/latest/getstarted.html?highlight=resume#modify-and-resume&quot;&gt;&lt;code&gt;-resume&lt;/code&gt; command line option&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Nextflow provides a simplified way to develop across multiple platforms and removes much of the overhead associated with running niche, user developed pipelines in an HPC environment. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>MPI-like distributed execution with Nextflow</title>
      <link>https://www.nextflow.io/blog/2015/mpi-like-execution-with-nextflow.html</link>
      <pubDate>Fri, 13 Nov 2015 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2015/mpi-like-execution-with-nextflow.html</guid>
      	<description>
	&lt;p&gt;The main goal of Nextflow is to make workflows portable across different computing platforms taking advantage of the parallelisation features provided by the underlying system without having to reimplement your application code. &lt;/p&gt;&lt;p&gt;From the beginning Nextflow has included executors designed to target the most popular resource managers and batch schedulers commonly used in HPC data centers, such as &lt;a href=&quot;http://www.univa.com&quot;&gt;Univa Grid Engine&lt;/a&gt;, &lt;a href=&quot;http://www.ibm.com/systems/platformcomputing/products/lsf/&quot;&gt;Platform LSF&lt;/a&gt;, &lt;a href=&quot;https://computing.llnl.gov/linux/slurm/&quot;&gt;SLURM&lt;/a&gt;, &lt;a href=&quot;http://www.pbsworks.com/Product.aspx?id=1&quot;&gt;PBS&lt;/a&gt; and &lt;a href=&quot;http://www.adaptivecomputing.com/products/open-source/torque/&quot;&gt;Torque&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;When using one of these executors Nextflow submits the computational workflow tasks as independent job requests to the underlying platform scheduler, specifying for each of them the computing resources needed to carry out its job. &lt;/p&gt;&lt;p&gt;This approach works well for workflows that are composed of long running tasks, which is the case of most common genomic pipelines.&lt;/p&gt;&lt;p&gt;However this approach does not scale well for workloads made up of a large number of short-lived tasks (e.g. a few seconds or sub-seconds). In this scenario the resource manager scheduling time is much longer than the actual task execution time, thus resulting in an overall execution time that is much longer than the real execution time. In some cases this represents an unacceptable waste of computing resources. &lt;/p&gt;&lt;p&gt;Moreover supercomputers, such as &lt;a href=&quot;https://www.bsc.es/marenostrum-support-services/mn3&quot;&gt;MareNostrum&lt;/a&gt; in the &lt;a href=&quot;https://www.bsc.es/&quot;&gt;Barcelona Supercomputer Center (BSC)&lt;/a&gt;, are optimized for memory distributed applications. In this context it is needed to allocate a certain amount of computing resources in advance to run the application in a distributed manner, commonly using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Message_Passing_Interface&quot;&gt;MPI&lt;/a&gt; standard.&lt;/p&gt;&lt;p&gt;In this scenario, the Nextflow execution model was far from optimal, if not unfeasible. &lt;/p&gt;&lt;h3&gt;Distributed execution&lt;/h3&gt;&lt;p&gt;For this reason, since the release 0.16.0, Nextflow has implemented a new distributed execution model that greatly improves the computation capability of the framework. It uses &lt;a href=&quot;https://ignite.apache.org/&quot;&gt;Apache Ignite&lt;/a&gt;, a lightweight clustering engine and in-memory data grid, which has been recently open sourced under the Apache software foundation umbrella. &lt;/p&gt;&lt;p&gt;When using this feature a Nextflow application is launched as if it were an MPI application. It uses a job wrapper that submits a single request specifying all the needed computing resources. The Nextflow command line is executed by using the &lt;code&gt;mpirun&lt;/code&gt; utility, as shown in the example below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
#$ -l virtual_free=120G
#$ -q &amp;lt;queue name&amp;gt;
#$ -N &amp;lt;job name&amp;gt;
#$ -pe ompi &amp;lt;nodes&amp;gt;
mpirun --pernode nextflow run &amp;lt;your-project-name&amp;gt; -with-mpi [pipeline parameters]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This tool spawns a Nextflow instance in each of the computing nodes allocated by the cluster manager. &lt;/p&gt;&lt;p&gt;Each Nextflow instance automatically connects with the other peers creating an &lt;em&gt;private&lt;/em&gt; internal cluster, thanks to the Apache Ignite clustering feature that is embedded within Nextflow itself. &lt;/p&gt;&lt;p&gt;The first node becomes the application driver that manages the execution of the workflow application, submitting the tasks to the remaining nodes that act as workers. &lt;/p&gt;&lt;p&gt;When the application is complete, the Nextflow driver automatically shuts down the Nextflow/Ignite cluster and terminates the job execution. &lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/img/nextflow-distributed-execution.png&quot; alt=&quot;Nextflow distributed execution&quot;&quot;/&gt;&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;In this way it is possible to deploy a Nextflow workload in a supercomputer using an execution strategy that resembles the MPI distributed execution model. This doesn&apos;t require to implement your application using the MPI api/library and it allows you to maintain your code portable across different execution platforms. &lt;/p&gt;&lt;p&gt;Although we do not currently have a performance comparison between a Nextflow distributed execution and an equivalent MPI application, we assume that the latter provides better performance due to its low-level optimisation. &lt;/p&gt;&lt;p&gt;Nextflow, however, focuses on the fast prototyping of scientific applications in a portable manner while maintaining the ability to scale and distribute the application workload in an efficient manner in an HPC cluster. &lt;/p&gt;&lt;p&gt;This allows researchers to validate an experiment, quickly, reusing existing tools and software components. This eventually makes it possible to implement an optimised version using a low-level programming language in the second stage of a project. &lt;/p&gt;&lt;p&gt;Read the documentation to learn more about the &lt;a href=&quot;http://www.nextflow.io/docs/latest/ignite.html#execution-with-mpi&quot;&gt;Nextflow distributed execution model&lt;/a&gt;.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>The impact of Docker containers on the performance of genomic pipelines</title>
      <link>https://www.nextflow.io/blog/2015/the-impact-of-docker-on-genomic-pipelines.html</link>
      <pubDate>Mon, 15 Jun 2015 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2015/the-impact-of-docker-on-genomic-pipelines.html</guid>
      	<description>
	&lt;p&gt;In a recent publication we assessed the impact of Docker containers technology on the performance of bioinformatic tools and data analysis workflows. &lt;/p&gt;&lt;p&gt;We benchmarked three different data analyses: a RNA sequence pipeline for gene expression, a consensus assembly and variant calling pipeline, and finally a pipeline for the detection and mapping of long non-coding RNAs. &lt;/p&gt;&lt;p&gt;We found that Docker containers have only a minor impact on the performance of common genomic data analysis, which is negligible when the executed tasks are demanding in terms of computational time.&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://peerj.com/preprints/1171/&quot;&gt;This publication is available as PeerJ preprint at this link&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Innovation In Science - The story behind Nextflow</title>
      <link>https://www.nextflow.io/blog/2015/innovation-in-science-the-story-behind-nextflow.html</link>
      <pubDate>Tue, 9 Jun 2015 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2015/innovation-in-science-the-story-behind-nextflow.html</guid>
      	<description>
	&lt;p&gt;Innovation can be viewed as the application of solutions that meet new requirements or existing market needs. Academia has traditionally been the driving force of innovation. Scientific ideas have shaped the world, but only a few of them were brought to market by the inventing scientists themselves, resulting in both time and financial loses. &lt;/p&gt;&lt;p&gt;Lately there have been several attempts to boost scientific innovation and translation, with most notable in Europe being the Horizon 2020 funding program. The problem with these types of funding is that they are not designed for PhDs and Postdocs, but rather aim to promote the collaboration of senior scientists in different institutions. This neglects two very important facts, first and foremost that most of the Nobel prizes were given for discoveries made when scientists were in their 20&apos;s / 30&apos;s (not in their 50&apos;s / 60&apos;s). Secondly, innovation really happens when a few individuals (not institutions) face a problem in their everyday life/work, and one day they just decide to do something about it (end-user innovation). Without realizing, these people address a need that many others have. They don’t do it for the money or the glory; they do it because it bothers them! Many examples of companies that started exactly this way include Apple, Google, and Virgin Airlines.&lt;/p&gt;&lt;h3&gt;The story of Nextflow&lt;/h3&gt;&lt;p&gt;Similarly, Nextflow started as an attempt to solve the every-day computational problems we were facing with “big biomedical data” analyses. We wished that our huge and almost cryptic BASH-based pipelines could handle parallelization automatically. In our effort to make that happen we stumbled upon the &lt;a href=&quot;http://en.wikipedia.org/wiki/Dataflow_programming&quot;&gt;Dataflow&lt;/a&gt; programming model and Nextflow was created. We were getting furious every time our two-week long pipelines were crashing and we had to re-execute them from the beginning. We, therefore, developed a caching system, which allows Nextflow to resume any pipeline from the last executed step. While we were really enjoying developing a new &lt;a href=&quot;http://en.wikipedia.org/wiki/Domain-specific_language&quot;&gt;DSL&lt;/a&gt; and creating our own operators, at the same time we were not willing to give up our favorite Perl/Python scripts and one-liners, and thus Nextflow became a polyglot. &lt;/p&gt;&lt;p&gt;Another problem we were facing was that our pipelines were invoking a lot of third-party software, making distribution and execution on different platforms a nightmare. Once again while searching for a solution to this problem, we were able to identify a breakthrough technology &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt;, which is now revolutionising cloud computation. Nextflow has been one of the first framework, that fully supports Docker containers and allows pipeline execution in an isolated and easy to distribute manner. Of course, sharing our pipelines with our friends rapidly became a necessity and so we had to make Nextflow smart enough to support &lt;a href=&quot;https://github.com&quot;&gt;Github&lt;/a&gt; and &lt;a href=&quot;https://bitbucket.org/&quot;&gt;Bitbucket&lt;/a&gt; integration.&lt;/p&gt;&lt;p&gt;I don’t know if Nextflow will make as much difference in the world as the Dataflow programming model and Docker container technology are making, but it has already made a big difference in our lives and that is all we ever wanted… &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Summarising, it is a pity that PhDs and Postdocs are the neglected engine of Innovation. They are not empowered to innovate, by identifying and addressing their needs, and to potentially set up commercial solutions to their problems. This fact becomes even sadder when you think that only 3% of Postdocs have a chance to become PIs in the UK. Instead more and more money is being invested into the senior scientists who only require their PhD students and Postdocs to put another step into a well-defined ladder. In todays world it seems that ideas, such as Nextflow, will only get funded for their scientific value, not as innovative concepts trying to address a need.&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Introducing Nextflow REPL Console</title>
      <link>https://www.nextflow.io/blog/2015/introducing-nextflow-console.html</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2015/introducing-nextflow-console.html</guid>
      	<description>
	&lt;p&gt;The latest version of Nextflow introduces a new &lt;em&gt;console&lt;/em&gt; graphical interface. &lt;/p&gt;&lt;p&gt;The Nextflow console is a REPL (&lt;a href=&quot;http://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop&quot;&gt;read-eval-print loop&lt;/a&gt;) environment that allows one to quickly test part of a script or pieces of Nextflow code in an interactive manner. &lt;/p&gt;&lt;p&gt;It is a handy tool that allows one to evaluate fragments of Nextflow/Groovy code or fast prototype a complete pipeline script. &lt;/p&gt;&lt;h3&gt;Getting started&lt;/h3&gt;&lt;p&gt;The console application is included in the latest version of Nextflow (&lt;a href=&quot;https://github.com/nextflow-io/nextflow/releases&quot;&gt;0.13.1&lt;/a&gt; or higher). &lt;/p&gt;&lt;p&gt;You can try this feature out, having Nextflow installed on your computer, by entering the following command in your shell terminal: &lt;code&gt;nextflow console&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;When you execute it for the first time, Nextflow will spend a few seconds downloading the required runtime dependencies. When complete the console window will appear as shown in the picture below. &lt;/p&gt;&lt;p&gt;&lt;img src=&apos;/img/nextflow-console1.png&apos; alt=&quot;Nextflow console&quot; style=&apos;width: 100%; padding: 2em 1em;&apos; /&gt;&lt;/p&gt;&lt;p&gt;It contains a text editor (the top white box) that allows you to enter and modify code snippets. The results area (the bottom yellow box) will show the executed code&apos;s output. &lt;/p&gt;&lt;p&gt;At the top you will find the menu bar (not shown in this picture) and the actions toolbar that allows you to open, save, execute (etc.) the code been tested.&lt;/p&gt;&lt;p&gt;As a practical execution example, simply copy and paste the following piece of code in the console editor box:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;echo true 

process sayHello {

 &amp;quot;&amp;quot;&amp;quot;
 echo Hello world
 &amp;quot;&amp;quot;&amp;quot; 

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then, in order to evaluate it, open the &lt;code&gt;Script&lt;/code&gt; menu in the top menu bar and select the &lt;code&gt;Run&lt;/code&gt; command. Alternatively you can use the &lt;code&gt;CTRL+R&lt;/code&gt; keyboard shortcut to run it (&lt;code&gt;⌘+R&lt;/code&gt; on the Mac). In the result box an output similar to the following will appear: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[warm up] executor &amp;gt; local
[00/d78a0f] Submitted process &amp;gt; sayHello (1)
Hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now you can try to modify the entered process script, execute it again and check that the printed result has changed. &lt;/p&gt;&lt;p&gt;If the output doesn&apos;t appear, open the &lt;code&gt;View&lt;/code&gt; menu and make sure that the entry &lt;code&gt;Capture Standard
Output&lt;/code&gt; is selected (it must have a tick on the left).&lt;/p&gt;&lt;p&gt;It is worth noting that the global script context is maintained across script executions. This means that variables declared in the global script scope are not lost when the script run is complete, and they can be accessed in further executions of the same or another piece of code.&lt;/p&gt;&lt;p&gt;In order to reset the global context you can use the command &lt;code&gt;Clear Script Context&lt;/code&gt; available in the &lt;code&gt;Script&lt;/code&gt; menu. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The Nextflow console is a REPL environment which allows you to experiment and get used to the Nextflow programming environment. By using it you can prototype or test your code without the need to create/edit script files.&lt;/p&gt;&lt;p&gt;Note: the Nextflow console is implemented by sub-classing the &lt;a href=&quot;http://groovy-lang.org/groovyconsole.html&quot;&gt;Groovy console&lt;/a&gt; tool. For this reason you may find some labels that refer to the Groovy programming environment in this program. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Using Docker for scientific data analysis in an HPC cluster   </title>
      <link>https://www.nextflow.io/blog/2014/using-docker-in-hpc-cluster.html</link>
      <pubDate>Thu, 6 Nov 2014 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2014/using-docker-in-hpc-cluster.html</guid>
      	<description>
	&lt;p&gt;Scientific data analysis pipelines are rarely composed by a single piece of software. In a real world scenario, computational pipelines are made up of multiple stages, each of which can execute many different scripts, system commands and external tools deployed in a hosting computing environment, usually an HPC cluster. &lt;/p&gt;&lt;p&gt;As I work as a research engineer in a bioinformatics lab I experience on a daily basis the difficulties related on keeping such a piece of software consistent. &lt;/p&gt;&lt;p&gt;Computing enviroments can change frequently in order to test new pieces of software or maybe because system libraries need to be updated. For this reason replicating the results of a data analysis over time can be a challenging task. &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.docker.com&quot;&gt;Docker&lt;/a&gt; has emerged recently as a new type of virtualisation technology that allows one to create a self-contained runtime environment. There are plenty of examples showing the benefits of using it to run application services, like web servers or databases. &lt;/p&gt;&lt;p&gt;However it seems that few people have considered using Docker for the deployment of scientific data analysis pipelines on distributed cluster of computer, in order to simplify the development, the deployment and the replicability of this kind of applications. &lt;/p&gt;&lt;p&gt;For this reason I wanted to test the capabilities of Docker to solve these problems in the cluster available in our &lt;a href=&quot;http://www.crg.eu&quot;&gt;institute&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;Method&lt;/h2&gt;&lt;p&gt;The Docker engine has been installed in each node of our cluster, that runs a &lt;a href=&quot;http://www.univa.com/products/grid-engine.php&quot;&gt;Univa grid engine&lt;/a&gt; resource manager. A Docker private registry instance has also been installed in our internal network, so that images can be pulled from the local repository in a much faster way when compared to the public &lt;a href=&quot;http://registry.hub.docker.com&quot;&gt;Docker registry&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Moreover the Univa grid engine has been configured with a custom &lt;a href=&quot;http://www.gridengine.eu/mangridengine/htmlman5/complex.html&quot;&gt;complex&lt;/a&gt; resource type. This allows us to request a specific Docker image as a resource type while submitting a job execution to the cluster. &lt;/p&gt;&lt;p&gt;The Docker image is requested as a &lt;em&gt;soft&lt;/em&gt; resource, by doing that the UGE scheduler tries to run a job to a node where that image has already been pulled, otherwise a lower priority is given to it and it is executed, eventually, by a node where the specified Docker image is not available. This will force the node to pull the required image from the local registry at the time of the job execution. &lt;/p&gt;&lt;p&gt;This environment has been tested with &lt;a href=&quot;https://github.com/cbcrg/piper-nf&quot;&gt;Piper-NF&lt;/a&gt;, a genomic pipeline for the detection and mapping of long non-coding RNAs. &lt;/p&gt;&lt;p&gt;The pipeline runs on top of Nextflow, which takes care of the tasks parallelisation and submits the jobs for execution to the Univa grid engine. &lt;/p&gt;&lt;p&gt;The Piper-NF code wasn&apos;t modified in order to run it using Docker. Nextflow is able to handle it automatically. The Docker containers are run in such a way that the tasks result files are created in the hosting file system, in other words it behaves in a completely transparent manner without requiring extra steps or affecting the flow of the pipeline execution. &lt;/p&gt;&lt;p&gt;It was only necessary to specify the Docker image (or images) to be used in the Nextflow configuration file for the pipeline. You can read more about this at &lt;a href=&quot;http://www.nextflow.io/docs/latest/docker.html&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;Results&lt;/h2&gt;&lt;p&gt;To benchmark the impact of Docker on the pipeline performance a comparison was made running it with and without Docker.&lt;/p&gt;&lt;p&gt;For this experiment 10 cluster nodes were used. The pipeline execution launches around 100 jobs, and it was run 5 times by using the same dataset with and without Docker. &lt;/p&gt;&lt;p&gt;The average execution time without Docker was 28.6 minutes, while the average pipeline execution time, running each job in a Docker container, was 32.2 minutes. Thus, by using Docker the overall execution time increased by something around 12.5%. &lt;/p&gt;&lt;p&gt;It is important to note that this time includes both the Docker bootstrap time, and the time overhead that is added to the task execution by the virtualisation layer. &lt;/p&gt;&lt;p&gt;For this reason the actual task run time was measured as well i.e. without including the Docker bootstrap time overhead. In this case, the aggregate average task execution time was 57.3 minutes and 59.5 minutes when running the same tasks using Docker. Thus, the time overhead added by the Docker virtualisation layer to the effective task run time can be estimated to around 4% in our test.&lt;/p&gt;&lt;p&gt;Keeping the complete toolset required by the pipeline execution within a Docker image dramatically reduced configuration and deployment problems. Also storing these images into the private and &lt;a href=&quot;https://registry.hub.docker.com/repos/cbcrg/&quot;&gt;public&lt;/a&gt; repositories with a unique tag allowed us to replicate the results without the usual burden required to set-up an identical computing environment. &lt;/p&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The fast start-up time for Docker containers technology allows one to virtualise a single process or the execution of a bunch of applications, instead of a complete operating system. This opens up new possibilities, for example the possibility to &quot;virtualise&quot; distributed job executions in an HPC cluster of computers. &lt;/p&gt;&lt;p&gt;The minimal performance loss introduced by the Docker engine is offset by the advantages of running your analysis in a self-contained and dead easy to reproduce runtime environment, which guarantees the consistency of the results over time and across different computing platforms. &lt;/p&gt;&lt;h4&gt;Credits&lt;/h4&gt;&lt;p&gt;Thanks to Arnau Bria and the all scientific systems admins team to manage the Docker installation in the CRG computing cluster. &lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Reproducibility in Science - Nextflow meets Docker </title>
      <link>https://www.nextflow.io/blog/2014/nextflow-meets-docker.html</link>
      <pubDate>Tue, 9 Sep 2014 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2014/nextflow-meets-docker.html</guid>
      	<description>
	&lt;p&gt;The scientific world nowadays operates on the basis of published articles. These are used to report novel discoveries to the rest of the scientific community. &lt;/p&gt;&lt;p&gt;But have you ever wondered what a scientific article is? It is a:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;defeasible argument for claims, supported by&lt;/li&gt;
  &lt;li&gt;exhibited, reproducible data and methods, and&lt;/li&gt;
  &lt;li&gt;explicit references to other work in that domain;&lt;/li&gt;
  &lt;li&gt;described using domain-agreed technical terminology,&lt;/li&gt;
  &lt;li&gt;which exists within a complex ecosystem of technologies, people and activities.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Hence the very essence of Science relies on the ability of scientists to reproduce and build upon each other’s published results.&lt;/p&gt;&lt;p&gt;So how much can we rely on published data? In a recent report in Nature, researchers at the Amgen corporation found that only 11% of the academic research in the literature was reproducible by their groups [&lt;a href=&quot;http://www.nature.com/nature/journal/v483/n7391/full/483531a.html&quot;&gt;1&lt;/a&gt;]. &lt;/p&gt;&lt;p&gt;While many factors are likely at play here, perhaps the most basic requirement for reproducibility holds that the materials reported in a study can be uniquely identified and obtained, such that experiments can be reproduced as faithfully as possible. This information is meant to be documented in the &quot;materials and methods&quot; of journal articles, but as many can attest, the information provided there is often not adequate for this task. &lt;/p&gt;&lt;h3&gt;Promoting Computational Research Reproducibility&lt;/h3&gt;&lt;p&gt;Encouragingly scientific reproducibility has been at the forefront of many news stories and there exist numerous initiatives to help address this problem. Particularly, when it comes to producing reproducible computational analyses, some publications are starting to publish the code and data used for analysing and generating figures. &lt;/p&gt;&lt;p&gt;For example, many articles in Nature and in the new Elife journal (and others) provide a &quot;source data&quot; download link next to figures. Sometimes Elife might even have an option to download the source code for figures.&lt;/p&gt;&lt;p&gt;As pointed out by Melissa Gymrek &lt;a href=&quot;http://melissagymrek.com/science/2014/08/29/docker-reproducible-research.html&quot;&gt;in a recent post&lt;/a&gt; this is a great start, but there are still lots of problems. She wrote that, for example, if one wants to re-execute a data analyses from these papers, he/she will have to download the scripts and the data, to only realize that he/she has not all the required libraries, or that it only runs on, for example, an Ubuntu version he/she doesn&apos;t have, or some paths are hard-coded to match the authors&apos; machine. &lt;/p&gt;&lt;p&gt;If it&apos;s not easy to run and doesn&apos;t run out of the box the chances that a researcher will actually ever run most of these scripts is close to zero, especially if they lack the time or expertise to manage the required installation of third-party libraries, tools or implement from scratch state-of-the-art data processing algorithms.&lt;/p&gt;&lt;h3&gt;Here comes Docker&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://www.docker.com&quot;&gt;Docker&lt;/a&gt; containers technology is a solution to many of the computational research reproducibility problems. Basically, it is a kind of a lightweight virtual machine where you can set up a computing environment including all the libraries, code and data that you need, within a single &lt;em&gt;image&lt;/em&gt;. &lt;/p&gt;&lt;p&gt;This image can be distributed publicly and can seamlessly run on any major Linux operating system. No need for the user to mess with installation, paths, etc. &lt;/p&gt;&lt;p&gt;They just run the Docker image you provided, and everything is set up to work out of the box. Researchers have already started discussing this (e.g. &lt;a href=&quot;http://www.bioinformaticszen.com/post/reproducible-assembler-benchmarks/&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://bcbio.wordpress.com/2014/03/06/improving-reproducibility-and-installation-of-genomic-analysis-pipelines-with-docker/&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;h3&gt;Docker and Nextflow: a perfect match&lt;/h3&gt;&lt;p&gt;One big advantage Docker has compared to &lt;em&gt;traditional&lt;/em&gt; machine virtualisation technology is that it doesn&apos;t need a complete copy of the operating system, thus it has a minimal startup time. This makes it possible to virtualise single applications or launch the execution of multiple containers, that can run in parallel, in order to speedup a large computation. &lt;/p&gt;&lt;p&gt;Nextflow is a data-driven toolkit for computational pipelines, which aims to simplify the deployment of distributed and highly parallelised pipelines for scientific applications. &lt;/p&gt;&lt;p&gt;The latest version integrates the support for Docker containers that enables the deployment of self-contained and truly reproducible pipelines. &lt;/p&gt;&lt;h3&gt;How they work together&lt;/h3&gt;&lt;p&gt;A Nextflow pipeline is made up by putting together several processes. Each process can be written in any scripting language that can be executed by the Linux platform (BASH, Perl, Ruby, Python, etc). Parallelisation is automatically managed by the framework and it is implicitly defined by the processes input and output declarations. &lt;/p&gt;&lt;p&gt;By integrating Docker with Nextflow, every pipeline process can be executed independently in its own container, this guarantees that each of them run in a predictable manner without worrying about the configuration of the target execution platform. Moreover the minimal overhead added by Docker allows us to spawn multiple container executions in a parallel manner with a negligible performance loss when compared to a platform &lt;em&gt;native&lt;/em&gt; execution. &lt;/p&gt;&lt;h3&gt;An example&lt;/h3&gt;&lt;p&gt;As a proof of concept of the Docker integration with Nextflow you can try out the pipeline example at this &lt;a href=&quot;https://github.com/nextflow-io/examples/blob/master/blast-parallel.nf&quot;&gt;link&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;It splits a protein sequences multi FASTA file into chunks of &lt;em&gt;n&lt;/em&gt; entries, executes a BLAST query for each of them, then extracts the top 10 matching sequences and finally aligns the results with the T-Coffee multiple sequence aligner. &lt;/p&gt;&lt;p&gt;In a common scenario you generally need to install and configure the tools required by this script: BLAST and T-Coffee. Moreover you should provide a formatted protein database in order to execute the BLAST search.&lt;/p&gt;&lt;p&gt;By using Docker with Nextflow you only need to have the Docker engine installed in your computer and a Java VM. In order to try this example out, follow these steps: &lt;/p&gt;&lt;p&gt;Install the latest version of Nextflow by entering the following command in your shell terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; curl -fsSL get.nextflow.io | bash
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then download the required Docker image with this command: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt; docker pull nextflow/examples
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can check the content of the image looking at the &lt;a href=&quot;https://github.com/nextflow-io/examples/blob/master/Dockerfile&quot;&gt;Dockerfile&lt;/a&gt; used to create it.&lt;/p&gt;&lt;p&gt;Now you are ready to run the demo by launching the pipeline execution as shown below: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run examples/blast-parallel.nf -with-docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will run the pipeline printing the final alignment out on the terminal screen. You can also provide your own protein sequences multi FASTA file by adding, in the above command line, the option &lt;code&gt;--query &amp;lt;file&amp;gt;&lt;/code&gt; and change the splitting chunk size with &lt;code&gt;--chunk n&lt;/code&gt; option. &lt;/p&gt;&lt;p&gt;Note: the result doesn&apos;t have a real biological meaning since it uses a very small protein database. &lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The mix of Docker, GitHub and Nextflow technologies make it possible to deploy self-contained and truly replicable pipelines. It requires zero configuration and enables the reproducibility of data analysis pipelines in any system in which a Java VM and the Docker engine are available.&lt;/p&gt;&lt;h3&gt;Learn how to do it!&lt;/h3&gt;&lt;p&gt;Follow our documentation for a quick start using Docker with Nextflow at the following link &lt;a href=&quot;http://www.nextflow.io/docs/latest/docker.html&quot;&gt;http://www.nextflow.io/docs/latest/docker.html&lt;/a&gt;&lt;/p&gt;
	</description>
    </item>
    <item>
      <title>Share Nextflow pipelines with GitHub</title>
      <link>https://www.nextflow.io/blog/2014/share-nextflow-pipelines-with-github.html</link>
      <pubDate>Thu, 7 Aug 2014 00:00:00 +0000</pubDate>
      <guid isPermaLink="false">blog/2014/share-nextflow-pipelines-with-github.html</guid>
      	<description>
	&lt;p&gt;The &lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt; code repository and collaboration platform is widely used between researchers to publish their work and to collaborate on projects source code. &lt;/p&gt;&lt;p&gt;Even more interestingly a few months ago &lt;a href=&quot;https://github.com/blog/1840-improving-github-for-science&quot;&gt;GitHub announced improved support for researchers&lt;/a&gt; making it possible to get a Digital Object Identifier (DOI) for any GitHub repository archive. &lt;/p&gt;&lt;p&gt;With a DOI for your GitHub repository archive your code becomes formally citable in scientific publications.&lt;/p&gt;&lt;h3&gt;Why use GitHub with Nextflow?&lt;/h3&gt;&lt;p&gt;The latest Nextflow release (0.9.0) seamlessly integrates with GitHub. This feature allows you to manage your code in a more consistent manner, or use other people&apos;s Nextflow pipelines, published through GitHub, in a quick and transparent manner. &lt;/p&gt;&lt;h3&gt;How it works&lt;/h3&gt;&lt;p&gt;The idea is very simple, when you launch a script execution with Nextflow, it will look for a file with the pipeline name you&apos;ve specified. If that file does not exist, it will look for a public repository with the same name on GitHub. If it is found, the repository is automatically downloaded to your computer and the code executed. This repository is stored in the Nextflow home directory, by default &lt;code&gt;$HOME/.nextflow&lt;/code&gt;, thus it will be reused for any further execution.&lt;/p&gt;&lt;p&gt;You can try this feature out, having Nextflow (version 0.9.0 or higher) installed in your computer, by simply entering the following command in your shell terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run nextflow-io/hello 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first time you execute this command Nextflow will download the pipeline at the following GitHub repository &lt;code&gt;https://github.com/nextflow-io/hello&lt;/code&gt;, as you don&apos;t already have it in your computer. It will then execute it producing the expected output.&lt;/p&gt;&lt;p&gt;In order for a GitHub repository to be used as a Nextflow project, it must contain at least one file named &lt;code&gt;main.nf&lt;/code&gt; that defines your Nextflow pipeline script.&lt;/p&gt;&lt;h3&gt;Run a specific revision&lt;/h3&gt;&lt;p&gt;Any Git branch, tag or commit ID in the GitHub repository can be used to specify a revision, that you want to execute, when running your pipeline by adding the &lt;code&gt;-r&lt;/code&gt; option to the run command line. So for example you could enter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run nextflow-io/hello -r mybranch   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;or &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow run nextflow-io/hello -r v1.1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This can be very useful when comparing different versions of your project. It also guarantees consistent results in your pipeline as your source code evolves.&lt;/p&gt;&lt;h3&gt;Commands to manage pipelines&lt;/h3&gt;&lt;p&gt;The following commands allows you to perform some basic operations that can be used to manage your pipelines. Anyway Nextflow is not meant to replace functionalities provided by the &lt;a href=&quot;http://git-scm.com/&quot;&gt;Git&lt;/a&gt; tool, you may still need it to create new repositories or commit changes, etc. &lt;/p&gt;&lt;h4&gt;List available pipelines&lt;/h4&gt;&lt;p&gt;The &lt;code&gt;ls&lt;/code&gt; command allows you to list all the pipelines you have downloaded in your computer. For example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow ls
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This prints a list similar to the following one: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cbcrg/piper-nf
nextflow-io/hello
&lt;/code&gt;&lt;/pre&gt;&lt;h4&gt;Show pipeline information&lt;/h4&gt;&lt;p&gt;By using the &lt;code&gt;info&lt;/code&gt; command you can show information from a downloaded pipeline. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nextflow info hello
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This command prints: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt; repo name  : nextflow-io/hello
 home page  : http://github.com/nextflow-io/hello
 local path : $HOME/.nextflow/assets/nextflow-io/hello
 main script: main.nf
 revisions  : 
 * master (default)
   mybranch
   v1.1 [t]
   v1.2 [t]   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Starting from the top it shows: 1) the repository name; 2) the project home page; 3) the local folder where the pipeline has been downloaded; 4) the script that is executed when launched; 5) the list of available revisions i.e. branches + tags. Tags are marked with a &lt;code&gt;[t]&lt;/code&gt; on the right, the current checked-out revision is marked with a &lt;code&gt;*&lt;/code&gt; on the left.&lt;/p&gt;&lt;h4&gt;Pull or update a pipeline&lt;/h4&gt;&lt;p&gt;The &lt;code&gt;pull&lt;/code&gt; command allows you to download a pipeline from a GitHub repository or to update it if that repository has already been downloaded. For example: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow pull nextflow-io/examples
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Downloaded pipelines are stored in the folder &lt;code&gt;$HOME/.nextflow/assets&lt;/code&gt; in your computer.&lt;/p&gt;&lt;h4&gt;Clone a pipeline into a folder&lt;/h4&gt;&lt;p&gt;The &lt;code&gt;clone&lt;/code&gt; command allows you to copy a Nextflow pipeline project to a directory of your choice. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow clone nextflow-io/hello target-dir 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If the destination directory is omitted the specified pipeline is cloned to a directory with the same name as the pipeline &lt;em&gt;base&lt;/em&gt; name (e.g. &lt;code&gt;hello&lt;/code&gt;) in the current folder. &lt;/p&gt;&lt;p&gt;The clone command can be used to inspect or modify the source code of a pipeline. You can eventually commit and push back your changes by using the usual Git/GitHub workflow. &lt;/p&gt;&lt;h4&gt;Drop an installed pipeline&lt;/h4&gt;&lt;p&gt;Downloaded pipelines can be deleted by using the &lt;code&gt;drop&lt;/code&gt; command, as shown below: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nextflow drop nextflow-io/hello
&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;Limitations and known problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;s&gt;GitHub private repositories currently are not supported&lt;/s&gt; Support for private GitHub repositories has been introduced with version 0.10.0.&lt;/li&gt;
  &lt;li&gt;&lt;s&gt;Symlinks committed in a Git repository are not resolved correctly  when downloaded/cloned by Nextflow&lt;/s&gt; Symlinks are resolved correctly when using Nextflow version 0.11.0 (or higher).&lt;/li&gt;
&lt;/ul&gt;
	</description>
    </item>

  </channel> 
</rss>
